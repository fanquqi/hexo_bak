<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F17%2Fpython%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[title: python字符串格式化问题date: 2017-08-11T15:59:05.000Ztags: python categories: python场景描述 我格式化输出一个值竟然报出这种神奇的错误，很不解。 仔细看了下 douban.critic[i]这个变量type是个Unicode。 而前面影评两个字是str不是Unicode 两个值在一起输出就会报错。 原因Python在格式化字符串的时候有一些隐藏着的小动作：如果%s对应的参数里有unicode，那么最终的结果也是unicode。在这种情况下模版字符串以及所有的%s参数中的str都会被decode成unicode，然而这个decode是隐式的，用户无法指定其使用的charset，Python只能用默认的ASCII。如果正好里面有非ASCII编码的字符串，就完蛋了…… 解决统一格式 参考：print u”影评：%s” % douban.critic[i]]]></content>
  </entry>
  <entry>
    <title><![CDATA[open-falcon升级]]></title>
    <url>%2F2017%2F08%2F15%2Fopen-falcon%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[这次迁移本来想使用docker，看了下官方给的镜像2G多，差点吓死我，另外还有mysql，redis融合在里面。我们的mysql不在机器本地，也没有打算迁移。为了防止上次ucloud B机房整体服务断绝联系之后一个报警都没有尴尬发生，我准备把这台机器放到其他机房，万一下次ucloud B机房挂掉最起码监控是有的，但是如果它全线挂。。。。呸，上次就是我这乌鸦嘴。。。 &emsp;&emsp;这次迁移过程具体分为 falcon—plus 部署 新系统测试 各机器falcon-agent升级 域名更改，grafana更改 falcon-plus部署&emsp;&emsp;参考官网&emsp;&emsp;首先数据库有更改，增加了一个alarm库 这个是用来存储报警历史的，在dashboard中有展示，需要我们down下来执行下这个SQL。然后sender模块和alarm模块合二为一了，也就是说报警直接由alarm模块发送了，这点我刚看到的时候有一点纠结，因为v0.1版本我们没有使用sender模块,而是直接使用的一个脚本实时读redis，然后根据级别判断，发送邮件，微信，短信，钉钉，这不知道是不是意味着我必须修改alarm源码了。我们先厚着头皮继续部署，遇到什么问题解决什么问题。 机器初始环境部署我在这里忽略了，包括golang等一些环境的安装。 执行SQL wget https://github.com/open-falcon/falcon-plus/blob/master/scripts/mysql/db_schema/5_alarms-db-schema.sql &gt; mysql -h127.0.0.1 -ufalcon -p 二进制安装 安装包下载地址 123cd $falcon_dirwget https://github.com/open-falcon/falcon-plus/releases/download/v0.2.0/open-falcon-v0.2.0.tar.gztar -zxvf open-falcon-v0.2.0.tar.gz 然后对应修改每个模块的配置文件这个我就不记录了 启动|停止|检查 ./open-falcon start|stop|check 各节点启动方式 进到相应模块bin目录，执行二进制文件 例：./falcon-agent start|stop|restart|tail 在$faocon_dir ./open-falcon start|stop|reload|monitor dashboard部署&emsp;&emsp;参考官网 操作按照官网文档依次进行 这里有个小坑配置openLDAP登录的时候，配置配置也较之前0.1版本有所不同） 0.1版本可以用reader用户+密码去验证用户名密码 &quot;ldap&quot;: { &quot;enabled&quot;: true, &quot;addr&quot;: &quot;ldap.example.com:389&quot;, &quot;baseDN&quot;: &quot;dc=example,dc=com&quot;, &quot;bindDN&quot;: &quot;cn=reader,dc=example,dc=com&quot;, &quot;bindPasswd&quot;: &quot;123456&quot;, &quot;userField&quot;: &quot;uid&quot;, &quot;attributes&quot;: [&quot;uid&quot;,&quot;givenName&quot;,&quot;sn&quot;,&quot;mail&quot;] 0.2版本验证方式直接是向openLDAP的389端口验证该用户名的密码正确性，不用配置reader用户和密码 LDAP_ENABLED = os.environ.get(&quot;LDAP_ENABLED&quot;,True) LDAP_SERVER = os.environ.get(&quot;LDAP_SERVER&quot;,&quot;ldap.example.com:389&quot;) LDAP_BASE_DN = os.environ.get(&quot;LDAP_BASE_DN&quot;,&quot;uid=%s,OU=People,DC=example,DC=com&quot;) LDAP_BINDDN_FMT = os.environ.get(&quot;LDAP_BINDDN_FMT&quot;,&quot;uid=%s,OU=People,DC=example,DC=com&quot;) LDAP_SEARCH_FMT = os.environ.get(&quot;LDAP_SEARCH_FMT&quot;,&quot;(uid=%s)&quot;) LDAP_ATTRS = [&quot;uid&quot;,&quot;givenName&quot;,&quot;sn&quot;,&quot;mail&quot;, &quot;cn&quot;] 具体LDAP概念比如cn sn 参考 配置完成。但是。。之前的用户不能登录成功。。。看源码看到这句WTF，so，只能删掉用户数据，重建，但是。。。 &emso;&emsp;我把用户都删了重建，这个导致了后来一个bug，因为报警都是按组发送的，所以用户删完之后，组别在dashboard中没有显示了。。新建会显示 组已经存在 我去，原来数据库中还有一个action表，里面还有原来分组的消息，我只能新建另一个分组，到portal中（v0.1版本我没有下掉，监控不能断）把报警接收组改成新加的这个组。。 然后就OK了。但是。上文说过，alarm与sender合并了，所以报警怎么发？ 我去看了下alarm的配置文件发现了这个 &quot;worker&quot;: { &quot;im&quot;: 10, &quot;sms&quot;: 20, &quot;mail&quot;: 10 }, 对比上下文，我感觉这可能就是发微信，短信，邮件的worker数，不知道代码做没做判断，我直接把worker数改成了0，重启 查看redis 123456进到cliredis-cli keys *lrange /sms 0 0 看到有报警信息 跟0.1版本一模一样 我的脚本都不用修改，真是完美。 新系统测试模拟报警发送消息。略 各机器agent更新 ./falcon-agent 二进制文件更新 配置文件更新（记得server地址更改） 写好 ansible role直接执行， nginx转发更改此dashboard是使用的flask+gunicron 起的 我在内网nginx上做反向代理转发到这台机器8080 一直502，不得其解，因为着急验收，我直接在falcon_plus这台云主机上yum安装了一个nginx，然后内网nginx请求转发到这台机器80，就OK了。原因有时间再去看吧。。。或者说下次遇到falsk再看吧。。。 总结&emsp;&emsp;遇到问题解决问题，不要思考的过于多导致自己迟迟不动手。]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>openfalcon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫一,简单爬取豆瓣高分电影]]></title>
    <url>%2F2017%2F08%2F11%2Fpython%E7%88%AC%E8%99%AB%E4%B8%80-%E7%AE%80%E5%8D%95%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E9%AB%98%E5%88%86%E7%94%B5%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[作者作为一个运维，始终有个开发的心，在打杂的空隙中抽时间搞点东西，怒刷存在感。 这将会是一个系列，本节先从基本的抓取豆瓣高分电影开始。后期将会有更多有意义的内容，比如做代理抓取草榴的图片，视频给各位看官。。。开玩笑，不要当真，这估计会被抓的。。 工具说明 python版本 2.7.8 编辑器 sublime 功能介绍 抓取Top100电影，及相关影评. 结果展示12345678910111213141516Top 1, 肖申克的救赎 影评：希望让人自由。Top 2, 霸王别姬 影评：风华绝代。Top 3, 这个杀手不太冷 影评：怪蜀黍和小萝莉不得不说的故事。Top 4, 阿甘正传 影评：一部美国近现代史。......Top 98, 末代皇帝 影评：“不要跟我比惨，我比你更惨”再适合这部电影不过了。Top 99, 阿凡达 影评：绝对意义上的美轮美奂。Top 100, 摩登时代 影评：大时代中的人生，小人物的悲喜。 前期工作流程 分析url 对每页的url特征有些了解 分析HTML 根据HTML特征设计相关正则对内容进行抓取 后期工作展望 使用多线程爬取 了解python爬虫框架Scrapy 了解模拟登陆或者加代理的爬取方法 熟悉HTML标签规则]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python 爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql误删除表恢复]]></title>
    <url>%2F2017%2F08%2F04%2Fmysql%E8%AF%AF%E5%88%A0%E9%99%A4%E8%A1%A8%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[场景描述 今天有个开发哥哥来找我。说有个医生信息表被删除了（测试服）但是现在版本正在测试，账号已经登不上了，bug群里已经有人说话了，CTO已经看到了，情况已经很紧急了。但是我之前也没有遇到过这类事情，而且测试服mysql是不备份的，一时不知道要怎么办。 解决但是 事情还是要解决也总是有解决办法。下边按照先后顺序记录了下我的操作方案。 方案一 使用bin-log，我们测试服mysql是有记录mysql-binlog的，（本来也是没有开的，在六月份的时候测试服总是内存不够用，我把数据库从本地迁移独立了出来开启了bin-log） bin-log恢复方案介绍 先查看这次今天mysql操作记录，发现没有什么操作，毕竟周五， 所以计划直接恢复到删表之前（就是昨天，中间操作的一段时间有几个今天修改的记录不见得情况），然后再从删表后的时间节点到现在恢复，等于只跳过了那条删表操作，别的操作都重新来一遍。 操作流程首先查看mysql-binlog 找到误操作drop table的时间节点 这里没有图文。。。 // 查找对应的bin-log文件 mysqlbinlog mysql-bin.000002 | grep -A4 &quot;table_name&quot; 然后恢复对应时间节点恢复（其实mysqlbinlog也可以对应postion节点恢复） mysqlbinlog -d test_medical –stop-datetime=’2017-08-03 17:57:05’ /home/mysql/3306/data/mysql-bin.000002 | mysql -uroot -p -P 3306 -S /home/mysql/3306/mysql.sock // -d 数据库名 –stop-datetime 结束时间点，–start-datetime 开始时间节点 二者可以一起写就是中间时间短的操作 可想而知我这样操作的结果，肯定是一堆冲突，因为当前数据库已经是操作过一遍的结果了，再把执行过得命令拿过来执行下，肯定是有错误。所以我们方案一肯定是不可取的。 方案二 我之前是有迁移过这个MySQL的，我赶紧看了下迁移的数据压缩包还在时间是2017-6-19,顿时感觉欣喜若狂 我的设想是这样的，因为时间久远肯定不能再这个mysql上恢复到一个多月之前，我想把数据包下到本地电脑mysql，恢复数据，把这个表的dump下来，传到测试服mysql。然后数据包压缩完之后有3G多 下载限速1M大约一个小时。太久了。我索性在测试数据库服务器上 用ansible由装了一个mysql实例端口3309用时两分钟不到。 恢复数据 参考文档 然后用mysqlbinlog恢复这一个多月来的数据。 mysqlbinlog -d test_medical --stop-datetime=&apos;2017-08-03 17:57:05&apos; /home/mysql/3306/data/mysql-bin.000001 | mysql -uroot -p -P3309 -S /home/mysql/3309/mysql.sock mysqlbinlog -d test_medical --stop-datetime=&apos;2017-08-03 17:57:05&apos; /home/mysql/3306/data/mysql-bin.000002 | mysql -uroot -p -P3309 -S /home/mysql/3309/mysql.sock 备份表 mysqldump -uroot -p test_medical table_name &gt; table_name.sql 然后进到mysql source一下 结束。。 总结binlog是结合备份来用的，其实我们这次恢复也算是 备份—恢复流程 所以 数据库备份很重要， 数据库备份很重要， 数据库备份很重要。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL问题查找,状态查看]]></title>
    <url>%2F2017%2F08%2F02%2FMySQL%E9%97%AE%E9%A2%98%E6%9F%A5%E6%89%BE-%E7%8A%B6%E6%80%81%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[导火索: 今天后端上线，上线脚本执行到数据库更改的时候卡住了(公司开发的devops,web操作)，登录机器查看ansible进程是在的，证明SQL 执行出错了。之前我很少接触mysql，全是部署增删改查，多表查询都少用，所以这次是一边Google一边查找原因。 首先，这次数据库修改一共有四处，前三个都已经执行完成，就是最后一个加字段的没有成功。 排查过程 查看表大小，确认不是因为表大导致执行时间过长。 其实这个再上线操作的时候又应该有个大概的认识，比如这个表功能大概是多少行，属于哪个工程等等，如果对业务熟悉，应该很快判断的出。 相关命令 123456// 直接查找，可能较慢select count(id) from table_name; // 查找information_schema中的信息select * from information_schema.TABLES where information_schema.TABLES.TABLE_SCHEMA=&apos;databasename&apos; and information_schema.TABLES.TABLE_NAME=&apos;tablename&apos;\G 如果不是表太大，可以看下正在进行的事务 了解事务的工作模式很重要，首先要知道自己的MySQL隔离级别，及每种隔离级别的特点。参考美团点评团队的分享 我们是使用的 已提交读（Read committed） 1234// 查看当前隔离级别SELECT @@session.tx_isolation;// 查看当前执行的事务SELECT * FROM information_schema.INNODB_TRX; 当时主库加上字段之后，从库一直加不上字段就是因为有另一个事务一直没有提交，导致表一直是只读状态，阻塞了。 select * from information_schema.TABLES where information_schema.TABLES.TABLE_SCHEMA=’online_medical’ and information_schema.TABLES.TABLE_NAME=’api_familydoctorservice’\G]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx之请求限流限速问题]]></title>
    <url>%2F2017%2F07%2F20%2Fnginx%E4%B9%8B%E8%AF%B7%E6%B1%82%E9%99%90%E6%B5%81%E9%99%90%E9%80%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[单位时间按照IP地址限速工作当中经常需要按照IP地址限速，加白名单或者黑名单，这里面就会用到map来设置。看下边例子展示 123456789101112131415161718192021http &#123; geo $white_ip &#123; ranges; default 0; 127.0.0.1-127.0.0.1 1; 36.110.16.242-36.110.16.242 1 &#125; map $white_ip $white_ip_address &#123; 0 $binary_remote_addr; 1 &quot;&quot;; &#125; limit_req_zone $white_ip_address zone=:10m rate=20r/s; ....&#125; 解释下 上述通过geo模块 设定了白名单 ngx_http_geo_module 模块可以用来创建变量，其值依赖于客户端IP地址。语法: geo [$address] $variable { … } 设置在http 模块中[$address] 可以为空，使用默认变量也就是$remote_addr 其实例子中 geo $white_ip {} 就相当于geo $remote_addr $white_ip {}$white_ip 命名为white_ipranges 使用以地址段的形式定义地址，这个参数必须放在首位。为了加速装载地址库，地址应按升序定义。default 设置默认值，如果客户端地址不能匹配任意一个定义的地址，nginx将使用此值。 注：如果36.110.16.242 这个IP地址访问本站, white_ip 这个变量值就是 1,否则就是0 通过map模块 对$white_ip进行映射 ngx_http_map_module 模块可以创建变量，这些变量的值与另外的变量值相关联（上文的$white_ip）。允许分类或者同时映射多个值到多个不同值并储存到一个变量中，map指令用来创建变量，但是仅在变量被接受的时候执行视图映射操作，对于处理没有引用变量的请求时，这个模块并没有性能上的缺失。 语法: map $var1 $var2 { … } 上文map指令是将$white_ip值为0的，也就是受限制的IP，映射为客户端IP。将$white_ip值为1的，映射为空的字符串。limit_conn_zone和limit_req_zone指令对于键为空值的将会被忽略，从而实现对于列出来的IP不做限制。 limit_req_zone 真正操作限速ngx_http_limit_req_module 模块 下载限速1234567891011location /file &#123; limit_rate 128k;# 限制下载速度128K/s &#125; # 如果想设置用户下载文件的前10m大小时不限速，大于10m后再以128kb/s限速可以增加以下配内容，修改nginx.conf文件location /download &#123; limit_rate_after 10m; limit_rate 128k; &#125;]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket解读]]></title>
    <url>%2F2017%2F07%2F18%2Fsocket%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[前言 之前一直部署各种开源软件，包括很多RPC服务，其中很多都会有一个socket，但是很多地方可以用地址加端口替代，我虽然说做运维也将近两年了，但是毕竟大学是数学专业，不是计算机科班出身，对什么TCP/IP啊socket啊，都得是自己看书了解。so，抽个空补习了一下。 用廖雪峰老师的话说 Socket是网络编程的一个抽象概念。通常我们用一个Socket表示“打开了一个网络链接”，而打开一个Socket需要知道目标计算机的IP地址和端口号，再指定协议类型即可。 TCP/IP要理解socket首先要理解TCP/IP，面试过程当中我们经常被问到ISO七层模型相关的知识，类似TCP/IP工作在第几层？TCP/IP（Transmission Control Protocol/Internet Protocol）即传输控制协议/网间协议，定义了主机如何连入因特网及数据如何再它们之间传输的标准， 从字面意思来看TCP/IP是TCP和IP协议的合称，但实际上TCP/IP协议是指因特网整个TCP/IP协议簇。不同于ISO模型的七个分层，TCP/IP协议参考模型把所有的TCP/IP系列协议归类到四个抽象层中 应用层：TFTP，HTTP，SNMP，FTP，SMTP，DNS，Telnet 等等 传输层：TCP，UDP 网络层：IP，ICMP，OSPF，EIGRP，IGMP 数据链路层：SLIP，CSLIP，PPP，MTU 每一层都是建立在下一层提供的服务上，为上一层提供服务 python socket客户端编写(down 新浪首页)12345678910111213141516171819202122# -*- coding:utf-8 -*-import sockets = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.connect((&apos;www.sina.com&apos;,80))s.send(&apos;GET / HTTP/1.1\r\nHost: www.sina.com.cn\r\nConnection: close\r\n\r\n&apos;)buffer = []while True: d = s.recv(2048) if d: buffer.append(d) else: breakdata=&apos;&apos;.join(buffer)s.closeheader, html = data.split(&apos;\r\n\r\n&apos;,1)print header server端编写]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL使用记录]]></title>
    <url>%2F2017%2F07%2F13%2FMySQL%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[数据库现在为止我只接触过MySQL，MySQL自从被oracle收购后，就衍生出三大阵营，Drizzle、MariaDB和Percona Server（包括XtraDB引擎），第一个没接触过不太了解，MariaDB与percona又有很多相似点，比如都支持XtraDB（innodb加强版）看看两者优缺点： Percona团队的最终声明是“Percona Server是由Oracle发布的最接近官方MySQL Enterprise发行版的版本”，因此与其他更改了大量基本核心MySQL代码的分支有所区别。 XtraDB 存储引擎是完全的向下兼容，在 MariaDB 中，XtraDB 存储引擎被标识为”ENGINE=InnoDB”，这个与 InnoDB 是一样的，所以你可以直接用XtraDB 替换掉 InnoDB 而不会产生任何问题。 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。在存储引擎方面，10.0.9版起使用XtraDB（名称代号为Aria）来代替MySQL的InnoDB。 Percona Server的一个缺点是他们自己管理代码，不接受外部开发人员的贡献，以这种方式确保他们对产品中所包含功能的控制。 总之，二者目前为止没有拉开什么差距，所以可以自由选择。阿里使用的percona 但是Google使用的MariaDB。。。 我也是用的percona。下边以它为例说明。 数据库也是个比较大的技术栈，三言两语说不完，工具书都特别厚，我写这篇文章的目的无非就是想给自己一个查看SQL语句的地方，见识比较少，有错误我肯定不是故意的，全文参考这个技能图谱 安装部署直接yum就可以，国内有清华源，中科院源，比较快，官网下载简直是灾难。可以写ansible-playbook，没有难点不多说。 配置文件详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174[mysqld3306]# 为PXC定制binlog_format=ROWdefault-storage-engine = InnoDBinnodb_autoinc_lock_mode=2# 绑定地址之后只能通过内部网络访问bind-address=10.******auto-increment-increment = 2auto-increment-offset = 1# 使用 xtrabackup-v2 必须制定datadir, 因为操作可能直接对 datadir进行datadir=/home/mysql/3306/data# http://www.percona.com/doc/percona-xtrabackup/2.2/innobackupex/privileges.html#permissions-and-privileges-needed# 权限的配置# xtrapbackup在Donor上执行，因此只需localhost的权限# 约定:server-id = 1# wsrep模式不依赖于GTID# 开启GTID# enforce_gtid_consistency=1# gtid_mode=on# 即便临时作为Slave，也记录自己的binloglog-slave-updates=1# master_info_repository=TABLE# relay_log_info_repository=TABLE# GENERAL #user = mysqlport = 3306socket = /home/mysql/3306/mysql.sockpid-file = /home/mysql/3306/mysql.pid# MyISAM #key-buffer-size = 32Mmyisam-recover = FORCE,BACKUPft-min-word-len = 4event-scheduler = 0# SAFETY #max-allowed-packet = 16Mskip-name-resolvemax_connections = 2000max_connect_errors = 30back-log = 500character-set-client-handshake = 1character-set-server = utf8mb4collation-server = utf8mb4_unicode_ci#character-set-client-handshake=1#character-set-client=utf8#character-set-server=utf8#collation-server=utf8_general_ci#key-buffer-size = 256Mtable-open-cache = 2048max-allowed-packet = 2048Mslave-skip-errors = all #Skip duplicated keysort-buffer-size = 4Mjoin-buffer-size = 8Mthread-cache-size = 50concurrent-insert = 2thread-stack = 192Knet-buffer-length = 8Kread-buffer-size = 256Kread-rnd-buffer-size = 16Mbulk-insert-buffer-size = 64M# 采用thread pool来处理连接thread_handling=pool-of-threadssql-mode = STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_AUTO_VALUE_ON_ZERO,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BYsysdate-is-now = 1innodb = FORCEinnodb-strict-mode = 1# BINARY LOGGING #log-bin = /home/mysql/3306/data/mysql-binexpire-logs-days = 5# LOGGING ## log-output=file 默认就是FILElog-error = /home/mysql/3306/data/mysql-error.loglong-query-time = 0.3# log-queries-not-using-indexes = 1slow-query-log = 1slow-query-log-file = /home/mysql/3306/data/mysql-slow.log# 默认为0(MySQL不控制binlog的输出)# sync-binlog = 1# CACHES AND LIMITS #tmp-table-size = 32Mmax-heap-table-size = 32M# 频繁修改的表不适合做query-cache, 否则反而影响效率query-cache-type = 0query-cache-size = 0# query-cache-limit = 2M# query-cache-min-res-unit = 512thread-cache-size = 100open-files-limit = 65535table-definition-cache = 1024table-open-cache = 4096# INNODB #innodb-flush-method = O_DIRECTinnodb-log-files-in-group = 2# innodb-file-per-table = 1设置之后， 下面的配置基本失效innodb_data_file_path = ibdata1:10M:autoextendinnodb-thread-concurrency = 32innodb-log-file-size = 256Minnodb-flush-log-at-trx-commit = 2innodb-file-per-table = 1# 内存: 全部内存*0.7innodb-buffer-pool-size = 25Gperformance-schema = 0net-read-timeout = 60# innodb-open-files 在MySQL5.6 auto-sized# 来自May2innodb-rollback-on-timeoutinnodb-status-file = 1# http://dev.mysql.com/doc/refman/5.6/en/innodb-performance-multiple_io_threads.html# http://zhan.renren.com/formysql?tagId=3942&amp;checked=true# 从MySQL 5.5# innodb_file_io_threads = 4innodb-read-io-threads = 16innodb-write-io-threads = 8innodb-io-capacity = 2000# innodb-stats-update-need-lock = 0 # MySQL 5.6中无效# innodb-stats-auto-update = 0innodb-old-blocks-pct = 75# innodb-adaptive-flushing-method = &quot;estimate&quot;# innodb-adaptive-flushing = 1# https://www.facebook.com/notes/mysql-at-facebook/repeatable-read-versus-read-committed-for-innodb/244956410932# READ-COMMITTED 每次QUERY都要求调用: read_view_open_now, 而REPEATABLE-READ每次Transaction中只要求一次# REPEATABLE-READ 会导致读写不同步transaction-isolation = READ-COMMITTEDinnodb-sync-spin-loops = 100innodb-spin-wait-delay = 30innodb-file-format = &quot;Barracuda&quot;innodb-file-format-max = &quot;Barracuda&quot; 基本操作 日常操作无非就是增删改查。也可分为库层面操作，与表层面操作。 库操作一般库操作很少，没有什么花样12345678910# 创建数据库create database online_database; # 查看数据库show databases; # 查看数据库信息 show create database online_database;# 修改数据库的编码，可使用上一条语句查看是否修改成功alter database online_database default character set gbk collate gbk_bin; # 删除数据库drop database online_database;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[uwsgi笔记]]></title>
    <url>%2F2017%2F07%2F13%2Fuwsgi%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言，来公司一年了，对uwsgi的操作，只停留在部署，或者restart，并没有主动配置过，今天看nginx的时候看到nginx(我们的nginx是只转发)的请求是转发的本地某个端口，是个uwsgi提供的fastrouter服务。在此记录一下。 由于我们后端使用的都是Django项目，所以整体都是这个架构nginx–&gt;uwsgi–&gt;django.所以对uwsgi的了解也很重要。fastrouter使用放到后面先说，uwsgi的普通配置。 版本：uWSGI==2.0.12 简介WSGI（Python Web Server Gateway Interface，缩写为WSGI） 是一种 Web 服务器网关接口。它是一个 Web 服务器（如 Nginx）与应用服务器（如 uWSGI 服务器）通信的一种规范。uwsgi 是一种协议uWSGI uWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。uWSGI，既不用wsgi协议也不用FastCGI协议，而是自创了一个uwsgi的协议，uwsgi协议是一个uWSGI服务器自有的协议，它用于定义传输信息的类型（type of information），每一个uwsgi packet前4byte为传输信息类型描述，它与WSGI相比是两样东西。据说该协议大约是fcgi协议的10倍那么快。 优点： 超快的性能。 低内存占用（实测为apache2的mod_wsgi的一半左右）。 多app管理。 详尽的日志功能（可以用来分析app性能和瓶颈）。 高度可定制（内存大小限制，服务一定次数后重启等）。 版本：uWSGI==2.0.12 安装配置安装就是直接在virtualenv中pip安装就好了pip install uWSGI uwsgitop 监控工具 部署pip install uwsgitop 使用uwsgitop /tmp/stats.sock plugins使用fastrouter这是一个负载均衡插件，比如说四个uwsgi节点提供服务，这样在nginx上面可以配置成upstream，分流到四个uwsgi服务上，但是如果是上线，或者有一个节点挂掉了怎么办，只能是收到500报警在手动剔除吗？ to yung to sample!! 当然不是，这个时候就用到fastrouter了。看官网 For advanced setups uWSGI includes the “fastrouter” plugin, a proxy/load-balancer/router speaking the uwsgi protocol. It is built in by default. You can put it between your webserver and real uWSGI instances to have more control over the routing of HTTP requests to your application servers. 它的功能proxy/load-banlance/router简述一下配置： nginx配置1234location /test &#123; include uwsgi_params; uwsgi_pass 127.0.0.1:3030; &#125; fastrouter-server端配置1234567&lt;uwsgi id = &quot;fastrouter&quot;&gt; &lt;fastrouter&gt;127.0.0.1:3030&lt;/fastrouter&gt; &lt;fastrouter-subscription-server&gt;127.0.0.1:3131&lt;/fastrouter-subscription-server&gt; &lt;enable-threads/&gt; &lt;master/&gt; &lt;fastrouter-stats&gt;127.0.0.1:9595&lt;/fastrouter-stats&gt;&lt;/uwsgi&gt; 3030为当前uWSGI fastrouter server的端口，前面的空代表当前主机地址。（nginx会用到这个端口） 3131fastrouter-subscription-server 表示当前uWSGI fastrouter server的订阅地址。（web应用服务会用到） stats：uWSGI的统计服务机制，访问会返回一个json对象，都是状态统计信息 格式可以是一个端口，也可以是一个socket uwsgi实例配置 实例11234567891011&lt;uwsgi id = &quot;subserver1&quot;&gt; &lt;stats&gt;127.0.0.1:9393&lt;/stats&gt; &lt;processes&gt;4&lt;/processes&gt; &lt;enable-threads/&gt; &lt;memory-report/&gt; &lt;subscribe-to&gt;127.0.0.1:3131:test&lt;/subscribe-to&gt; &lt;socket&gt;127.0.0.1:3232&lt;/socket&gt; &lt;file&gt;./server.py&lt;/file&gt; &lt;master/&gt; &lt;weight&gt;8&lt;/weight&gt;&lt;/uwsgi&gt; 实例21234567891011&lt;uwsgi id = &quot;subserver2&quot;&gt; &lt;stats&gt;127.0.0.1:9494&lt;/stats&gt; &lt;processes&gt;4&lt;/processes&gt; &lt;enable-threads/&gt; &lt;memory-report/&gt; &lt;subscribe-to&gt;127.0.0.1:3131:test&lt;/subscribe-to&gt; &lt;socket&gt;127.0.0.1:3333&lt;/socket&gt; &lt;file&gt;./server.py&lt;/file&gt; &lt;master/&gt; &lt;weight&gt;2&lt;/weight&gt;&lt;/uwsgi&gt; 127.0.0.1:9494 这个可以把9494改为0 则为自动分配 也可以改成socket 我们通过subscribe-to变量来订阅fastrouter server（127.0.0.1:3131）,冒号后跟着的是对应请求的域名，只有来自当前域名的请求才会进入当前web节点。当然这个可以设置多个subscribe-to，例如：subscribe-to=127.0.0.1:3131:test1 weight 权重分配 由于我们线上使用的是fastrouter,所以在这儿就只说了这个，其实uwsgi的负载均衡使用不只有这一种手段有兴趣可以看下这篇博文 harakiri]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>uwsgi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录主机history]]></title>
    <url>%2F2017%2F07%2F12%2F%E8%AE%B0%E5%BD%95%E4%B8%BB%E6%9C%BAhistory%2F</url>
    <content type="text"><![CDATA[起因，有个哥们儿要离职，直接上线上把他机器训练的东西拷贝到电脑本地，我们用的vpn，有些服务对此有点依赖，不详细说，总之影响到了一丢丢线上的情况，所以CTO很不高兴，机器历史记录也没有，啥都没有，多亏他承认了。但是我这个运维还是多少显得有点尴尬。 下面说一些改进措施 不准任何人直接登录线上机器，必须通过跳板机（上传现在文件也必须通过跳板机加以控制） openvpn与线上带宽解耦 openvpn限速，线上加iptables阻止openvpn的地址，只接受跳板机(加一条运维通道，永远有B方案) 历史记录需要详细记录 前三条很好就解决了。下面内容详细记录下第四条的实现方法。 bash是多数Linux发行版默认的shell，虽然不及zsh好用，但比其它的shell好太多。我们的生产服务器很多，没有用跳板机，又是多人共用root用户，为了审计用户操作，需要记录执行命令的用户、时间和ip等信息。本文之所以要优化，主要是因为bash默认配置存在以下几点不足： 历史记录保存数目有限，默认1000条 记录不详细，不记录命令执行时间/执行用户名/用户ip等 历史记录会丢失，主要有两种情况： bash异常退出 同一用户多处登录或开了多个会话，只会记录最后退出的会话历史 所以我决心自己记录下bash_history 并把它写到ES之中传送门,这样有人做了什么非法的事情，即使他清空了历史记录我的ES中也能存着他的罪证，除非他每条history都秒删，在速度上超过filebeat的读取的速度，事实证明不怎么可能。 常规rsyslog实现 参考链接 配置全局bash历史记录格式在/etc/bashrc中写入1export PROMPT_COMMAND=&apos;RETRN_VAL=$?;logger -p local6.debug &quot;$(who am i) [$$]: $(history 1 | sed &quot;s/^[ ]*[0-9]\+[ ]*//&quot; ) [$RETRN_VAL]&quot;&apos; 配置rsyslog新增文件/etc/rsyslog.d/bash.conf,内容1local6.* /var/log/bash_history.log 重启rsyslogd1systemctl restart rsyslogd ansible 脚本123456789101112131415161718192021222324252627---#- name: add scripts to bashrc# lineinfile:# dest=/etc/bashrc# line=&#123;&#123;item&#125;&#125;# with_items: &apos;&#123;&#123;bashrc_line&#125;&#125;&apos;# register: profile- name: copy file to /etc copy: src=bash_log.conf dest=/var/tmp- name: echo to /etc/bashrc shell: cat /var/tmp/bash_log.conf &gt;&gt; /etc/bashrc register: bashrc- name: source file shell: source /etc/bashrc when: bashrc.changed- name: copy bash.conf to /etc/rsyslog.d copy: src=bash.conf dest=/etc/rsyslog.d register: rsyslog_conf- name: restart rsyslog service: name=rsyslog.service state=restarted when: rsyslog_conf.changed 刷到每个机器上就好了 注意最好配置下logrotate每天切割一下 这样就可以在kibana中看到每个登录人员的操作情况了。 查看用户痕迹过程展示上面中控机上是每个人对应一个自己名字拼音的用户，使用此用户跳到线上机器，但是测试环境是直接本地可以连，测试被人搞坏了进度delay怎么办？也需要查证，可以使用以下方法]]></content>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过ucloud_API查看资源价格]]></title>
    <url>%2F2017%2F07%2F11%2Fucloud-API%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[我们公司有些服务使用ucloud主机，付费等问题都是我负责，前一段时间跟他们的架构沟通了一下，发现他们的API还是很方便的，尤其他们提供弹性IP可以自由伸缩，我们完全可以写个脚本统计带宽，实时调整。如果在云上跑docker，完全可以直接通过流量挥着访问量实时业务扩容。运维真的做到自动化。 ucloud提供了官方的`apk`[链接](https://github.com/ucloud-web/python-sdk-v2.git) 有的同学直接把它写成了python命令 链接 本来只是想做一个统计价格的工具，写着写着，就像反正收集实例信息还不如都挨个统计一下展示出来，因为： ucloud信息展示的并不清晰 如果看到价格有异常肯定第一时间想知道到底哪个贵些，贵在哪里，更方便直观一些 我在下边主要展示一下自己手写的几个脚本通过API获取实例信息，再通过实例信息获取实例价格，最后统一发送到influxDB中去到grafana上呈现。定时每天跑一次。代码水平有限，大家请不要嘲笑。 先来看下他的apk和配置文件他的public_key 和 private_key 涉及到自己独特的加秘方式。可以去ucloud官网看看 apk.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# -*- coding: utf-8 -*-import hashlib, json, httplibimport urlparseimport urllibimport sysfrom config import *class UCLOUDException(Exception): def __str__(self): return &quot;Error&quot;def _verfy_ac(private_key, params): items = params.items() items.sort() params_data = &quot;&quot; for key, value in items: params_data = params_data + str(key) + str(value) params_data = params_data+private_key &apos;&apos;&apos;use sha1 to encode keys&apos;&apos;&apos; hash_new = hashlib.sha1() hash_new.update(params_data) hash_value = hash_new.hexdigest() return hash_valueclass UConnection(object): def __init__(self, base_url): self.base_url = base_url o = urlparse.urlsplit(base_url) if o.scheme == &apos;https&apos;: self.conn = httplib.HTTPSConnection(o.netloc) else: self.conn = httplib.HTTPConnection(o.netloc) def __del__(self): self.conn.close() def get(self, resouse, params): resouse += &quot;?&quot; + urllib.urlencode(params) print(&quot;%s%s&quot; % (self.base_url, resouse)) self.conn.request(&quot;GET&quot;, resouse) response = json.loads(self.conn.getresponse().read()) return responseclass UcloudApiClient(object): # 添加 设置 数据中心和 zone 参数 def __init__(self, base_url, public_key, private_key): self.g_params = &#123;&#125; self.g_params[&apos;PublicKey&apos;] = public_key self.private_key = private_keyurl self.conn = UConnection(base_url) def get(self, uri, params): # print params _params = dict(self.g_params, **params) if project_id : _params[&quot;ProjectId&quot;] = project_id _params[&quot;Signature&quot;] = _verfy_ac(self.private_key, _params) return self.conn.get(uri, _params) conf.py1234567#-*- encoding: utf-8 -*-#配置公私钥&quot;&quot;&quot;public_key = &quot;at************************&quot;private_key = &quot;e7***************&quot;#project_id = &quot;******&quot; # 项目ID 请在Dashbord 上获取base_url = &quot;https://api.ucloud.cn&quot; collect_uhost_price.py 通过查看机器示例，按付费方式查看机器价格。具体查看ucloud uhost API123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#!/usr/bin/python#-*- coding:utf-8 -*-# author:fanquanqing# collect ucloud uhost pricefrom sdk import UcloudApiClientfrom config import *from collections import Iterableimport sysimport json#host_list = []# 收集uhost信息def collect_uhost_info(Regions_list,ProjectId): &apos;&apos;&apos; 包含 hostname, ip, region, cpu, mem, diskspace, chargetype, count, ImageId, osname. &apos;&apos;&apos; host_info_list = [] for Region in Regions_list: ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123; &quot;Action&quot;:&quot;DescribeUHostInstance&quot;, &quot;Region&quot;:Region, &quot;ProjectId&quot;:ProjectId, &quot;Limit&quot;:&quot;1000&quot; &#125; response = ApiClient.get(&quot;/&quot;, Parameters); length = response[&apos;TotalCount&apos;] for i in range(length): host_info = &#123;&#125; #ImageId = response[&quot;UHostSet&quot;][i][&quot;BasicImageId&quot;].encode(&quot;utf-8&quot;) ChargeType = response[&quot;UHostSet&quot;][i][&quot;ChargeType&quot;].encode(&quot;utf-8&quot;) #host_info[&quot;Action&quot;] = &quot;GetUHostInstancePrice&quot; host_info[&quot;Region&quot;] = Region host_info[&quot;ImageId&quot;] = &quot;uimage-kg0w4u&quot; host_info[&quot;Hostname&quot;] = response[&quot;UHostSet&quot;][i][&quot;Name&quot;] host_info[&quot;CPU&quot;] = response[&quot;UHostSet&quot;][i][&quot;CPU&quot;] host_info[&quot;Memory&quot;] = response[&quot;UHostSet&quot;][i][&quot;Memory&quot;] if len(response[&quot;UHostSet&quot;][i][&quot;DiskSet&quot;]) &gt; 1: host_info[&quot;DiskSpace&quot;] = response[&quot;UHostSet&quot;][i][&quot;DiskSet&quot;][1][&quot;Size&quot;] else: host_info[&quot;DiskSpace&quot;]=0 host_info[&quot;Count&quot;] = 1 host_info[&quot;ChargeType&quot;] = ChargeType host_info[&quot;OsName&quot;] = response[&quot;UHostSet&quot;][i][&quot;OsName&quot;].split()[0] host_info[&quot;loaclIP&quot;] = response[&quot;UHostSet&quot;][i][&quot;IPSet&quot;][0][&quot;IP&quot;] if len(response[&quot;UHostSet&quot;][i][&quot;IPSet&quot;])&gt;1: host_info[&quot;EIP&quot;]=response[&quot;UHostSet&quot;][i][&quot;IPSet&quot;][1][&quot;IP&quot;] else: host_info[&quot;EIP&quot;]=&quot;none&quot; host_info_list.append(host_info) #print host_info_list return host_info_list#通过机器配置得到某一台机器价格def get_uhost_price(host_instance_info): ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters= host_instance_info response = ApiClient.get(&quot;/&quot;, Parameters );# print json.dumps(response, sort_keys=True, indent=4, separators=(&apos;,&apos;, &apos;: &apos;)) price = float(response[&quot;PriceSet&quot;][0].values()[0]) return price#通过机器配置信息得到所有机器价格def get_all_uhost_price(host_info_list): for host_info in host_info_list: host_params=&#123;&#125; host_params[&quot;Action&quot;]=&quot;GetUHostInstancePrice&quot; host_params[&quot;ImageId&quot;]=host_info[&quot;ImageId&quot;] host_params[&quot;CPU&quot;]=host_info[&quot;CPU&quot;] host_params[&quot;Memory&quot;]=host_info[&quot;Memory&quot;] host_params[&quot;Count&quot;]=host_info[&quot;Count&quot;] host_params[&quot;DiskSpace&quot;]=host_info[&quot;DiskSpace&quot;] host_params[&quot;Region&quot;]=&quot;cn-bj2&quot; host_params[&quot;ChargeType&quot;]=host_info[&quot;ChargeType&quot;] if host_params[&quot;ChargeType&quot;]==&quot;Year&quot;: price = get_uhost_price(host_params) elif host_params[&quot;ChargeType&quot;]==&quot;Month&quot;: price = get_uhost_price(host_params)*12 else: price=0 print &quot;有临时机器请排查。&quot; host_info[&quot;Price&quot;]=price return host_info_listdef collect_uhost_price(setting_info): host_info_total = [] for Projectname in setting_info: for ProjectId in setting_info[Projectname]: Regions_list = setting_info[Projectname][ProjectId] host_info_list = collect_uhost_info(Regions_list,ProjectId) project_host_list = get_all_uhost_price(host_info_list) host_info_total.extend(project_host_list) return host_info_totalif __name__ == &apos;__main__&apos;: host_info_list = collect_uhost_info([&apos;cn-bj2&apos;,&apos;hk&apos;],&quot;org-oddm1w&quot;) host_price_list = get_all_uhost_price(host_info_list) #print host_price_list collect_eip_price.py 收集弹性IP信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/env python# -*- coding:utf-8 -*-# author:fanquanqing# collect ucloud eip pricefrom sdk import UcloudApiClientfrom config import *import sysimport json# 得到所有的EIP实例信息def get_eip_instance(Regions_list,ProjectId): eip_all = [] for Region in Regions_list: ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123;&quot;Action&quot;:&quot;DescribeEIP&quot;, &quot;Region&quot;:Region, &quot;ProjectId&quot;:ProjectId&#125; response = ApiClient.get(&quot;/&quot;, Parameters ); for eip in response[&apos;EIPSet&apos;]: eip_instance = &#123;&#125; # 地域 eip_instance[&apos;Region&apos;]=Region # IP eip_instance[&apos;IP&apos;]=eip[&apos;EIPAddr&apos;][0][&apos;IP&apos;] # 运营商线路 eip_instance[&apos;OperatorName&apos;]=eip[&apos;EIPAddr&apos;][0][&apos;OperatorName&apos;].encode(&apos;utf-8&apos;) # 带宽 eip_instance[&apos;Bandwidth&apos;]=eip[&apos;Bandwidth&apos;] # 付费周期 eip_instance[&apos;ChargeType&apos;]=eip[&apos;ChargeType&apos;].encode(&apos;utf-8&apos;) # 付费方式(是否绑定共享带宽) eip_instance[&apos;PayMode&apos;]=eip[&apos;PayMode&apos;].encode(&apos;utf-8&apos;) eip_all.append(eip_instance) return eip_all #print json.dumps(response, sort_keys=True, indent=4, separators=(&apos;,&apos;, &apos;: &apos;))# 查看单个EIP实例价格def get_eip_price(eip): ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=eip response = ApiClient.get(&quot;/&quot;, Parameters ); #print response price = response[&apos;PriceSet&apos;][0][&apos;Price&apos;] return price# 获取所有EIP价格def get_all_eip_price(eip_all): for eip_info in eip_all: eip_params=&#123;&#125; eip_params[&apos;Action&apos;]=&quot;GetEIPPrice&quot; eip_params[&apos;Region&apos;]=eip_info[&apos;Region&apos;] eip_params[&apos;OperatorName&apos;]=eip_info[&apos;OperatorName&apos;] eip_params[&apos;Bandwidth&apos;]=eip_info[&apos;Bandwidth&apos;] eip_params[&apos;ChargeType&apos;]=eip_info[&apos;ChargeType&apos;] eip_params[&apos;PayMode&apos;]=eip_info[&apos;PayMode&apos;] if eip_params[&apos;ChargeType&apos;]==&quot;Year&quot;: price = get_eip_price(eip_params) elif eip_params[&apos;ChargeType&apos;]==&quot;Month&quot;: price = get_eip_price(eip_params)*12 else: price=0 print &quot;有临时EIP请排查&quot; eip_info[&quot;Price&quot;]=price return eip_alldef collect_eip_price(setting_info): eip_info_total = [] for Projectname in setting_info: for ProjectId in setting_info[Projectname]: Regions_list = setting_info[Projectname][ProjectId] eip_all=get_eip_instance(Regions_list,ProjectId) price_all=get_all_eip_price(eip_all) eip_info_total.extend(price_all) #print eip_info_total return eip_info_totalif __name__ == &apos;__main__&apos;: collect_eip_price(&#123;&quot;chunyu&quot;:&#123;&quot;org-oddm1w&quot;:[&apos;cn-bj2&apos;,&apos;hk&apos;]&#125;, &quot;uhs&quot;:&#123;&quot;org-shbbct&quot;:[&quot;cn-bj2&quot;]&#125;&#125;) collect_udisk_price.py 收集云硬盘信息，这个只是取到实例自己手动算的价格123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#!/usr/bin/env python# -*- coding:utf-8 -*-# author: fanquanqing# 收集ucloud云硬盘信息 及价格from sdk import UcloudApiClientfrom config import *import sysimport json# 获取udisk 信息def get_udisk_info(Regions_list,ProjectId): udisk_list = [] for Region in Regions_list: ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123;&quot;Action&quot;:&quot;DescribeUDisk&quot;, &quot;Region&quot;:Region, &quot;ProjectId&quot;:ProjectId&#125; response = ApiClient.get(&quot;/&quot;, Parameters ); for udisk in response[&apos;DataSet&apos;]: udisk_dic = &#123;&#125; udisk_dic[&apos;Region&apos;] = Region #udisk_dic[&apos;Action&apos;] = &quot;DescribeUDiskPrice&quot; udisk_dic[&apos;Size&apos;] = udisk[&apos;Size&apos;] udisk_dic[&apos;ChargeType&apos;] = udisk[&apos;ChargeType&apos;].encode(&apos;utf-8&apos;) udisk_dic[&apos;UHostName&apos;] = udisk[&apos;UHostName&apos;] udisk_dic[&apos;Quantity&apos;] = 1 udisk_dic[&apos;Zone&apos;] = &quot;cn-bj2-02&quot; udisk_list.append(udisk_dic) return udisk_list# 通过udisk信息获取价格def get_udisk_price(udisk_list): for udisk in udisk_list: ApiClient = UcloudApiClient(base_url, public_key, private_key) udisk_params=&#123;&#125; udisk_params[&apos;Action&apos;]=&quot;DescribeUDiskPrice&quot; udisk_params[&apos;Region&apos;]=udisk[&apos;Region&apos;] udisk_params[&apos;Size&apos;]=udisk[&apos;Size&apos;] udisk_params[&apos;ChargeType&apos;]=udisk[&apos;ChargeType&apos;] udisk_params[&apos;Quantity&apos;]=udisk[&apos;Quantity&apos;] udisk_params[&apos;Zone&apos;]=udisk[&apos;Zone&apos;] Parameters = udisk_params response = ApiClient.get(&quot;/&quot;, Parameters ); if udisk_params[&apos;ChargeType&apos;]==&quot;Year&quot;: price = response[&apos;DataSet&apos;][0][&apos;Price&apos;]/100 elif udisk_params[&apos;ChargeType&apos;]==&quot;Month&quot;: price = response[&apos;DataSet&apos;][0][&apos;Price&apos;]/10 else: print &quot;有付费方式异常的云硬盘，请排查。&quot; udisk[&quot;Price&quot;]=price return udisk_list# 通过付费周期获取udisk价格def collect_udisk_price(setting_info): udisk_price_info = [] for Projectname in setting_info: for ProjectId in setting_info[Projectname]: Regions_list = setting_info[Projectname][ProjectId] udisk_list = get_udisk_info(Regions_list,ProjectId) udisk_price_info.extend(get_udisk_price(udisk_list)) return udisk_price_infoif __name__ == &apos;__main__&apos;: collect_udisk_price(&#123;&quot;chunyu&quot;:&#123;&quot;org-oddm1w&quot;:[&apos;cn-bj2&apos;,&apos;hk&apos;]&#125;, &quot;uhs&quot;:&#123;&quot;org-shbbct&quot;:[&quot;cn-bj2&quot;]&#125;&#125;) collect_sharebandwidth_price.py 收集共享带宽信息,因为没有计算价格的API这个价格也是手动算的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/usr/bin/python# -*- coding: utf-8 -*-#author fanquanqing#收集共享带宽信息获取每年消费情况from sdk import UcloudApiClientfrom config import *import sysimport jsondef get_bandwidth_info(Regions_list,ProjectId): bandwidth = [] for Region in Regions_list: ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123;&quot;Action&quot;:&quot;DescribeShareBandwidth&quot;,&quot;Region&quot;:Region,&quot;ProjectId&quot;:ProjectId&#125; response = ApiClient.get(&quot;/&quot;, Parameters ); #print response for bw in response[&apos;DataSet&apos;]: bw_instance = &#123;&#125; bw_instance[&apos;ShareBandwidth&apos;] = bw[&apos;ShareBandwidth&apos;] bw_instance[&apos;ChargeType&apos;] = bw[&apos;ChargeType&apos;] bw_instance[&apos;Name&apos;]=bw[&apos;Name&apos;] bandwidth.append(bw_instance) return bandwidth# print json.dumps(response, sort_keys=True, indent=4, separators=(&apos;,&apos;, &apos;: &apos;))def get_price(bandwidth): &apos;&apos;&apos; 价格计算说明:ucloud没有提供API查询共享带宽价格，所有的共享带宽价格都是90/M/月 月付*12,年付*10 &apos;&apos;&apos; price_list = [] for bw in bandwidth: if bw[&apos;ChargeType&apos;]==&quot;Month&quot;: price = bw[&apos;ShareBandwidth&apos;]*90*12 bw[&quot;Price&quot;]=price else: price = bw[&apos;ShareBandwidrh&apos;] bw[&quot;Price&quot;]=price return bandwidthdef collect_sharebandwidth_price(setting_info): bw_price_info = [] for Projectname in setting_info: for ProjectId in setting_info[Projectname]: Regions_list = setting_info[Projectname][ProjectId] bw_info = get_bandwidth_info(Regions_list,ProjectId) bw_price_info.extend(get_price(bw_info)) #print bw_price_info return bw_price_infoif __name__ == &apos;__main__&apos;: collect_sharebandwidth_price(&#123;&quot;chunyu&quot;:&#123;&quot;org-oddm1w&quot;:[&apos;cn-bj2&apos;,&apos;hk&apos;]&#125;, &quot;uhs&quot;:&#123;&quot;org-shbbct&quot;:[&quot;cn-bj2&quot;]&#125;&#125;) get_all_price.py 给各实例价格求和，发送到influxdb。可以每天跑一下cron更新下内容。里面发送的influxDB是事先封装好的包直接导入的，并不是官方包1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#!/usr/bin/env python# -*- coding:utf-8 -*-# author: fanquanqing#import datatimefrom collect_eip_price import collect_eip_pricefrom collect_uhost_price import collect_uhost_pricefrom collect_sharebandwidth_price import collect_sharebandwidth_pricefrom collect_udisk_price import collect_udisk_pricefrom op_tools.api_influxdb import write_data_to_influxdbdef get_price(setting_info): &apos;&apos;&apos; 各个实例price求和,并把实例信息发送到influxdb &apos;&apos;&apos; eip_price=0 uhost_price=0 sharebw_price=0 udisk_price=0 eip_price_info_list = collect_eip_price(setting_info) for eip_info in eip_price_info_list: eip_headers = eip_info.keys() eip_rows = [] eip_rows.append(eip_info.values()) #print eip_headers, eip_rows # 发送EIP数据到influxdb write_data_to_influxdb(&apos;EIP_info_daliy&apos;, eip_headers, eip_rows, [&apos;IP&apos;, &apos;OperatorName&apos;, &apos;ChargeType&apos;]) eip_price += round(eip_info[&apos;Price&apos;]) uhost_price_info_list = collect_uhost_price(setting_info) for uhost_info in uhost_price_info_list: uhost_headers=uhost_info.keys() uhost_rows=[] uhost_rows.append(uhost_info.values()) #print uhost_headers, uhost_rows # 发送云主机信息到influxdb write_data_to_influxdb(&apos;Uhost_info_daliy&apos;, uhost_headers, uhost_rows, [&apos;Hostname&apos;,&apos;ChargeType&apos;]) uhost_price += round(uhost_info[&apos;Price&apos;]) sharebandwidth_info_list = collect_sharebandwidth_price(setting_info) for sharebandwidth_info in sharebandwidth_info_list: sharebw_headers=sharebandwidth_info.keys() sharebw_rows=[] sharebw_rows.append(sharebandwidth_info.values()) #print sharebw_headers, sharebw_rows # 发送共享带宽信息到influxdb write_data_to_influxdb(&apos;ShareBandwidth_info_daliy&apos;,sharebw_headers,sharebw_rows,[]) sharebw_price += round(sharebandwidth_info[&apos;Price&apos;]) udisk_info_list=collect_udisk_price(setting_info) for udisk_info in udisk_info_list: udisk_headers=udisk_info.keys() udisk_rows=[] udisk_rows.append(udisk_info.values()) #print udisk_headers, udisk_rows write_data_to_influxdb(&apos;Udisk_info_daliy&apos;, udisk_headers,udisk_rows,[&apos;UHostName&apos;,&apos;Region&apos;]) udisk_price += round(udisk_info[&apos;Price&apos;]) # 托管机房价格 physical_price = get_physical_host_price() total_price=eip_price+uhost_price+sharebw_price+udisk_price+physical_price # 价格列表 price_list=[eip_price,uhost_price,sharebw_price,udisk_price,total_price] price_headers=[&apos;eip_price&apos;,&apos;uhost_price&apos;,&apos;sharebw_price&apos;,&apos;udisk_price&apos;,&apos;total_price&apos;] price_rows=[] price_rows.append(price_list) write_data_to_influxdb(&apos;Ucloud_price_total_daliy&apos;,price_headers,price_rows,[]) #print uhost_price return uhost_price# 托管机器价格(两个机柜一个10M外网)def get_physical_host_price(): host_price = 9000*2*12 tg_cloud_switch_port_price = (288*2+217)*12 tg_bandwidth_price = 10*90*12 physical_price = host_price+tg_bandwidth_price+tg_cloud_switch_port_price return physical_priceif __name__ == &apos;__main__&apos;: # 可用区列表 #Regions_list = [&apos;cn-bj2&apos;,&apos;hk&apos;] # 项目ID列表 #Project_Id_list = [&apos;org-shbbct&apos;,&apos;org-oddm1w&apos;] # 项目ID对应关系 setting_info = &#123;&quot;chunyu&quot;:&#123;&quot;org-oddm1w&quot;:[&apos;cn-bj2&apos;,&apos;hk&apos;]&#125;, &quot;uhs&quot;:&#123;&quot;org-shbbct&quot;:[&quot;cn-bj2&quot;]&#125;&#125; get_price(setting_info) grafana效果展示 grafana跟influxDB配合的非常好，设置也非常简单。下面我在下面放几张效果图。 简单查询语句 table展示效果 价格图展示 获取实时带宽信息并发送报警到钉钉 这个跟上面的脚本没有关联，只是获取实时带宽使用量的报警脚本。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152#!/usr/bin/env python# -*- coding: utf-8 -*-from sdk import UcloudApiClientfrom config import *import sysimport jsonimport urllib2import time#from op_tools import falcon#实例化 API 句柄localtime = time.asctime( time.localtime(time.time()) )def get_eip_info(): &apos;&apos;&apos; 获取EIP对应的主机名，以及EIP信息返回 &apos;&apos;&apos; arg_length = len(sys.argv) ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123;&quot;Action&quot;:&quot;DescribeEIP&quot;, &quot;Region&quot;:&quot;cn-bj2&quot;&#125; response = ApiClient.get(&quot;/&quot;, Parameters ); eip_info_list = response[&apos;EIPSet&apos;] eip_dic = &#123;&#125; for eip_info in eip_info_list: # EIP绑定主机名 eip_host = eip_info[&apos;Resource&apos;][&apos;ResourceName&apos;] eip_ip = eip_info[&apos;EIPAddr&apos;][0][&apos;IP&apos;].encode(&apos;utf-8&apos;) eip_id = eip_info[&apos;EIPId&apos;] dic = &#123;&#125; dic[eip_ip] = eip_id #print &quot;eip_host:%s,dic:%s&quot; % (eip_host,dic) eip_dic[eip_host] = dic #print len(eip_dic) eip_dic[&apos;nginx-online1&apos;] = &#123;&apos;106.75.28.177&apos;: u&apos;eip-00gv0l&apos;&#125; return eip_dicdef get_eip_usage(eip_dic): &apos;&apos;&apos; 获取每个EIP的实时用量(要求EIPid), return list:[&#123;ip:usage&#125;...] &apos;&apos;&apos; eip_usage_dic = &#123;&#125; for eip_host in eip_dic: #eip_useage_dic = &#123;&#125; eip_id=eip_dic[eip_host].values()[0].encode(&quot;utf-8&quot;) #print eip_id ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123; &quot;Action&quot;:&quot;DescribeBandwidthUsage&quot;, &quot;Region&quot;:&quot;cn-bj2&quot;, &quot;EIPIds.1&quot;:eip_id, &#125; response = ApiClient.get(&quot;/&quot;, Parameters); #print json.dumps(response, sort_keys=True, indent=4, separators=(&apos;,&apos;, &apos;: &apos;)) #print response eip_usage = response[&apos;EIPSet&apos;][0][&apos;CurBandwidth&apos;] eip_usage_dic[eip_host]=eip_usage #eip_usage_list.append(eip_useage_dic) return eip_usage_dicdef sendto_falcon(eip_usage_list): &apos;&apos;&apos; 报警发送到falcon &apos;&apos;&apos; collect_step = 60 counter_type = falcon.CounterType.GAUGE metric = &quot;bandwidthusage&quot; for eip_usage in eip_usage_list: tags=&quot;host=&quot; + eip_usage.keys()[0] value=eip_usage.values()[0] #print valuedef get_sharebw_info(): &apos;&apos;&apos; 获取共享带宽的带宽大小，以及所包含的EIP, return list:[&#123;&apos;eiplist&apos;:[ip1,ip1],&apos;bandwidth&apos;:20&#125;...] &apos;&apos;&apos; ApiClient = UcloudApiClient(base_url, public_key, private_key) Parameters=&#123;&quot;Action&quot;:&quot;DescribeShareBandwidth&quot;, &quot;Region&quot;:&quot;cn-bj2&quot;&#125; response = ApiClient.get(&quot;/&quot;, Parameters ); #print json.dumps(response, sort_keys=True, indent=4, separators=(&apos;,&apos;, &apos;: &apos;)) share_bw_list = response[&apos;DataSet&apos;] share_bw_info = [] for share_bw in share_bw_list: share_bw_dic = &#123;&#125; bandwidth = share_bw[&apos;ShareBandwidth&apos;] eip_list = [] for eip_dic in share_bw[&apos;EIPSet&apos;]: ip = eip_dic[&apos;EIPAddr&apos;][0][&apos;IP&apos;] eip_list.append(ip) share_bw_dic[&apos;bandwidth&apos;]=bandwidth share_bw_dic[&apos;eiplist&apos;]=eip_list share_bw_info.append(share_bw_dic) return share_bw_infodef sum(): &apos;&apos;&apos; 返回比值，并简单记录log到/var/log/ubandwidth.log &apos;&apos;&apos; eip_dic = get_eip_info() #print eip_dic eip_to_host = &#123;&#125; # 通过eip_dic获取IP-host对此应关系dic for host in eip_dic: eip = eip_dic[host].keys()[0] eip_to_host[eip]=host #print eip_to_host eip_usage_dic = get_eip_usage(eip_dic) #print eip_usage_dic share_bw_info = get_sharebw_info() #print share_bw_info #各带宽和与带宽比值 ratios = &#123;&#125; for share_bandwidth in share_bw_info: bandwidth = share_bandwidth[&apos;bandwidth&apos;] sum = 0 for eip in share_bandwidth[&apos;eiplist&apos;]: host = eip_to_host[eip] usage = eip_usage_dic[host] sum += usage #带宽用量与带宽的比值 ratio = sum/bandwidth parts = [str(bandwidth),str(sum),str(ratio)] log =localtime + &apos; &apos; + &apos;,&apos;.join(parts) + &apos;\n&apos; with open(&apos;/var/log/ubandwidth.log&apos;,&apos;a&apos;) as f: f.write(log) f.close() ratios[bandwidth]=ratio return ratiosdef send_to_dingtalk(content): url = &quot;https://oapi.dingtalk.com/robot/send?access_token=17cf865229a63452ff411243b53d64949d5a54b1ee8774e20e1ec7d4c5d60f43&quot; #con=&#123;&quot;msgtype&quot;:&quot;text&quot;,&quot;text&quot;:&#123;&quot;content&quot;:content&#125;,&quot;isAtAll&quot;: &quot;true&quot;&#125; con=&#123;&quot;msgtype&quot;:&quot;markdown&quot;,&quot;markdown&quot;:&#123;&quot;title&quot;:&quot;ucloud共享带宽报警&quot;,&quot;text&quot;:content&#125;,&quot;isAtAll&quot;: &quot;ture&quot;&#125; jd=json.dumps(con) req=urllib2.Request(url,jd) req.add_header(&apos;Content-Type&apos;, &apos;application/json&apos;) response=urllib2.urlopen(req)if __name__ == &apos;__main__&apos;: ratios=sum() localtime = time.asctime( time.localtime(time.time()) ) for bw in ratios: if ratios[bw] &gt; 0.8: content = u&quot;# **ucloud共享带宽报警** - %d兆那个。。。\n\n - 用量超过百分之80 \n - **值**:%f \n &gt; [请排查...](https://console.ucloud.cn/unet/sharebandwidth)&quot; % (bw,ratios[bw]) send_to_dingtalk(content)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx之location,upstream,rewrite]]></title>
    <url>%2F2017%2F07%2F08%2Fnginx%E4%B9%8Blocation-upstream-rewrite%2F</url>
    <content type="text"><![CDATA[nginx配置第三篇，nginx安装配置中讲了很多基础配置，这次讲一下具体的操作配置。 location配置 location模块工作在虚拟主机server之下，对URL进行匹配，如果匹配成功就按照该location之中写的语句进行操作。 语法 location [=|~|~*|^~] /uri/ { … } 匹配规则 模式 含义 location = /uri = 表示精确匹配，只有完全匹配上才能生效。 location ^~ /uri ^~ 开头对URL路径进行前缀匹配，并且在正则之前。 location ~ pattern 区分大小写的正则匹配 location ~* pattern 不区分大小写的正则匹配 location /uri 不带任何修饰符，也表示前缀匹配，但是在正则匹配之后 location / 通用匹配，任何未匹配到其它location的请求都会匹配到，相当于switch中的default &emsp;&emsp;那么如果我们在一个虚拟主机下边写了很多location 规则，哪一个先匹配哪一个后匹配呢？是这样的。nginx会根据模糊程度排序的 首先精确匹配 = 其次前缀匹配 ^~ 其次是按文件中顺序的正则匹配 然后匹配不带任何修饰的前缀匹配。 最后是交给 / 通用匹配 当有匹配成功时候，停止匹配，按当前匹配规则处理请求 upstream 负载均衡模块,负责根据配置合理分流到各个代理节点，而且自带后端节点健康检查（需要自己通过proxy_read_timeout指令和proxy_next_upstream指令配置） 参考文章语法 upstream name {server ip:port 状态} 12345678910111213141516171819202122http &#123; upstream name &#123; [ip_hash;] server ip:port [weight=n]; server ip:port [weight=n]; &#125;server &#123; listen 80; server_name www.xxx.com; proxy_pass http://name; 或者 uwsgi_pass 或者 fast_cgi_pass &#125; &#125; 上下文 http server location 负载均衡策略: 轮询(默认) 设置权重(weight) ip_hash(每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。) 其次还可以定义状态 down 表示单前的server暂时不参与负载 weight 默认为1.weight越大，负载的权重就越大。 max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误 fail_timeout:max_fails次失败后，暂停的时间。 backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 负载均衡后端节点健康检查&emsp;&emsp;严格来说，nginx自带是没有针对负载均衡后端节点的健康检查的，但是可以通过默认自带的ngx_http_proxy_module. 模块和ngx_http_upstream_module模块中的相关指令来完成当后端节点出现故障时，自动切换到健康节点来提供访问。 这里学习下ngx_http_proxy_module 模块中的 proxy_connect_timeout 指令、proxy_read_timeout指令和proxy_next_upstream指令 设置与后端服务器建立连接的超时时间。 应该注意这个超时一般不可能大于75秒。 proxy_connect_timeout 60s; 设置从后端服务器读取响应的超时 proxy_read_timeout 60s; 指定在何种情况下一个失败的请求应该被发送到下一台后端服务节点 proxy_next_upstream error timeout; error 和后端服务器建立连接时，或者向后端服务器发送请求时，或者从后端服务器接收响应头时，出现错误 timeout 和后端服务器建立连接时，或者向后端服务器发送请求时，或者从后端服务器接收响应头时，出现超时 invalid_header 后端服务器返回空响应或者非法响应头 http_500 后端服务器返回的响应状态码为500 http_502 后端服务器返回的响应状态码为502 http_503 后端服务器返回的响应状态码为503 http_504 后端服务器返回的响应状态码为504 http_404 后端服务器返回的响应状态码为404 off 停止将请求发送给下一台后端服务器 &emsp;&emsp;还可以通过tengine来实现，淘宝技术团队开发的nginx_upstream_check_module模块来实现。如果没有使用tengine需要打补丁。配置起来比上述方法简单一些。参考地址 1234567891011121314151617http &#123; upstream cluster1 &#123; # simple round-robin server 192.168.0.1:80; server 192.168.0.2:80; check interval=3000 rise=2 fall=5 timeout=1000 type=http; check_http_send &quot;HEAD / HTTP/1.0\r\n\r\n&quot;; check_http_expect_alive http_2xx http_3xx; &#125; upstream cluster2 &#123; # simple round-robin server 192.168.0.3:80; server 192.168.0.4:80; check interval=3000 rise=2 fall=5 timeout=1000 type=http; check_keepalive_requests 100; check_http_send &quot;HEAD / HTTP/1.1\r\nConnection: keep-alive\r\n\r\n&quot;; check_http_expect_alive http_2xx http_3xx; &emsp;&emsp;上面配置的意思是，对name这个负载均衡条目中的所有节点，每个3秒检测一次，请求2次正常则标记 realserver状态为up，如果检测 5 次都失败，则标记 realserver的状态为down，超时时间为1秒。 interval：向后端发送的健康检查包的间隔。 fall(fall_count): 如果连续失败次数达到fall_count，服务器就被认为是down。 rise(rise_count): 如果连续成功次数达到rise_count，服务器就被认为是up。 timeout: 后端健康请求的超时时间。 type：健康检查包的类型，现在支持以下多种类型 tcp：简单的tcp连接，如果连接成功，就说明后端正常。 ssl_hello：发送一个初始的SSL hello包并接受服务器的SSL hello包。 http：发送HTTP请求，通过后端的回复包的状态来判断后端是否存活。 mysql: 向mysql服务器连接，通过接收服务器的greeting包来判断后端是否存活。 ajp：向后端发送AJP协议的Cping包，通过接收Cpong包来判断后端是否存活。 check_keepalive_requests &emsp;&emsp;该指令可以配置一个连接发送的请求数，其默认值为1，表示Tengine完成1次请求后即关闭连接。 check_http_send &emsp;&emsp;该指令可以配置http健康检查包发送的请求内容。为了减少传输数据量，推荐采用”HEAD”方法。 当采用长连接进行健康检查时，需在该指令中添加keep-alive请求头，如：”HEAD / HTTP/1.1\r\nConnection: keep-alive\r\n\r\n”。同时，在采用”GET”方法的情况下，请求uri的size不宜过大，确保可以在1个interval内传输完成，否则会被健康检查模块视为后端服务器或网络异常。 check_http_expect_alive &emsp;&emsp;该指令指定HTTP回复的成功状态，默认认为2XX和3XX的状态是健康的。如果爬虫多404多可以把4XX也写进去。 rewrite rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用 语法rewrite regex replacement [flag]; flag标志位 last – 相当于Apache的[L]标记，表示完成rewrite，浏览器地址不变 break – 中止 Rewirte，不再继续匹配，浏览器地址不变 redirect – 返回临时重定向的 HTTP 状态 302 浏览器地址显示跳转后的地址 permanent – 返回永久重定向的 HTTP 状态 301 浏览器地址显示跳转后的地址 last一般写在server和if中，而break一般使用在location中 last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配。 简单举例 1234567// 访问/example.html 的时候重写到/index.htmlrewrite /example.html /index.html last;// 访问/example.html 的时候重写到/index.html,并停止匹配rewrite /example.html /index.html break;// 把 /search/key =&gt; /search.html?keyword=keyrewrite &apos;^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$&apos; /data?file=$3.$4 last; &quot;^&quot; 表示开头匹配 &quot;$&quot; 表示结尾匹配 而且表示路径的&quot;/&quot;是需要转义的，&quot;$1&quot;表示表达式匹配到的第一个括号里面的内容，即([a-z]&#123;2&#125;) 正则实例可以参考]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx之HTTPS配置]]></title>
    <url>%2F2017%2F07%2F07%2Fnginx%E4%B9%8BHTTPS%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[https现在是标配了，HTTPS 可以给用户带来更安全、比如减少了被劫持的概率，更好隐私保护的网络体验，这些好处大家都耳熟能详，本文不再赘述。现在很多浏览器都在推行HTTPS的普及。尽快升级吧。 其实大体就是分为ssl证书申请和配置HTTPS两个步骤 证书申请这次介绍并没有从申请证书开始，因为之前已经申请过了，申请步骤请参考SSL 证书主要有两个功能：加密和身份证明，通常需要购买，也有免费的，通过第三方 SSL 证书机构颁发。分为企业级别和个人级别。SSL 具体加密实现参考查看证书相关配置，包括哪个机构颁发的，过期时间等等信息可以到https://www.chinassl.net 自助查看 nginx HTTPS配置首先我们把得到的domain.key domain.crt 放到 nginx的conf下，可以在nginx.conf 的server中配置 基础配置123456789101112 server &#123; listen 443 ssl http2; #证书文件(注意路径及权限) ssl on; ssl_certificate example.com.crt; #私钥文件 ssl_certificate_key example.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5;#....&#125; 必须使用监听命令 listen 的 ssl 参数和定义服务器证书文件和私钥文件 ssl_protocols 可以用来限制连接只包含 SSL/TLS 的加強版本，默认值如上。ssl_ciphers 选择加密套件，不同的浏览器所支持的套件（和顺序）可能会不同。这里指定的是OpenSSL库能够识别的写法，你可以通过 openssl -v cipher ‘RC4:HIGH:!aNULL:!MD5’（后面是你所指定的套件加密算法） 来看所支持算法。 加强 HTTPS 安全性 1234ssl_prefer_server_ciphers ON;add_header X-Frame-Options DENY;add_header X-Content-Type-Options nosniff;add_header X-Xss-Protection 1; ssl_prefer_server_ciphers ON设置协商加密算法时，优先使用我们服务端的加密套件，而不是客户端浏览器的加密套件。add_header X-Frame-Options DENY减少点击劫持 HTTPS优化参数1234ssl_session_cache shared:SSL:10m;ssl_session_timeout 10m;ssl_buffer_size 1400; ssl_session_cache shared:SSL:10m; 设置ssl/tls会话缓存的类型和大小。如果设置了这个参数一般是shared，buildin可能会参数内存碎片，默认是none，和off差不多，停用缓存。如shared:SSL:10m表示我所有的nginx工作进程共享ssl会话缓存，官网介绍说1M可以存放约4000个sessions。ssl_session_timeout 10m; 客户端可以重用会话缓存中ssl参数的过期时间。ssl_buffer_size 1400; 缓冲区调优，从1.5.9版本开始,Nginx允许使用ssl_buffer_size指令自定义TLS缓冲区的大小，默认值是 16 KB,但是这个值不一定是最优化的,尤其是你希望首字节数据被尽早发送时,有报告显示使 用1400字节的配置可以显著减少延迟。参考 配置参考123456789101112ssl on;ssl_session_timeout 30m;ssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA:!CAMELLIA;ssl_prefer_server_ciphers on;ssl_buffer_size 1400;ssl_session_cache shared:SSL:10m;ssl_certificate domain.crt;ssl_certificate_key domain.key;]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[influxdb使用笔记]]></title>
    <url>%2F2017%2F07%2F06%2Finfluxdb%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[之前从ucloudAPI上取下来的数据需要存到influxdb中，在grafana中进行展示。grafana是我自行部署推广使用的，influxdb是别的同事之前就开始用了。我这次正好用上，所以仔细看了下。虽然人家都把API封装好了，只是拿过来就用的事儿。但是我只是个爱学习的孩子。。。 介绍 关于时序数据库，除了常用的ElasticSearch之外，InfluxDB也是一个选择。 InfluxDB 使用 go 语言编写。个人认为几个外在的优点在于： 无特殊依赖，几乎开箱即用（如ES需要Java）； 自带HTTP管理界面，免插件配置（如ES的kopf或者head）； 自带数据过期功能； 类SQL查询语句（再提ES，查询使用自己的DSL，虽然也可以通过sql插件来使用类SQL进行查询）； 自带权限管理，精细到“表”级别； 关键词解读参考官方文档 time 这个概念首先说明，influxdb本身就是一个时序数据库类似Elasticsearch，所以用来做流处理是很好的，一次插入，多次读写，少改动。 每次插入一条数据都必须要求一个时间戳，自己不定义他就自动生成。 database 就是数据库 measurement 相当于mysql中的table field 类似于MySQL的字段，没有索引的列，是influxDB数据必须的组成部分。 tags 相当于MySQL带索引的字段，不必须 retention policy (RP) 描述数据存储多久，以及规定几个分片 point 同一时间戳产生的数据集合 sereis measurement, tag set, and retention policy 都相同的数据集合 CLI命令 CLI官方文档influx的CLI命令与mysql还是有很多相似之处的。不过用influxdb我们更多的是用他的API很少用到CLI，只是了解下，自己调试代码的时候可以验证一下自己的数据到底写没写进来。 建库1234567CREATE DATABASE mydb验证：SHOW DATABASES使用：USE mydb 现在，建好库可以插入数据了。 数据类型只支持以下几种float,integer,string,Boolean,Timestamp 建表并插入数据这条命令表示: 新建一个表名为cpu的表，设置host为serverA,region为us_west,value为0.64,其中 host,region 属性为tags, value 属性为field1INSERT cpu,host=serverA,region=us_west value=0.64 查询数据1SELECT &quot;host&quot;, &quot;region&quot;, &quot;value&quot; FROM &quot;cpu&quot; WHERE &quot;value&quot;&gt;0.9 HTTP API使用官方文档查看 建库1curl -i -XPOST http://localhost:8086/query --data-urlencode &quot;q=CREATE DATABASE mydb&quot; 单条数据写入 1curl -i -XPOST &apos;http://localhost:8086/write?db=mydb&apos; --data-binary &apos;cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000&apos; 多条数据写入123curl -i -XPOST &apos;http://localhost:8086/write?db=mydb&apos; --data-binary &apos;cpu_load_short,host=server02 value=0.67cpu_load_short,host=server02,region=us-west value=0.55 1422568543702900257cpu_load_short,direction=in,host=server01,region=us-west value=2.0 1422568543702900257&apos;]]></content>
      <categories>
        <category>influxdb</category>
      </categories>
      <tags>
        <tag>influxdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx之安装配置]]></title>
    <url>%2F2017%2F07%2F06%2Fnginx%E4%B9%8B%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[nginx 相关知识参考网站Tengine nginx 首先从安装配置说起问题说起 安装 一般都是源码编译安装，直接解压编译安装就好没有啥可说的 一般需要安装PCRE zlib openssl 库 以及所需要模块例如openLDAP等 特别说明安装完成之后再添加模块,需要重启服务，reload不会生效。参考链接 配置 nginx的配置是一门很深的功课，首先我们要对基本的http协议特别了解，配置过程可能要各种rewrite，各种location。不要慌，一点一点来。 配置文件参数详解 首先对nginx.conf里面的各个配置项进行一下解释。 首先说下少数几个高级配置，一般写在开头，模块配置之上进程运行的用户组1user nginx nginx; 进程数 一般跟CPU核数相匹配。nginx启动后有多少个worker处理请求，不包括master，(master不处理请求，二十主要接受客户端的请求并分配给worker处理)这里还涉及到下边要说的worker_connections, 正常被大家接受的nginx最大连接数就是靠这两个计算出来的. nginx作为http服务器的时候： max_clients = worker_processes * worker_connections nginx作为反向代理服务器的时候： max_clients = worker_processes * worker_connections/4 具体为什么去参考1worker_processes 8; 最大打开文件数量 这个如果没有设置会使用linux系统默认的文件最大打开数 ulimit -a可以查看到。1worker_rlimit_nofile 65535; Events模块这里包含nginx所有处理连接的设置12345events &#123; worker_connections 2048; use epoll;&#125; worker_connections表示一个worker同时打开最大连接数。use epoll定义轮询方法 如果你的内核为linux 2.6+ 应该使用epoll异步非阻塞模型 http模块 HTTP模块控制着nginx http处理的所有核心特性 12345678910111213http &#123; server_tokens off; sendfile on; tcp_nopush on; tcp_nodelay on; ...&#125; server_tokens 这个只是在错误页面不显示nginx版本，为了安全。sendfile ,tcp_nopush,tcp_nodelay这三条一般同时出现 提高读写速度 提升性能 ，tcp_nopush 依赖sendfile, tcp_nodelay 不缓存数据，(禁用nagle算法，发送小数据不缓存直接发) 123log_format main &apos;$remote_addr - - [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; [$request_time, $upstream_response_time] $host ($remote_port) &quot;sid=$cookie_sessionid&quot;&apos;;access_log logs/access.log main; 定义日志格式 main 下边引用该格式。其实可以设置成json格式的，以后收集解析也方便。 123456lingering_close off;keepalive_timeout 5;send_timeout 20;proxy_connect_timeout 30;proxy_read_timeout 20;proxy_send_timeout 20; lingering_close 定义关闭连接的方式，有三个选项 off|on|always off：请求完成之后，关闭连接，不管此时有没有收到客户端数据；on是中间值，一般情况下在关闭连接前都会处理连接上的用户发送的数据，除了有些情况下在业务上认定这之后的数据是不必要的；always无条件处理完所有用户请求。Tengine 默认off效率高些，但是存在误杀状况，nginx默认on 算个小坑。 keepalive_timeout 给客户端分配keep-alive链接超时时间 send_timeout 发送响应的超时时间，两个客户端请求之间的时间 这几个一般用在nginx做反向代理的时候proxy_connect_timeout 后端服务器连接的超时时间_发起握手等候响应超时时间proxy_read_timeout 后端服务器处理请求的时间proxy_send_timeout 后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 123include /etc/nginx/mime.types;default_type application/octet-stream; include只是一个在当前文件中包含另一个文件内容的指令。这里我们使用它来加载稍后会用到的一系列的MIME类型。其实就是content-type与扩展名的映射。在客户端发来一个请求之后，nginx通过扩展名找到对应的content-type,下载返回的头信息中，浏览器收到之后会按照这个类型做解析展示。这样就不至于发生css文件本当做html一样当文本展示了。如果在mime.types中没有找到，会使用default_type 1234567## GZIP Settinggzip on;gzip_min_length 1000;gzip_buffers 4 8k;gzip_http_version 1.0;gzip_comp_level 5;gzip_types text/plain text/css application/x-javascript application/json application/xml; gzip是GNU zip的缩写，它是一个GNU自由软件的文件压缩程序，可以极大的加速网站.有时压缩比率高到80%,近来测试了一下,最少都有40%以上,还是相当不错的。gzip决定是否开启gzip模块 gzip_min_length当返回内容大于此值时才会使用gzip进行压缩,以K为单位,当值为0时，所有页面都进行压缩gzip_buffers设置gzip申请内存的大小,其作用是按块大小的倍数申请内存空间gzip_http_version用于识别http协议的版本，早期的浏览器不支持gzip压缩，用户会看到乱码，所以为了支持前期版本加了此选项,目前此项基本可以忽略gzip_comp_level设置gzip压缩等级，等级越底压缩速度越快文件压缩比越小，反之速度越慢文件压缩比越大gzip_types设置需要压缩的MIME类型,非设置值不进行压缩 server模块 server模块是http的子模块，定义虚拟主机格式12345678910server &#123; listen 80; server_name map.baidu.com www.baidu.com; client_max_body_size 10m; root /Users/yangyi/www; index index.php index.html index.htm; charset utf-8; &#125;&#125; server {} server标志虚拟主机开始，在 {} 中配置listen监听 80端口server_name用来指定IP地址或者域名，多个域名之间用空格分开。这里指定域名为map.baidu.com 或者www.baidu.com。client_max_body_size 文件上传大小charset 声明网站默认编码格式 直接转发到到10.0.0.1:9000，这几个proxy_set_header的意思是改变请求头的Host为客户端的Host，ip 否则在下层的服务端会认为客户端是这台代理的nginx。X-Forwarded-For 是一个 HTTP 扩展头部。HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP。如今它已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 RFC 7239（Forwarded HTTP Extension）标准之中。格式如下 X-Forwarded-For: client, proxy1, proxy2 可以看到，XFF 的内容由「英文逗号 + 空格」隔开的多个部分组成，最开始的是离服务端最远的设备 IP，然后是每一级代理设备的 IP。如果一个 HTTP 请求到达服务器之前，经过了三个代理 Proxy1、Proxy2、Proxy3，IP 分别为 IP1、IP2、IP3，用户真实 IP 为 IP0，那么按照 XFF 标准，服务端最终会收到以下信息： X-Forwarded-For: IP0, IP1, IP2 这个在多级代理的时候可以设置下，逻辑关系更清晰。 location 模块 在server内部 1234location / &#123; root /Users/yangyi/www; index index.php index.html index.htm; &#125; 1234567location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://10.0.0.1:9000; proxy_redirect default; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 一个配置样例```user www-data; pid /var/run/nginx.pid; worker_processes auto; worker_rlimit_nofile 100000; events { worker_connections 2048; multi_accept on; use epoll; } http { server_tokens off; sendfile on; tcp_nopush on; tcp_nodelay on; log_format main ‘$remote_addr - - [$time_local] “$request” $status $body_bytes_sent “$http_referer” ‘ ‘“$http_user_agent” [$request_time, $upstream_response_time] $host ($remote_port) “sid=$cookie_sessionid”‘; access_log logs/access.log main; error_log /var/log/nginx/error.log crit; keepalive_timeout 10; client_header_timeout 10; client_body_timeout 10; reset_timedout_connection on; send_timeout 10; limit_conn_zone $binary_remote_addr zone=addr:5m; limit_conn addr 100; include /etc/nginx/mime.types; default_type text/html; charset UTF-8; gzip on; gzip_disable &quot;msie6&quot;; gzip_proxied any; gzip_min_length 1000; gzip_comp_level 6; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; open_file_cache max=100000 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 2; open_file_cache_errors on; include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } 最后 关于nginx和http https等的设置可以参考博客]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次PHP服务部署]]></title>
    <url>%2F2017%2F07%2F05%2F%E8%AE%B0%E4%B8%80%E6%AC%A1PHP%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[本来很少接触这门世界上最好的语言，公司里面也没有，但是这次有这个需求，考虑到fastcgi与uwsgi有这么一点点共同点，我就照葫芦画瓢，打算用我们测试的nginx做转发。但是踩到几个坑，听我带着悔恨一点一点的说。。。 nginx代理转发介绍我们的测试跟线上的服务都是用nginx做全职代理转发在nginx.conf 声明如下:123456http &#123;...include /usr/local/nginx/conf/servers/*/upstream.conf;include /usr/local/nginx/conf/servers/*/site.conf;...&#125; 所以就可以在server目录下建立各种监听二级域名的目录。 类似这种,uwsgi的转发使用uwsgi_pass ,普通web代理使用proxy_pass,fastcgi使用fastcgi_pass下面举例uwsgi转发配置说明一下。/usr/local/nginx/conf/servers/dier/site.conf123456789101112131415161718server &#123; listen 443 ssl http2; server_name dier.chunyu.me; include /usr/local/nginx/conf/servers/common/ssl_config.location; location / &#123; uwsgi_pass devops_uwsgi; include uwsgi_params; &#125;&#125;server &#123; listen 80; server_name .chunyu.me;# 强转https rewrite ^/(.*)$ https://devops.chunyu.me/$1 permanent;&#125; /usr/local/nginx/conf/servers/dier/upstream.conf123upstream devops_uwsgi &#123; server 10.9.77.8:5001;&#125; PHP服务部署 源码编译安装，全程Google教程,直接按照参考地址配置即可。 PHP-FPMPHP-FPM是一个PHP FastCGI管理器，是只用于PHP的,可以在 http://php-fpm.org/download下载得到。PHP-FPM其实是PHP源代码的一个补丁，旨在将FastCGI进程管理整合进PHP包中。必须将它patch到你的PHP源代码中，在编译安装PHP后才可以使用。FPM（FastCGI 进程管理器）用于替换 PHP-CGI 的大部分附加功能，对于高负载网站是非常有用的。它的功能包括： 支持平滑停止/启动的高级进程管理功能； 可以工作于不同的 uid/gid/chroot 环境下，并监听不同的端口和使用不同的 php.ini 配置文件（可取代 safe_mode 的设置）； stdout 和 stderr 日志记录; 在发生意外情况的时候能够重新启动并缓存被破坏的 opcode; 文件上传优化支持; “慢日志” – 记录脚本（不仅记录文件名，还记录 PHP backtrace 信息，可以使用 ptrace或者类似工具读取和分析远程进程的运行数据）运行所导致的异常缓慢; fastcgi_finish_request() – 特殊功能：用于在请求完成和刷新数据后，继续在后台执行耗时的工作（录入视频转换、统计处理等）；8.动态／静态子进程产生 ； 基本 SAPI 运行状态信息（类似Apache的 mod_status）； 基于 php.ini 的配置文件。 环境配置123456789yum -y install gcc gcc-c++groupadd webuseradd -M -s /sbin/nologin -g web phpyum -y install epel-releaseyum -y updateyum -y install libmcrypt libmcrypt-devel mcrypt mhashyum -y install libxml2-devel libpng-devel libjpeg-devel zlib bzip2 bzip2-devel \libtool-ltdl-devel pcre-devel openssl-devel freetype-devel libcurl-devel icu \perl-libintl postgresql libicu-devel 下载解压1234cd /usr/local/src/wget http://cn2.php.net/distributions/php-5.6.27.tar.gztar -zxvf php-5.6.27.tar.gzcd php-5.6.27/ 编译安装123456789101112131415161718192021222324252627282930313233343536373839404142./configure \--prefix=/usr/local/php5.6.27 \--with-config-file-path=/usr/local/php5.6.27/etc/ \--enable-inline-optimization \--enable-shared \--enable-opcache \--enable-fpm \--with-fpm-user=php \--with-fpm-group=web \--with-mysql=mysqlnd \--with-mysqli=mysqlnd \--with-pdo-mysql=mysqlnd \--with-gettext \--enable-mbstring \--with-iconv \--with-mcrypt \--with-mhash \--with-openssl \--enable-bcmath \--enable-soap \--with-libxml-dir \--enable-pcntl \--enable-shmop \--enable-sysvmsg \--enable-sysvsem \--enable-sysvshm \--enable-sockets \--enable-intl \--with-curl \--with-zlib \--enable-zip \--with-bz2 \--enable-xml \--with-pcre-dir \--with-gd \--enable-static \--enable-wddx \--with-xmlrpc \--with-libdir=/usr/lib64 \--with-jpeg-dir=/usr/lib64 \--with-freetype-dir=/usr/lib64 \--with-png-dir=/usr/lib64 1make &amp;&amp; make install 简单配置12cp php.ini-development /usr/local/php5.6.27/etc/php.inicp /usr/local/php5.6.27/etc/php-fpm.conf.default /usr/local/php5.6.27/etc/php-fpm.conf 创建开机启动1vi /lib/systemd/system/php-fpmd.service 1234567891011121314[Unit]Description=The PHP FastCGI Process ManagerAfter=network.target[Service]Type=forkingPIDFile=/run/php-fpm.pidExecStart=/usr/local/php5.6.27/sbin/php-fpm --daemonize -g /run/php-fpm.pidExecReload=/bin/kill -USR2 $MAINPIDExecStop=/bin/kill -SIGINT $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target 12systemctl enable php-fpmd.servicesystemctl start php-fpmd.service 注意 php.ini 中设置open_basedir=/usr/local/nginx/html/webappsphp-frm.conf 中security.limit_extensions = .php .html .js .css .jpg .jpeg .gif .png .htm .txt mysql 安装 ansible 自动安装，脚本以后会附上，导入数据手动，没有任何问题 nginx 配置 我最开始的想法是，在起PHP这个服务的云主机上起一个nginx 做web服务器。但是我组大神告诉我，不用这么麻烦直接用测试服nginx代理就好。于是我没有反驳，毕竟这个看起来更简单更合理。于是我就开始配置。。。 最开始配置,如下 1234567891011121314server &#123; listen 80; server_name dier.chunyu.me; location ~ [^.]+\.php$ &#123; root /usr/share/webapps; fastcgi_pass 10.0.0.1:9000; index index.html index.php; #fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 写到一半，我突然发现静态文件怎么办，于是我直接把location ~ [^.]+\.php$`` 改成了location /所有文件都这么走。 结果问题就出现了，如下图。css文件的请求头Content_type为text/html` 我试着在测试服nginx上各种add_header 都不好使，于是请教之前大神，他一脸不屑的看着我，看了三秒。。。然后他解决，我看到机器上文件的修改，一次次add_header ,过了20多分钟，他扭头给我说，这个fastcgi好像不支持静态文件代理，而且他的代码里面没有加判断。你在这个机器上装个nginx吧。。。恩，于是我有用ansible跑了一遍安装nginx的脚本。配置如下: 12345678910111213location ~ [^.]+\.php$ &#123; root /usr/local/nginx/html/webapps; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; #fastcgi_param SCRIPT_FILENAME /usr/local/nginx/html/webapps$fastcgi_script_name; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi.conf; &#125;location ~* \.(css|js|png|jpg|jpeg|gif|ico)$ &#123; expires max; log_not_found off; &#125; 问题解决。。。 总结 多学习，多看书，少说话 如果说话学会和人一样说话 多做总结，比如写博客加深一下印象，不太懂也没有关系，写着写着有可能就懂了。 一台机器上启动两个PHP服务 SEO的哥们找我说克隆一个和之前一模一样的服务，我的原则是PHP这种漏洞比较多的服务最好还是给他们独立出来不要跟线上有联系，前不久让一个白帽子给我们扫了一下，我们才发现之前商务部门归到我们这边的一个提供PHP服务的机器直接被人拿到了root的shell，真可怕。于是，我这次把他们的数据库都从线上拆出来了。放在本地。 方法 直接再装一个php-fpm （我刚开始用一个PHP-fpm提供和两个PHP服务的动态处理在nginx中把他们的静态文件分开，事实证明不可以，会发生一些奇怪的情况，两个系统的各种配置，包括数据库都会混淆）换个端口启动就好了 参考地址](https://my.oschina.net/yule526751/blog/795807)]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用总结]]></title>
    <url>%2F2017%2F06%2F30%2FGit%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[很长一段时间使用git都是只有 add,commit,pull,push等这几个简单的命令。想想自己也是使用github的人，怎么能只会这些皮毛。 首先我们知道git主要是做版本控制工具，所以一些概念逻辑都是为了更好地实现这个功能。 三个目录概念首先要对概念清楚 Working Directory：工作目录，这个可以简单的理解为你在文件系统里真实看到的文件 Stage（Index）：暂存“目录”，用git add命令添加的文件就到了这里，即将被commit的文件 Repository：项目“目录”，用git commit提交的文件就到了这里 commit介绍几种比较6的commit的操作，记住使用git的一贯原则还是少量改动频繁提交，方便做版本控制。 平常我们都是git add ./然后git commit -m &quot;fix&quot;提交代码 这两条可以结合到一起直接git commit -am &quot;fix&quot;就做到了或者指定文件git commit test.txt -m &quot;fix2.0&quot;. 修改上一次提交 git commit --amend -am &quot;fix2.0, 2.5 这样之前2.0的commit_id直接被覆盖了。 checkout 分支相关操作：git checkout 分支名/commit hash切换到相应的分支或commit，加上-b参数则会创建分支并切换过去 git checkout -b branch3 1a222c3 注意这里commit_id为新分支起点 恢复文件相关操作：git checkout [分支名/commit hash/HEAD快捷方式] – 文件名恢复指定分支的最新commit或指定commit或快捷方式指向的commit的文件到工作目录，若省略中间的参数，则 暂存区有内容且暂存区内容与工作目录不同，则恢复暂存区的状态到工作目录(之前git add过恢复到当时的状态) 暂存区无内容，则恢复HEAD（最新的commit）的状态到工作目录 diff 使用方式基本就是 git diff [source] [target] 也就是说 target相对于source有哪些变化 ,这里的target,source 可以是commit_id也可以是两个分支,同时git diff master branch2和git diff HEAD branch2显示结果是一样的 只给一个参数 这个参数默认是 source 而target 默认是当前分支最新的commit 不给参数 source为暂存区，target为工作目录 如果想要使暂存目录作为target的话，需要使用–cached参数 reset git reset [commit hash/分支名/快捷方式] [文件名]类似“git add的反操作”，直接将所在commit的文件状态恢复到暂存区域。省略commit则默认为HEAD，省略文件名默认为所有文件。只改变暂存目录，不改变工作目录，当前commit不变。 git reset –soft [commit hash/分支名/快捷方式]软恢复，将恢复前所在commit的文件状态恢复到暂存区，当前最新commit为参数中的commit。只改变暂存目录，不改变工作目录，当前commit改变。 git reset –hard [commit hash/分支名/快捷方式]硬恢复，强制将整个项目恢复为参数中的commit时的文件状态，清空暂存目录，工作目录clean。暂存目录和工作目录同时被改变，当前commit改变。 关于reset命令的其他补充：当前HEAD已经位于“不是最新”，是不是前面的commit都找不回来了？当然不会，reset过的操作也是可以被reset的。有两种方法： 如果记得“最新”的hash（，则直接git reset –hard 1a222c3，则项目直接强制恢复到“最新”所在的状态。 如果不记得的话，运行git reflog，这个命令会输出一个列表，包含HEAD发生的所有变化。 cherry-pick cherry-pick其实在工作中还挺常用的，就像copy一样，把一个分之上的某个或者某几个commit复制到另一个分之。一种常见的场景就是，比如我在A分支做了几次commit以后，发现其实我并不应该在A分支上工作，应该在B分支上工作，这时就需要将这些commit从A分支复制到B分支去了，这时候就需要cherry-pick命令了 模拟下上面场景 在master分之上提交了两个commit 一个commit1 一个commit2，发现不应该在master上面操作，应该新建分之branch1123456首先新建分之 从commit0开始，并切到branch1git checkout -b branch1 commit0复制master分之 两个commit到当前分之git cherry-pick commit1 commit2在master分支上将这两个commit删除。先切回master分支：git checkout master，运行git reset --hard commit0 mergerebase]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascripts记录]]></title>
    <url>%2F2017%2F06%2F27%2Fjavascripts%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[这只是个愿望，有时间就看下，战线有可能会拖得很长，大家不要吐槽。 基本语法 赋值 1var a = 1; if 格式(if (条件) {执行语句} 支持嵌套) 12345678910if (2&gt;1) &#123; x = 1; y = 2; if (x&gt;y) &#123; result = 0; &#125; if (x&lt;y) &#123; result = 1; &#125;&#125; 注释 1/* 这是注释 */]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>javascripts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML/CSS]]></title>
    <url>%2F2017%2F06%2F27%2FHTML-CSS%2F</url>
    <content type="text"><![CDATA[熟悉前端是我一直以来的理想，有时间就看下总结下。。。 123456789101112131415&lt;html&gt;&lt;head&gt;&lt;style type=&quot;text/css&quot;&gt;h1 &#123;color: red&#125;p &#123;color: blue&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;header 1&lt;/h1&gt;&lt;p&gt;A paragraph.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 小白时期不了解HTML与CSS是怎么配合的，看上面代码 &lt;head&gt;标签中的CSS 声明h1颜色为红 p 颜色为蓝，其实就是把各个标签的样式在这个里面做一个总的约束，不用写到各个标签当中了。 提高了效率。 &lt;!DOCTYPE html&gt; test alert(‘Hello, world!’) &gt; &gt;]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>HTML,CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openvpn安装]]></title>
    <url>%2F2017%2F06%2F25%2Fopenvpn%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[openvpn不多作介绍，直接上部署过程 服务器环境 机器名：host01 操作系统：CentOS Linux release 7.0.1406 (Core) 内网IP：10.** 外网IP：12.** 安装方式：yum openvpn版本：OpenVPN 2.3.12 x86_64-redhat-linux-gnu 安装步骤安装前操作关闭selinux 配置防火墙 12345setenforce 0sed -i &apos;/^SELINUX=/c\SELINUX=disabled&apos; /etc/selinux/configiptables -I INPUT -p udp --dport 1194 -m comment --comment &quot;openvpn&quot; -j ACCEPTiptables -t nat -A POSTROUTING -s 10.8.0.0/24 -j MASQUERADE service iptables save 开启路由转发功能123sed -i &apos;/net.ipv4.ip_forward/s/0/1/&apos; /etc/sysctl.confecho &quot;1&quot;&gt;/proc/sys/net/ipv4/ip_forwardsysctl -p 安装openssl，lzo（用于压缩通讯数据，加快传输速度）12yum install openssl openssl-delvelyum install lzo 安装步骤安装配置openvpn和easy-rsa 1yum install openvpn easy-rsa 修改vars文件 123456789101112131415161718192021222324cat /usr/share/easy-rsa/2.0/vars | grep -Ev &quot;^$|#&quot;export EASY_RSA=&quot;`pwd`&quot;export OPENSSL=&quot;openssl&quot;export PKCS11TOOL=&quot;pkcs11-tool&quot;export GREP=&quot;grep&quot;export KEY_CONFIG=`$EASY_RSA/whichopensslcnf $EASY_RSA`export KEY_DIR=&quot;$EASY_RSA/keys&quot;echo NOTE: If you run ./clean-all, I will be doing a rm -rf on $KEY_DIRexport PKCS11_MODULE_PATH=&quot;dummy&quot;export PKCS11_PIN=&quot;dummy&quot;export KEY_SIZE=2048export CA_EXPIRE=3650export KEY_EXPIRE=3650export KEY_COUNTRY=&quot;CN&quot;export KEY_PROVINCE=&quot;CA&quot;export KEY_CITY=&quot;Bei Jing&quot;export KEY_ORG=&quot;Fort-Funston&quot;export KEY_EMAIL=&quot;me@myhost.mydomain&quot;export KEY_OU=&quot;MyOrganizationalUnit&quot;export KEY_NAME=&quot;EasyRSA&quot; **copy easy_rsa目录**cp -r /usr/share/easy-rsa/2.0/* /etc/openvpn/ 初始化环境变量1source vars 清除keys目录下所有与证书相关的文件 1./clean-all 生成根证书ca.crt 根秘钥ca.key（一路回车） 1./build-ca 为服务端生成证书秘钥（一路回车） 1./build-key-server server 创建迪菲·赫尔曼密钥，会生成dh2048.pem文件（过程比较慢） 1./build-dh 生成ta.key（防DOS攻击，等） 1openvpn --genkey --secret keys/ta.key 创建服务端配置文件 12在openvpn目录下创建一个keys目录mkdir /etc/openvpn/keys 复制一份刚创建好的证书秘钥到新创建的keys 1cp /usr/share/easy-rsa/2.0/keys/&#123;ca.crt,server.&#123;crt,key&#125;,dh2048.pem,ta.key&#125; /etc/openvpn/keys/ 复制一份配置文件模板到/etc/openvpn/ 1cp /usr/share/doc/openvpn-2.3.12/sample/sample-config-files/server.conf /etc/openvpn/ 修改一下配置文件(这里使用的udp,会比tcp更快一下)12345678910111213141516171819[root@vpn ~]# cat /etc/openvpn/server.conf | grep -Ev &quot;^#|;|^$&quot;port 1194proto udpdev tunca keys/ca.crtcert keys/server.crtkey keys/server.key # This file should be kept secretdh keys/dh2048.pemserver 10.8.3.0 255.255.255.0ifconfig-pool-persist ipp.txtpush &quot;route 10.0.0.0 255.0.0.0&quot;keepalive 10 120tls-auth keys/ta.key 0 # This file is secretcomp-lzopersist-keypersist-tunstatus openvpn-status.loglog openvpn.logverb 5 openvpn启动systemctl -f enable openvpn@server.servicesystemctl start openvpn@server.service 添加|删除用户添加用户脚本(可以使用它自动添加用户)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/bin/bashvpnServer1=10.9.104.39#vpnServer2=10.12.1.28# a. 在vpnserver01中创建新vpn用户if [ -z $1 ]then echo &quot;Error:请在脚本后添加用户名作为参数，例如：&apos;./01_addUser.sh zhangsan&apos;&quot;else cp /etc/openvpn/keys/$1.crt ./ &gt; /dev/null 2&gt;&amp;1 if [ -f $1.crt ] then echo &quot;$1 用户已存在，请检查！&quot; rm -f *.crt else cd /etc/openvpn/ &amp;&amp; pwd &amp;&amp; source /etc/openvpn/vars &amp;&amp; ./build-key --batch $1# cd /etc/openvpn/ &amp;&amp; pwd &amp;&amp; source /usr/share/easy-rsa/2.0/vars &amp;&amp; ./build-key --batch $1 # b. 新用户配置文件修改以及打包发送mail到用户 cd - mkdir -p $1cat &lt;&lt; EOF &gt;&gt; ./$1/$1.ovpnclientdev tunproto udpremote ****** 1194remote-randomresolv-retry 10nobindpersist-keypersist-tunca ca.crtcert $1.crtkey $1.keyremote-cert-tls servertls-auth ta.key 1comp-lzoverb 3EOF cp /etc/openvpn/keys/&#123;ca.crt,$1.&#123;crt,key&#125;,ta.key&#125; ./$1/# cp /etc/openvpn/keys/$1.&#123;crt,key&#125; /home/chunyu_sys/workspace/cy_ansible/roles/vpn_agent/files/ cp README.txt ./$1/;cp openVPN-clinet-config-for-Mac.pdf ./$1/ cp /etc/hosts ./$1/chunyu_hosts tar czf $1.tar.gz $1/ rm -fr $1/# python ./send_mail.py $1@chunyu.me &quot;[运维][vpn申请]openVPN configuration files&quot; $1.tar.gz# mutt -s &quot;openVPN configuration files&quot; -a $1.tar.gz -- xiepengcheng@chunyu.me &lt; $1.tar.gz #rm -f $1.tar.gz echo &quot;用户 $1 创建完毕&quot; fifi 删除用户脚本 12345678910111213141516171819#!/bin/bashvpnServer1=host01if [ -z $1 ]then echo &quot;Error:请在脚本后添加用户名作为参数，例如：&apos;./02_delUser.sh zhangsan&apos;&quot;else cp /etc/openvpn/keys/$1.crt ./ if [ -f $1.crt ] then cd /etc/openvpn &amp;&amp; source vars &amp;&amp; ./revoke-full $1 rm -rf /etc/openvpn/keys/$1.* echo &quot;用户 $1 删除完毕&quot; cd - rm -f $1.crt else echo &quot;$1 此VPN用户不存在，请检查!&quot; fifi 使用TC进行限速因为大家都不遵守规则，总是把vpn带宽占满，影响别的用户使用，所以必须加以限制 123456789tc qdisc add dev tun0 root handle 1:0 htb default 10tc class add dev tun0 parent 1:0 classid 1:1 htb rate 10Mbit burst 15ktc class add dev tun0 parent 1:1 classid 1:10 htb rate 640kbit ceil 640kbit burst 15ktc qdisc add dev tun0 parent 1:10 handle 10: sfq perturb 10tc filter add dev tun0 protocol ip parent 1:0 prio 3 u32 match ip dst 10.0.0.6 flowid 1:10 上面规则可以控制10.0.0.6这个用户的下载带宽为:80KB/s,以此类推限制其它用户.如果会写shell,可以作一个程序,加到openvpn的拨入脚本中. 上传是在eth0(假如外网卡是eth0)上做. 总结：如果iptables关了，openvpn服务起了，客户端还是连不上，telnet 1194端口也不通，记得把云主机外网防火墙改一下。]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>openvpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本检测硬盘]]></title>
    <url>%2F2017%2F06%2F25%2Fshell%E8%84%9A%E6%9C%AC%E6%A3%80%E6%B5%8B%E7%A1%AC%E7%9B%98%2F</url>
    <content type="text"><![CDATA[这个脚本是依赖dell提供的megacli工具写的，使用了一段时间，但是之后就使用了falcon的硬件监控工具，包括主板，风扇，硬盘，raid卡等比这个方便。 123456789101112131415161718192021222324252627#!/bin/shallhosts=&quot;host01 &quot;log_dir=/home/chunyu_sys/disklog/log_name=_raid_disk_monitorlogtime=$(date +%Y%m%d --date=&apos;1 days ago&apos;)fix=.logfor i in $allhosts;dohost=`ssh $i &quot;hostname&quot;`echo &quot;Checking RAID status on $host&quot; &gt;&gt; $log_dir$logtime$log_name$fixecho -e &quot;\033[31m $host \033[0m&quot;echo &quot;$host&quot; &gt;&gt;$log_dir$logtime$log_name$fixRAID_Contrller=`ssh $i &apos;megacli -AdpAllInfo -aALL |grep &quot;Product Name&quot; | cut -d: -f2&apos;`echo &quot;Controller : $RAID_Contrller&quot; &gt;&gt; $log_dir$logtime$log_name$fixOnline_disk_num=`ssh $i &apos;megacli -PDList -aALL | grep Online | wc -l&apos;`echo &quot;Totall number of Physical disks online : $Online_disk_num&quot; &gt;&gt; $log_dir$logtime$log_name$fixDegrade_disk=`ssh $i &apos;megacli -AdpAllInfo -a0 |grep &quot;Degrade&quot;&apos;`echo &quot;$Degrade_disk&quot; &gt;&gt;$log_dir$logtime$log_name$fixFailed_disk=`ssh $i &apos;megacli -AdpAllInfo -a0 |grep &quot;Failed Disks&quot;&apos;`echo &quot;$Failed_disk &quot; &gt;&gt;$log_dir$logtime$log_name$fix#Failed_disk_num=`ssh $i &apos;echo $Failed_disk |cut -d &quot; &quot; -f4&apos;`done]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell, 脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本自动安装zabbix_agent]]></title>
    <url>%2F2017%2F06%2F25%2Fshell%E8%84%9A%E6%9C%AC%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85zabbix-agent%2F</url>
    <content type="text"><![CDATA[现在公司都用ansible替换zabbix了，之前写的安装zabbix_agent脚本，在这儿记录一下，以备不时之需。 zabbix_agent_install.sh123456789101112131415161718192021222324252627#!/usr/bin/bash#allhosts=&quot;mysql1 mysql2 mysql3&quot;#可以执行远端命令for host in $allhostsdoecho $hostssh root@$host &quot;mkdir -p /usr/local/zabbix/etc/&quot;ssh root@$host &quot;mkdir -p /root/soft/&quot;ssh root@$host &quot;mkdir -p /var/log/zabbix/&quot;sed -i &quot;s/unknown/$host/g&quot; ./zabbix_agentd.confcat ./zabbix_agentd.confscp ./zabbix_agentd.conf root@$host:/usr/local/zabbix/etc/sed -i &quot;s/$host/unknown/g&quot; ./zabbix_agentd.confscp ./zabbix-2.4.2.tar.gz root@$host:/root/soft/scp ./install_zabbix_agent_impl.sh root@$host:/root/soft/ssh root@$host &quot;bash /root/soft/install_zabbix_agent_impl.sh&quot;done zabbix_agent_install_impl.sh12345678910111213#!/usr/bin/bashcd /root/softtar zxvf ./zabbix-2.4.2.tar.gzcd /root/soft/zabbix-2.4.2./configure --prefix=/usr/local/zabbix --enable-agentmake;make installgroupadd zabbixuseradd -g zabbix -s /sbin/nologin zabbixchown zabbix:zabbix /var/log/zabbixchown zabbix:zabbix /var/log/zabbix/usr/local/zabbix/sbin/zabbix_agentd -c /usr/local/zabbix/etc/zabbix_agentd.confecho &quot;/usr/local/zabbix/sbin/zabbix_agentd -c /usr/local/zabbix/etc/zabbix_agentd.conf&quot; &gt;&gt;/etc/rc.local zabbix_agent.conf 1234567891011121314PidFile=/var/log/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.log#server 为zabbix_server地址Server=192.168.0.1ListenPort=10050ServerActive=192.168.0.1Hostname=unknown#HostnameItem=system.hostnameRefreshActiveChecks=60BufferSize=1024UnsafeUserParameters=1Include=/usr/local/zabbix/etc/zabbix_agentd.conf.d/*.confTimeout=20AllowRoot=1 记得把安装包也放在脚本目录下]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell, 脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker命令]]></title>
    <url>%2F2017%2F06%2F23%2Fdocker%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这些命令运行都是在本地Mac上运行的并非线上。Mac安装docker参考官网之后最好申请个加速器阿里加速，再配置一下 Preferences–&gt;Deamon–&gt;Basic–&gt;Registry mirrors 镜像相关获取镜像1docker pull [选项] [Docker Registry地址]&lt;仓库名&gt;:&lt;标签&gt; 查看镜像显示镜像12docker imagesdocker images ubuntu:16.04 查看虚悬镜像(有些镜像升级后旧版本就会变成虚悬镜像[dangling image] 仓库 标签 都是 )1docker images -f dangling=True 删除虚悬镜像1docker rmi $(docker images -q -f dangling=true) 安装自定义格式显示12➜ workspace docker images --format &quot;&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;&quot;958a7ae9e569: nginx --filter 过滤器1docker images -f since=mongo:3.2 容器相关容器启动1docker run -d -p 80:80 --name webserver nginx 这条命令 是用nginx 镜像 启动一个名为webserver的 容器， 并且映射到80 端口 进到容器1docker exec -it webserver bash 可以进到容器里面对容器webserver容器进行更改 exit 退出 保存镜像(docker commit) 修改定制完容器，我们可以使用docker commit把它保存为新的镜像(但是因为修改历史不好查看，版本不好控制，一般很忌讳这样修改，要慎用) 12➜ docker commit --author &quot;fanquqi&quot; --message &quot;change index.html&quot; webserver nginx:v2sha256:2ad516b8fb8a76714e58a865d8099cbf6c31c36aabf0e4733b7d2950706674c3 可以通过docker images 查看1234➜ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx v2 2ad516b8fb8a 10 seconds ago 109 MBnginx latest 958a7ae9e569 3 weeks ago 109 MB 可以通过docker history 查看更改历史12345678910111213➜ workspace docker history nginx:v2IMAGE CREATED CREATED BY SIZE COMMENT2ad516b8fb8a About a minute ago nginx -g daemon off; 306 B change index.html958a7ae9e569 3 weeks ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daem... 0 B&lt;missing&gt; 3 weeks ago /bin/sh -c #(nop) STOPSIGNAL [SIGTERM] 0 B&lt;missing&gt; 3 weeks ago /bin/sh -c #(nop) EXPOSE 80/tcp 0 B&lt;missing&gt; 3 weeks ago /bin/sh -c ln -sf /dev/stdout /var/log/ngi... 22 B&lt;missing&gt; 3 weeks ago /bin/sh -c apt-get update &amp;&amp; apt-get inst... 52.2 MB&lt;missing&gt; 3 weeks ago /bin/sh -c #(nop) ENV NJS_VERSION=1.13.1.... 0 B&lt;missing&gt; 3 weeks ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.13.... 0 B&lt;missing&gt; 6 weeks ago /bin/sh -c #(nop) MAINTAINER NGINX Docker... 0 B&lt;missing&gt; 6 weeks ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0 B&lt;missing&gt; 6 weeks ago /bin/sh -c #(nop) ADD file:a90ec883129f86b... 57.1 MB 紧接着我们可以运行这个镜像到一个新的容器1docker run --name web2 -d -p 81:80 nginx:v2 使用Dockerfile]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker是什么]]></title>
    <url>%2F2017%2F06%2F23%2Fdocker%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[之前公司在我来之前有个哥们儿是负责docker的，但是一直没有推广起来，测试环境都没有，这个项目就不了了之了。但是就在前几天跟ucloud架构交流，说起他之前在云上搞docker根据业务自动扩容，带宽自动调整。说的我真的眼睛放光。决心研究一下。 我去年搞过虚拟化产品对虚拟化的产品做了调研。做了proxmox，类似kvm给测试服务用，开了几台虚拟机，这个产品支持机器热迁移等等。(这个博客刚开始写没多久，以后会把这些笔记也都放上来)其实虚拟化可以分为以下几种 对比传统虚拟化传统虚拟化最流行的虚拟化方法，使用Hypervisor这种中间层软件，在虚拟服务器和底层硬件之间建立一个抽象层。 Hypervisor可以捕获CPU指令，为指令访问硬件控制器和外设充当中介。因而，完全虚拟化技术几乎能让任何一款操作系统不用改动就能安装到虚拟服务器上，而它们不知道自己运行在虚拟化环境下。主要缺点是，性能方面不如裸机，因为Hypervisor需要占用一些资源，给处理器带来开销。 操作系统层虚拟化实现虚拟化还有一个方法，那就是在操作系统层面增添虚拟服务器功能。就操作系统层的虚拟化而言，没有独立的Hypervisor层。相反主机操作系统本身就负责在多个虚拟服务器之间分配硬件资源，并且让这些服务器彼此独立。一个明显的区别是，如果使用操作系统层虚拟化，所有虚拟服务器必须运行同一操作系统。 虽然操作系统层虚拟化的灵活性比较差，但本机速度性能比较高。此外，由于架构在所有虚拟服务器上使用单一、标准的操作系统，管理起来比异构环境要容易。docker属于操作系统层面的虚拟化。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。它与传统hyper-V 的区别在于理念，之前的虚拟化都是以操作系统为中心的，而docker是以应用为中心，把应用的环境代码等打包成镜像直接发布。服务启动也是秒级的。 docker优势 更高效的利用系统资源 更快速的启动时间 一致的运行环境 持续交付和部署 更轻松的迁移 更轻松的维护和扩展 这些优势都是运维喜欢的，所以运维要推行docker。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell命令之xargs与exec]]></title>
    <url>%2F2017%2F06%2F23%2Fshell%E5%91%BD%E4%BB%A4%E4%B9%8Bxargs%E4%B8%8Eexec%2F</url>
    <content type="text"><![CDATA[exec看起来就是执行某动作的命令格式为 -exec echo {} \;其中 echo 为动作, {} 为参数(即为之前找到的文件) \ 转义 ; 是结束符 示例1234567find ./ -name \*.yml -exec echo &#123;&#125; \;./logrotate_conf.yml./filebeat.yml./group_vars/all.yml./group_vars/ehr.yml./group_vars/http_proxy.yml./group_vars/medweb.yml xargsxargs依赖管道,下边直接写了几个个实例 基本使用 把多行文件合并成一行 或者每行指定元素个数(-n 参数) 格式输出 123456789101112131415161718192021222324252627282930[root@md6 ~]# cat test.txta b c d e f g h i j k l m n o p q r st u v w x y z[root@md6 ~]# cat test.txt | xargs echoa b c d e f g h i j k l m n o p q r s t u v w x y z[root@md6 ~]# cat test.txt | xargs echo &gt; test1.txt[root@md6 ~]# cat test1.txta b c d e f g h i j k l m n o p q r s t u v w x y z[root@md6 ~]# cat test.txt | xargs -n 4a b c de f g hi j k lm n o pq r s tu v w xy z[root@md6 ~]# cat test.txt | xargs -n 3a b cd e fg h ij k lm n op q rs t uv w xy z 实例1 直接打印 12find ./ -name \*.yml | xargs echo./logrotate_conf.yml ./filebeat.yml ./group_vars/all.yml ./group_vars/ehr.yml ./group_vars/http_proxy.yml ./group_vars/medweb.yml 实例2 查找文件内容中带hostname的 1find . -type f -print | xargs grep -n &quot;hostname&quot; (-n输出行号) 实例3 mv 或者 cp 使用-i参数 -p 打印过程(交互确认) 1ls *.txt | xargs -n1 -i -p cp &#123;&#125; /var/tmp/ 实例4 查找当下文件按大小排序 1find . -maxdepth 1 ! -name &quot;.&quot; -print0 | xargs -0 du -b | sort -nr | head -10 | nl 实例5 结合sed替换 1find . -name &quot;*.txt&quot; -print0 | xargs -0 sed -i &apos;s/aaa/bbb/g&apos; 总结首先使用方式不一样，exec操作麻烦些 xargs更简单直接方法多样另外从输出结果看得出 exec是遇到一个找到的文件就执行一次命令，xargs是把结果放到一起，执行一次。(这样可以使用在 把多行文件合并成一行的特定场景中)需要强调xargs遇到文件名中有空格的行为是处理不了的。这种情况可以这样 1find . -name \*.txt -print0 | xargs -0 rm 其中find找到每个文件定义以null字符结尾 xargs找文件定义null分割文件 就可以了]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率神器Alfred使用]]></title>
    <url>%2F2017%2F06%2F21%2F%E6%95%88%E7%8E%87%E7%A5%9E%E5%99%A8Alfred%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[如果看过蝙蝠侠的同学可能会感觉到熟悉，Alfred明明是蝙蝠侠的管家嘛，是的，我严重怀疑开发者是蝙蝠侠的铁粉。这里这个Alfred也可以成为你的管家，简直无所不至。 下载地址: https://xclient.info 这里主要记录几个常用的功能。 首先快捷键介绍(可以自定义) alt + space 打开主应用 alt + command + c 粘贴板 web search 我们不需要打开浏览器 只需要在主应用中 输入”Google keyword” 回车 就可以Google你所想要的内容 默认给出Amazon Facebook等 可以自定义添加 “百度”,”知乎”等 使用方式: 打开主应用 输入 Google keyword file search 本地文件搜索 使用方式: 打开主应用 输入 ‘filename 粘贴板 可以记录一天的历史复制内容(可以灵活设置，默认一天) 密码记录 首先安装1password 配置(收费) workflows 工作流，类似TODOlist，据说很强大,感兴趣的可以学习下,我并没使用此功能。]]></content>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种mysql迁移]]></title>
    <url>%2F2017%2F06%2F16%2F%E5%87%A0%E7%A7%8Dmysql%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[运维在日常免不了迁移服务，而迁移服务又免不了迁移数据库。 通常情况下数据库还比较重要，所以怎么样平滑的迁移数据库就显得很重要。我在前段时间迁移了青云几台机器的服务，其中涉及到一些MySQL数据库的操作，做了些记录。 mysqldump 这是比较常见，一般数据库数据量很小的时候，首要我们就会考虑这个。优点:在于能够与正在运行的 MySQL 自动协同工作，支持备份 InnoDB 以及 MyISAM 表；锁表既是优点又是缺点，看怎么看待；缺点:在于数据量大，备份速度慢； 这次服务迁移 中涉及到我们的OA系统，之前是部署在一个windows-server上的。 需求 windows 上mysql迁移到 centos7 之前单实例换为双主(haproxy代理) 迁移具体过程在centos7机器直接远程dump1mysqldump -h117.******* -uoa -p oa --flush-logs --single-transaction | mysql -h localhost -uoa -p -P3314 -S /home/mysql/3314/mysql.sock v50 --flush-logs在开始导出前刷新服务器的日志文件。注意，如果你一次性导出很多数据库（使用 -databases=或–all-databases选项），导出每个库时都会触发日志刷新。例外是当使用了–lock-all-tables或–master-data时：日志只会被刷新一次，那个时候所有表都会被锁住。所以如果你希望你的导出和日志刷新发生在同一个确定的时刻，你需要使用–lock-all-tables，或者–master-data配合–flush-logs。 --single-transactionInnoDB 表在备份时，通常启用选项 –single-transaction 来保证备份的一致性，实际上它的工作原理是设定本次会话的隔离级别为：REPEATABLE READ，以确保本次会话(dump)时，不会看到其他会话已经提交了的数据。 之后查看windows机器的bin-log日志，查看在备份时间节点新的bin-log文件 之后建立两个mysql主从(期间我centos7机器的mysql双主已经使用ansible搭建成功在此省略) windows——MySQL 授权123设置chunyu账号负责数据同步GRANT REPLICATION SLAVE,RELOAD,SUPER ON *.* TO &apos;chunyu&apos;@&apos;%&apos; IDENTIFIED BY &apos;123&apos;;FLUSH PRIVILEGES ; centos7_1 停掉双主中自己slave角色，重新设置master12345STOP SLAVE;RESET SLAVE;change master to master_host=&apos;117.****&apos;,master_port=3306,master_user=&apos;chunyu&apos;,master_password=&apos;123&apos;,master_log_file=&apos;mysql-bin.000008&apos;,master_log_pos=120;start slave;show slave status; 因为centos是双主，所以需要先断掉这台机的slave，另一台centos不用操作，这样双主变成了主从，在设置windows——MySQL为主此时关系应该是:win_mysql(master) centos7_1(slave)centos7_1(master) centos7_2(slave) 之后服务迁移完之后，可以再设置回双主此处省略。 percona-xtrabackup 这台是redmine的数据库数据比较多，部署在ubuntu上。数据量大可以使用xtrabackup 热备份。 优点: 可靠高效的备份DB;备份过程中不中断事务处理，热备份;快速进行恢复等。缺点: 需求 ubuntu上mysql迁移到centos7 mysql 之前单实例换为双主(haproxy代理) 操作过程 也是先备份传输，建立主从，恢复双主。 ubuntu上 备份123apt-get install percona-xtrabackupinnobackupex --user=root --password=123 ./tar -czvf mysql_back.tar.gz 2017-04-25_10-58-40 centos7 传输 解压 放到相应目录12345rsync -auvzP --bwlimit=5000 root@117.****:/mnt/sdc/mysql_back.tar.gz ./cd /home/mysql/ &amp;&amp; mv data/data_backtar -zxvf mysql_back.tar.gz mv 2017-04-25_10-58-40/ datachown -R mysql:mysql data postion点查看:12[root@test_biz 2017-06-16_15-39-01]# cat xtrabackup_binlog_infomysql-bin.000054 57952335 之后主从设置 迁移完服务之后恢复双主。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rpm包制作]]></title>
    <url>%2F2017%2F06%2F16%2Frpm%E5%8C%85%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[今天突然收到老大发的这篇文章分分钟拯救监控知识体系，看到硬件监控的时候。突然想到我们是用的我们的硬件监控只是用了Dell的megacli工具监控了raid和磁盘的状态，连CPU温度主板温度这些基本指标好像都没有。因为我们用的open-falcon于是google下看到这篇open-falconHWcheck [toc] 制作过程123 git clone https://github.com/51web/hwcheck hwcheck-0.2tar czf hwcheck-0.2.tar.gz hwcheck-0.2rpmbuild -tb hwcheck-0.2.tar.gz 具体可以使用 rpm –help 查看帮助 rpm包相关操作RPM包安装：1rpm -ivh example.rpm 查看已经安装的RPM包12rpm -qa rpm -qa | grep tomcat4 查看 tomcat4 是否被安装； 验证RPM包 这个可用作系统有问题的时候或者是有人恶意更改系统文件的时候 12345[root@md5 lib]# rpm -Vf /etc/*S.5....T. c /etc/sysctl.conf.......T. c /etc/bashrcS.5....T. c /etc/hosts.allowS.5....T. c /etc/hosts.deny 解释下上边的命令其中，S 表示文件大小修改过，T 表示文件日期修改过。具体看man page]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[curl查看接口各阶段响应时间]]></title>
    <url>%2F2017%2F06%2F16%2Fcurl%E6%9F%A5%E7%9C%8B%E6%8E%A5%E5%8F%A3%E5%90%84%E9%98%B6%E6%AE%B5%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[使用场景 有接口查询比较慢，需要查找具体到底是哪块响应时间比较长，然后再定位具体的问题。 curl 有个 -w参数。可以格式化输出我们可以使用。它能够按照指定的格式打印某些信息，里面可以使用某些特定的变量，而且支持 \n、\t和 \r 转义字符。提供的变量很多，比如 status_code、local_port、sizedownload 等等，这篇文章我们只关注和请求时间有关的变量（以 time 开头的变量）。 具体用法 1234567891011➜ ~ curl &apos;https://api.mch.weixin.qq.com/pay/unifiedorder&apos; -w &apos;\ntime_namelookup: %&#123;time_namelookup&#125;\ntime_connect: %&#123;time_connect&#125;\ntime_appconnect: %&#123;time_appconnect&#125;\ntime_pretransfer: %&#123;time_pretransfer&#125;\ntime_redirect: %&#123;time_redirect&#125;\ntime_starttransfer: %&#123;time_starttransfer&#125;\ntotal: %&#123;time_total&#125;\n&apos;&lt;xml&gt;&lt;return_code&gt;&lt;![CDATA[FAIL]]&gt;&lt;/return_code&gt;&lt;return_msg&gt;&lt;![CDATA[请使用post方法]]&gt;&lt;/return_msg&gt;&lt;/xml&gt;time_namelookup: 0.013time_connect: 0.018time_appconnect: 0.113time_pretransfer: 0.113time_redirect: 0.000time_starttransfer: 0.180total: 0.180 具体变量解读： time_namelookup：DNS 域名解析的时候，就是把 URL 转换成 ip 地址的过程time_connect：TCP 连接建立的时间，就是三次握手的时间time_appconnect：SSL/SSH 等上层协议建立连接的时间，比如 connect/handshake 的时间time_redirect：从开始到最后一个请求事务的时间time_pretransfer：从请求开始到响应开始传输的时间time_starttransfer：从请求开始到第一个字节将要传输的时间time_total：这次请求花费的全部时间 而我们这个接口使用的是openvpn访问的外网，openvpn使用的openssl认证。所以基本定位到问题就在openvpn的ssl认证过程中。]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash优化]]></title>
    <url>%2F2017%2F06%2F15%2FLogstash%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前部署的ELK只加了nginx_access, elapsed两个日志，每天的日志量有400G左右，架构可以撑住。但是现在又引入了一些新的日志，并且随着SEO部门的来临。每天晚上服务都被爬虫爬。日志量也翻了两三翻。产生了日志延时的问题。有的延时能有几个小时。看kibana中上传的时间跟日志里的时间差距好几个小时。说明日志已经阻塞了。 问题分析这期间机器负载啥的一直也没有报警，ES集群也很健康，所以初步估计是logstash遇到了瓶颈。因为我们有grok，之前就听说这个过滤会对速度影响很大，还有geoip等。 解决办法我第一想到的是，收集日志换成filebeat然后像之前加的syslog一样放到kafka缓冲一下，再用logstash插入到ES，但是无奈，机器资源有限，kafka又是直接写问加你系统，运维机的磁盘已经很满了，在扩容磁盘之前是无望了。所以现在只能优化一下logstash，坚持到磁盘扩容。 logstash优化 加大内存 增加线程 减少grok或者去掉geoip定位 另外更改@timestamp 参考:http://udn.yyuap.com/doc/logstash-best-practice-cn/filter/date.html为了防止每次延时比较大的时候图像显示不准确。]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>ELK, logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django之入门学习]]></title>
    <url>%2F2017%2F06%2F14%2FDjango%E4%B9%8B%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[自己动手从0开始 python版本：3.5.1Django版本：1.9.5 安装初始化python312345678910yum groupinstall &apos;Development Tools&apos;yum install zlib-devel bzip2-devel openssl-devel ncurese-develwget https://www.python.org/ftp/python/3.5.1/Python-3.5.1.tar.xz &amp;&amp; mv Python-3.5.1.tar.xz /usr/local/ &amp;&amp; cd /usr/localtar -Jxvf Python-3.5.1.tar.xz &amp;&amp; cd Python-3.5.1./configure --prefix=/usr/local/python3make &amp;&amp; make install ln -s /usr/local/python3/bin/python3.5 /usr/bin/python3ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3# python3 -m venv myvenv django安装12source myvenv/bin/activatepip install django==1.9.5 生成项目1django-admin.py startproject mysite mysite目录结构如下 1234567mysite├───manage.py└───mysite settings.py urls.py wsgi.py __init__.py manage.py是管理网站的脚本，可以使用它来启动一个简单的web服务器，这个对于开发调试非常有用。 setting.py是工程的核心配置文件。 urls.py是路径配置文件，可以配置URL到实际Controller的映射关系。 配置更改更给settings.py语言，时区 12LANGUAGE_CODE = &apos;zh-cn&apos;TIME_ZONE = &apos;Asia/Shanghai&apos; 数据库不用改 先使用默认的sqlite3,生成数据库1python manage.py migrate runserver 看下，可以再浏览器访问下 ip:8000python manage.py runserver 0.0.0.0:8000 创建应用django 中 工程(project)与应用(application)概念要分清，我们使用 django-admin.py manage.py startproject mysite创建mysite的是工程.下边使用python manage.py startapp blog 创建的blog是应用，一个工程可以包括很多个应用。 创建应用python manage.py startapp blog现在整个工程目录结构如下12345678910111213141516171819202122.├── blog│ ├── admin.py│ ├── apps.py│ ├── __init__.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py├── db.sqlite3├── manage.py└── mysite ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-35.pyc │ ├── settings.cpython-35.pyc │ ├── urls.cpython-35.pyc │ └── wsgi.cpython-35.pyc ├── settings.py ├── urls.py └── wsgi.py 然后，我们需要把这个应用在settings.py注册一下。123456789INSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;blog&apos;,]]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blog收藏]]></title>
    <url>%2F2017%2F06%2F13%2FBlog%E6%94%B6%E8%97%8F%2F</url>
    <content type="text"><![CDATA[以上的博客并无排名，只是随着自己发现的时间排序 架构 酷壳 峰云 徐亮偉架构师之路 运维 流水理鱼 pmars reboot 阿小信的博客 安全 离别歌 前端 进击的马斯特 Jerry Qu python alex 系统管理员资源大全 系统管理员资源大全]]></content>
  </entry>
  <entry>
    <title><![CDATA[uwsgi_worker监控脚本]]></title>
    <url>%2F2017%2F06%2F12%2Fuwsgi-worker%E7%9B%91%E6%8E%A7%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[该脚本为falcon上报uwsgi的监控数据，总worker数，busy_worker数，busy率。 需要写到cron中定时去跑。push过程忽略，使用的为内部封装的op_tools包。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# Ansible managed: /data/home/chunyu_sys/workspace/cy_ansible/roles/zabbix_agent/templates/uwsgi_busy_count.py.j2 modified on 2016-09-14 23:41:10 by chunyu_sys on control# Do NOT modify this file by hand!import socketimport jsonimport sysfrom op_tools import falconproject_or_socket = &quot;medweb&quot;uwsgi_map = &#123;&#125;uwsgi_map[&apos;medweb&apos;]=&apos;/home/chunyu/workspace/medweb/log/uwsgi_stats.socket&apos;uwsgi_map[&apos;cmsapi&apos;]=&apos;/tmp/stats_cmsapi.socket&apos;addr = uwsgi_map.get(project_or_socket, project_or_socket)data_type = &quot;busy&quot;sfamily = socket.AF_UNIXs = socket.socket(sfamily, socket.SOCK_STREAM)s.connect(addr)js = &quot;&quot;while True: data = s.recv(4096) if len(data) &lt; 1: break js += data.decode(&apos;utf8&apos;)dd = json.loads(js)workers = dd[&quot;workers&quot;]busy_count = 0total_count = len(workers)for worker in workers: if worker[&quot;status&quot;] == &quot;busy&quot;: busy_count += 1busy_rate = busy_count/float(total_count or 1)dic_count = &#123; &apos;busy_count&apos; : busy_count, &apos;busy_rate&apos; : busy_rate, &apos;total_count&apos; : total_count, &#125;metric = &quot;medweb_uwsgi_busy_worker&quot;collect_step = 60counter_type = falcon.CounterType.GAUGEfor key in dic_count: tag = &quot;type=&quot; + key value = dic_count.get(key) info = falcon.build_metric_info(metric, value, collect_step, counter_type, tags=tag) print info falcon.push_metric_info_list_to_falcon([info])]]></content>
      <categories>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>scripts, 监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运维工具之sar命令]]></title>
    <url>%2F2017%2F06%2F09%2F%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7%E4%B9%8Bsar%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[参考:http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/index.html sar是System Activity Reporter（系统活动情况报告）的缩写。sar工具将对系统当前的状态进行取样，然后通过计算数据和比例来表达系统的当前运行状态。它的特点是可以连续对系统取样，获得大量的取样数据；取样数据和分析的结果都可以存入文件，所需的负载很小。sar是目前Linux上最为全面的系统性能分析工具之一，可以从14个大方面对系统的活动进行报告，包括文件的读写情况、系统调用的使用情况、串口、CPU效率、内存使用状况、进程活动及IPC有关的活动等，使用也是较为复杂。 sar是查看操作系统报告指标的各种工具中，最为普遍和方便的；它有两种用法； 追溯过去的统计数据（默认） 周期性的查看当前数据]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>sar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[falcon问题处理]]></title>
    <url>%2F2017%2F06%2F08%2Ffalcon%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[grafana上面不支持open-falcon的大写metric(mymon metric更改小写) (这是openfalcon的bug，falcon-plus中已经修复) 但是我们现在使用的falcon没有升级，而且升级的得计划暂时没有安排。我们只能手动更改metric不用大写尽量用小写。以下记录mymon更改。mymon是使用go语言编写的，用来监控mysql的插件。mymon代码链接由于之前的mymon是通过ansible 脚本直接git pull下来的，所以有源码在，可以更改之后再build一下。找到metric.go修改大写的metric为小写。 修改完脚本如下。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;time&quot; &quot;strings&quot; #添加string)const ( TIME_OUT = 30 ORIGIN = &quot;GAUGE&quot; DELTA_PS = &quot;COUNTER&quot; DELTA = &quot;&quot;)// COUNTER: Speed per second// GAUGE: Original, DEFAULTvar DataType = map[string]string&#123; &quot;Innodb_buffer_pool_reads&quot;: DELTA_PS, &quot;Innodb_buffer_pool_read_requests&quot;: DELTA_PS, &quot;Innodb_compress_time&quot;: DELTA_PS, &quot;Innodb_data_fsyncs&quot;: DELTA_PS, &quot;Innodb_data_read&quot;: DELTA_PS, &quot;Innodb_data_reads&quot;: DELTA_PS, &quot;Innodb_data_writes&quot;: DELTA_PS, &quot;Innodb_data_written&quot;: DELTA_PS, &quot;Innodb_last_checkpoint_at&quot;: DELTA_PS, &quot;Innodb_log_flushed_up_to&quot;: DELTA_PS, &quot;Innodb_log_sequence_number&quot;: DELTA_PS, &quot;Innodb_mutex_os_waits&quot;: DELTA_PS, &quot;Innodb_mutex_spin_rounds&quot;: DELTA_PS, &quot;Innodb_mutex_spin_waits&quot;: DELTA_PS, &quot;Innodb_pages_flushed_up_to&quot;: DELTA_PS, &quot;Innodb_rows_deleted&quot;: DELTA_PS, &quot;Innodb_rows_inserted&quot;: DELTA_PS, &quot;Innodb_rows_locked&quot;: DELTA_PS, &quot;Innodb_rows_modified&quot;: DELTA_PS, &quot;Innodb_rows_read&quot;: DELTA_PS, &quot;Innodb_rows_updated&quot;: DELTA_PS, &quot;Innodb_row_lock_time&quot;: DELTA_PS, &quot;Innodb_row_lock_waits&quot;: DELTA_PS, &quot;Innodb_uncompress_time&quot;: DELTA_PS, &quot;Binlog_event_count&quot;: DELTA_PS, &quot;Binlog_number&quot;: DELTA_PS, &quot;Slave_count&quot;: DELTA_PS, &quot;Com_admin_commands&quot;: DELTA_PS, &quot;Com_assign_to_keycache&quot;: DELTA_PS, &quot;Com_alter_db&quot;: DELTA_PS, &quot;Com_alter_db_upgrade&quot;: DELTA_PS, &quot;Com_alter_event&quot;: DELTA_PS, &quot;Com_alter_function&quot;: DELTA_PS, &quot;Com_alter_procedure&quot;: DELTA_PS, &quot;Com_alter_server&quot;: DELTA_PS, &quot;Com_alter_table&quot;: DELTA_PS, &quot;Com_alter_tablespace&quot;: DELTA_PS, &quot;Com_analyze&quot;: DELTA_PS, &quot;Com_begin&quot;: DELTA_PS, &quot;Com_binlog&quot;: DELTA_PS, &quot;Com_call_procedure&quot;: DELTA_PS, &quot;Com_change_db&quot;: DELTA_PS, &quot;Com_change_master&quot;: DELTA_PS, &quot;Com_check&quot;: DELTA_PS, &quot;Com_checksum&quot;: DELTA_PS, &quot;Com_commit&quot;: DELTA_PS, &quot;Com_create_db&quot;: DELTA_PS, &quot;Com_create_event&quot;: DELTA_PS, &quot;Com_create_function&quot;: DELTA_PS, &quot;Com_create_index&quot;: DELTA_PS, &quot;Com_create_procedure&quot;: DELTA_PS, &quot;Com_create_server&quot;: DELTA_PS, &quot;Com_create_table&quot;: DELTA_PS, &quot;Com_create_trigger&quot;: DELTA_PS, &quot;Com_create_udf&quot;: DELTA_PS, &quot;Com_create_user&quot;: DELTA_PS, &quot;Com_create_view&quot;: DELTA_PS, &quot;Com_dealloc_sql&quot;: DELTA_PS, &quot;Com_delete&quot;: DELTA_PS, &quot;Com_delete_multi&quot;: DELTA_PS, &quot;Com_do&quot;: DELTA_PS, &quot;Com_drop_db&quot;: DELTA_PS, &quot;Com_drop_event&quot;: DELTA_PS, &quot;Com_drop_function&quot;: DELTA_PS, &quot;Com_drop_index&quot;: DELTA_PS, &quot;Com_drop_procedure&quot;: DELTA_PS, &quot;Com_drop_server&quot;: DELTA_PS, &quot;Com_drop_table&quot;: DELTA_PS, &quot;Com_drop_trigger&quot;: DELTA_PS, &quot;Com_drop_user&quot;: DELTA_PS, &quot;Com_drop_view&quot;: DELTA_PS, &quot;Com_empty_query&quot;: DELTA_PS, &quot;Com_execute_sql&quot;: DELTA_PS, &quot;Com_flush&quot;: DELTA_PS, &quot;Com_grant&quot;: DELTA_PS, &quot;Com_ha_close&quot;: DELTA_PS, &quot;Com_ha_open&quot;: DELTA_PS, &quot;Com_ha_read&quot;: DELTA_PS, &quot;Com_help&quot;: DELTA_PS, &quot;Com_insert&quot;: DELTA_PS, &quot;Com_insert_select&quot;: DELTA_PS, &quot;Com_install_plugin&quot;: DELTA_PS, &quot;Com_kill&quot;: DELTA_PS, &quot;Com_load&quot;: DELTA_PS, &quot;Com_lock_tables&quot;: DELTA_PS, &quot;Com_optimize&quot;: DELTA_PS, &quot;Com_preload_keys&quot;: DELTA_PS, &quot;Com_prepare_sql&quot;: DELTA_PS, &quot;Com_purge&quot;: DELTA_PS, &quot;Com_purge_before_date&quot;: DELTA_PS, &quot;Com_release_savepoint&quot;: DELTA_PS, &quot;Com_rename_table&quot;: DELTA_PS, &quot;Com_rename_user&quot;: DELTA_PS, &quot;Com_repair&quot;: DELTA_PS, &quot;Com_replace&quot;: DELTA_PS, &quot;Com_replace_select&quot;: DELTA_PS, &quot;Com_reset&quot;: DELTA_PS, &quot;Com_resignal&quot;: DELTA_PS, &quot;Com_revoke&quot;: DELTA_PS, &quot;Com_revoke_all&quot;: DELTA_PS, &quot;Com_rollback&quot;: DELTA_PS, &quot;Com_rollback_to_savepoint&quot;: DELTA_PS, &quot;Com_savepoint&quot;: DELTA_PS, &quot;Com_select&quot;: DELTA_PS, &quot;Com_set_option&quot;: DELTA_PS, &quot;Com_signal&quot;: DELTA_PS, &quot;Com_show_authors&quot;: DELTA_PS, &quot;Com_show_binlog_events&quot;: DELTA_PS, &quot;Com_show_binlogs&quot;: DELTA_PS, &quot;Com_show_charsets&quot;: DELTA_PS, &quot;Com_show_collations&quot;: DELTA_PS, &quot;Com_show_contributors&quot;: DELTA_PS, &quot;Com_show_create_db&quot;: DELTA_PS, &quot;Com_show_create_event&quot;: DELTA_PS, &quot;Com_show_create_func&quot;: DELTA_PS, &quot;Com_show_create_proc&quot;: DELTA_PS, &quot;Com_show_create_table&quot;: DELTA_PS, &quot;Com_show_create_trigger&quot;: DELTA_PS, &quot;Com_show_databases&quot;: DELTA_PS, &quot;Com_show_engine_logs&quot;: DELTA_PS, &quot;Com_show_engine_mutex&quot;: DELTA_PS, &quot;Com_show_engine_status&quot;: DELTA_PS, &quot;Com_show_events&quot;: DELTA_PS, &quot;Com_show_errors&quot;: DELTA_PS, &quot;Com_show_fields&quot;: DELTA_PS, &quot;Com_show_function_status&quot;: DELTA_PS, &quot;Com_show_grants&quot;: DELTA_PS, &quot;Com_show_keys&quot;: DELTA_PS, &quot;Com_show_master_status&quot;: DELTA_PS, &quot;Com_show_open_tables&quot;: DELTA_PS, &quot;Com_show_plugins&quot;: DELTA_PS, &quot;Com_show_privileges&quot;: DELTA_PS, &quot;Com_show_procedure_status&quot;: DELTA_PS, &quot;Com_show_processlist&quot;: DELTA_PS, &quot;Com_show_profile&quot;: DELTA_PS, &quot;Com_show_profiles&quot;: DELTA_PS, &quot;Com_show_relaylog_events&quot;: DELTA_PS, &quot;Com_show_slave_hosts&quot;: DELTA_PS, &quot;Com_show_slave_status&quot;: DELTA_PS, &quot;Com_show_status&quot;: DELTA_PS, &quot;Com_show_storage_engines&quot;: DELTA_PS, &quot;Com_show_table_status&quot;: DELTA_PS, &quot;Com_show_tables&quot;: DELTA_PS, &quot;Com_show_triggers&quot;: DELTA_PS, &quot;Com_show_variables&quot;: DELTA_PS, &quot;Com_show_warnings&quot;: DELTA_PS, &quot;Com_slave_start&quot;: DELTA_PS, &quot;Com_slave_stop&quot;: DELTA_PS, &quot;Com_stmt_close&quot;: DELTA_PS, &quot;Com_stmt_execute&quot;: DELTA_PS, &quot;Com_stmt_fetch&quot;: DELTA_PS, &quot;Com_stmt_prepare&quot;: DELTA_PS, &quot;Com_stmt_reprepare&quot;: DELTA_PS, &quot;Com_stmt_reset&quot;: DELTA_PS, &quot;Com_stmt_send_long_data&quot;: DELTA_PS, &quot;Com_truncate&quot;: DELTA_PS, &quot;Com_uninstall_plugin&quot;: DELTA_PS, &quot;Com_unlock_tables&quot;: DELTA_PS, &quot;Com_update&quot;: DELTA_PS, &quot;Com_update_multi&quot;: DELTA_PS, &quot;Com_xa_commit&quot;: DELTA_PS, &quot;Com_xa_end&quot;: DELTA_PS, &quot;Com_xa_prepare&quot;: DELTA_PS, &quot;Com_xa_recover&quot;: DELTA_PS, &quot;Com_xa_rollback&quot;: DELTA_PS, &quot;Com_xa_start&quot;: DELTA_PS, &quot;Aborted_clients&quot;: DELTA_PS, &quot;Aborted_connects&quot;: DELTA_PS, &quot;Access_denied_errors&quot;: DELTA_PS, &quot;Binlog_bytes_written&quot;: DELTA_PS, &quot;Binlog_cache_disk_use&quot;: DELTA_PS, &quot;Binlog_cache_use&quot;: DELTA_PS, &quot;Binlog_stmt_cache_disk_use&quot;: DELTA_PS, &quot;Binlog_stmt_cache_use&quot;: DELTA_PS, &quot;Bytes_received&quot;: DELTA_PS, &quot;Bytes_sent&quot;: DELTA_PS, &quot;Connections&quot;: DELTA_PS, &quot;Created_tmp_disk_tables&quot;: DELTA_PS, &quot;Created_tmp_files&quot;: DELTA_PS, &quot;Created_tmp_tables&quot;: DELTA_PS, &quot;Handler_delete&quot;: DELTA_PS, &quot;Handler_read_first&quot;: DELTA_PS, &quot;Handler_read_key&quot;: DELTA_PS, &quot;Handler_read_last&quot;: DELTA_PS, &quot;Handler_read_next&quot;: DELTA_PS, &quot;Handler_read_prev&quot;: DELTA_PS, &quot;Handler_read_rnd&quot;: DELTA_PS, &quot;Handler_read_rnd_next&quot;: DELTA_PS, &quot;Handler_update&quot;: DELTA_PS, &quot;Handler_write&quot;: DELTA_PS, &quot;Opened_files&quot;: DELTA_PS, &quot;Opened_tables&quot;: DELTA_PS, &quot;Opened_table_definitions&quot;: DELTA_PS, &quot;Qcache_hits&quot;: DELTA_PS, &quot;Qcache_inserts&quot;: DELTA_PS, &quot;Qcache_lowmem_prunes&quot;: DELTA_PS, &quot;Qcache_not_cached&quot;: DELTA_PS, &quot;Queries&quot;: DELTA_PS, &quot;Questions&quot;: DELTA_PS, &quot;Select_full_join&quot;: DELTA_PS, &quot;Select_full_range_join&quot;: DELTA_PS, &quot;Select_range_check&quot;: DELTA_PS, &quot;Select_scan&quot;: DELTA_PS, &quot;Slow_queries&quot;: DELTA_PS, &quot;Sort_merge_passes&quot;: DELTA_PS, &quot;Sort_range&quot;: DELTA_PS, &quot;Sort_rows&quot;: DELTA_PS, &quot;Sort_scan&quot;: DELTA_PS, &quot;Table_locks_immediate&quot;: DELTA_PS, &quot;Table_locks_waited&quot;: DELTA_PS, &quot;Threads_created&quot;: DELTA_PS,&#125;type MysqlIns struct &#123; Host string Port int Tag string&#125;func dataType(key_ string) string &#123; if v, ok := DataType[key_]; ok &#123; return v &#125; return ORIGIN&#125;type MetaData struct &#123; Metric string `json:&quot;metric&quot;` //key Endpoint string `json:&quot;endpoint&quot;` //hostname Value interface&#123;&#125; `json:&quot;value&quot;` // number or string CounterType string `json:&quot;counterType&quot;` // GAUGE 原值 COUNTER 差值(ps) Tags string `json:&quot;tags&quot;` // port=3306,k=v Timestamp int64 `json:&quot;timestamp&quot;` Step int64 `json:&quot;step&quot;`&#125;func (m *MetaData) String() string &#123; s := fmt.Sprintf(&quot;MetaData Metric:%s Endpoint:%s Value:%v CounterType:%s Tags:%s Timestamp:%d Step:%d&quot;, m.Metric, m.Endpoint, m.Value, m.CounterType, m.Tags, m.Timestamp, m.Step) return s&#125;func NewMetric(name string) *MetaData &#123; return &amp;MetaData&#123; Metric: strings.ToLower(name), #name 改为strings.ToLower(name) Endpoint: hostname(), CounterType: dataType(name), Tags: fmt.Sprintf(&quot;port=%d&quot;, cfg.Port), Timestamp: time.Now().Unix(), Step: 60, &#125;&#125;func hostname() string &#123; host := cfg.Endpoint if host != &quot;&quot; &#123; return host &#125; host, err := os.Hostname() if err != nil &#123; host = cfg.Host &#125; return host&#125;func (m *MetaData) SetValue(v interface&#123;&#125;) &#123; m.Value = v&#125; 之后在mymon目录 执行12go get ./...go build -o mymon 即可 之后可以去数据库手动删掉openfalcon 过期的metric(但是新的metric并没有更新，反而旧metric没了数据，可以试着修改endpoint，然后去数据区删除旧的endpoint)]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>falcon, 监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime使用]]></title>
    <url>%2F2017%2F06%2F08%2Fsublime%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[mac下zsh直接用sublime打开文件夹建一个软链12ln -s /Applications/Sublime\ Text.app/Contents/SharedSupport/bin/subl /usr/bin/sublsubl workspace/medweb 会直接打开medweb工程，很方便 配置文件 建议直接传github 这样之后换电脑或者离职之后就很方便了。]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix迁移到open-falcon]]></title>
    <url>%2F2017%2F06%2F06%2Fzabbix%E8%BF%81%E7%A7%BB%E5%88%B0open-falcon%2F</url>
    <content type="text"><![CDATA[falcon 介绍 由于zabbix监控的可拓展性不是很高，业务的监控并不是很灵活，所以运维打算换成灵活度更好的open-falcon，他是小米公司推出的一款开源软件，基于go语言开发，安装比较方便。因为有着比较好看及灵活UI所以我们使用起来也是十分方便。但是社区相比zabbix小很多，坑也多一点，毕竟第一个吃螃蟹的人得付出点勇气。 优点说明 水平扩展能力：支持每个周期上亿次的数据采集、告警判定、历史数据存储和查询 高效率的告警策略管理：高效的portal、支持策略模板、模板继承和覆盖、多种告警方式、支持callback调用 人性化的告警设置：最大告警次数、告警级别、告警恢复通知、告警暂停、不同时段不同阈值 高效率的graph组件：单机支撑200万metric的上报、归档、存储（周期为1分钟） 高效的历史数据query组件：采用rrdtool的数据归档策略，秒级返回上百个metric一年的历史数据 dashboard：多维度的数据展示，用户自定义Screen 高可用：整个系统无核心单点，易运维，易部署，可水平扩展 开发语言： 整个系统的后端，全部golang编写，portal和dashboard使用python编写。 组件介绍agent 组件* 监控客户端 相当于zabbix_agent 收集监控数据，提供接口供我们写脚本上传数据 * 支持自发现，默认自带很多基本监控，它的自定义监控指标通过HBS（HBS读取portal数据库）获得 * 每隔60秒push给Transfer。agent与Transfer建立了长连接，数据发送速度比较快，agent提供了一个http接口/v1/push用于接收用户手工push的一些数据，然后通过长连接迅速转发给Transfer。 Aggregater组件 集群聚合模块。（目前我们还没有应用）* 聚合某集群下的所有机器的某个指标的值，比如所有机器的qps加和才是整个集群的qps，所有机器的request_fail数量 ÷ 所有机器的request_total数量=整个集群的请求失败率然后push回监控server端。 hbs（heartbeat Server）组件心跳服务器，公司所有agent都会连到HBS，每分钟发一次心跳请求。 agent发送心跳信息给HBS的时候，会把hostname、ip、agent version、plugin version等信息告诉HBS，HBS负责更新host表。 插入到portal数据库中 portal组件Portal是用来配置报警策略的，策略存放于数据库中，通过hbs的读取传递到agent端 transfer组件transfer是数据转发服务。它接收agent上报的数据，然后按照哈希规则进行数据分片、并将分片后的数据分别push给graph&amp;judge等组件。 judge组件Judge用于告警判断，agent将数据push给Transfer，Transfer不但会转发给Graph组件来绘图，还会转发给Judge用于判断是否触发告警。 alarm组件alarm模块是处理报警event的，judge产生的报警event写入redis，alarm从redis读取处理 graph组件graph是存储绘图数据的组件。graph组件 接收transfer组件推送上来的监控数据，同时处理query组件的查询请求、返回绘图数据。 query组件提供统一的绘图数据查询入口。query组件接收查询请求，根据一致性哈希算法去相应的graph实例查询不同metric的数据，然后汇总拿到的数据，最后统一返回给用户。 dashboard组件dashboard是面向用户的查询界面。在这里，用户可以看到push到graph中的所有数据，并查看其趋势图。 fe组件导航界面 nodata组件nodata用于检测监控数据的上报异常。nodata和实时报警judge模块协同工作，过程为: 配置了nodata的采集项超时未上报数据，nodata生成一条默认的模拟数据；用户配置相应的报警策略，收到mock数据就产生报警。采集项上报异常检测，作为judge模块的一个必要补充，能够使judge的实时报警功能更加可靠、完善 task组件task是监控系统一个必要的辅助模块。定时任务，实现了如下几个功能： index更新。包括图表索引的全量更新 和 垃圾索引清理。 falcon服务组件的自身状态数据采集。定时任务了采集了transfer、graph、task这三个服务的内部状态数据。 falcon自检控任务。 部署 部署使用的二进制包部署，方便快捷，agent使用ansible安装到所有机器上，设置systemctl启动。其他组件包括redis安装到运维机器上。 参照官网的部署这里略过 数据采集 falcon 数据采集分为以下三种方式 =基础库= 业务代码加载基础库、服务实例在运行过程中主动采集并push相关数据到agent =nginx_lua_module= 嵌在nginx中主动采集业务相关指标 如状态码次数，QPS，服务可用度等 =自定义数据采集= 自己通过脚本push到agent，查看https://git.chunyu.me/op/op_tools/blob/master/op_tools/falcon.py 这个很灵活 可以采集很多东西 数据模型 metric: 最核心的字段，代表这个数据采集项具体度量的是什么东西，比如内存的使用量、某个接口的调用次数 endpoint: 监控实体，代表metric的主体，比如metric是内存使用量，那么endpoint就表示该metric属于哪台机器，也可以表示某个业务比如medweb，其实endpoint是一个特殊的tag。 tags: 这是一组逗号分隔的键值对，用来对metric进行进一步的描述，比如service=falcon,location=beijing timestamp: UNIX时间戳，表示产生该数据的时间 value: 整型或者浮点型，代表该metric在指定时间点的取值 step: 整型，表示该数据采集项的汇报周期，这对于后续的监控策略配置、图表展示很重要，必须明确指定 counterType: 只能是COUNTER或者GAUGE二选一，前者表示该采集项为计数器类型，后者表示其为原值；对于计数器类型，告警判定以及图表展示前，会被先计算为速率]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>falcon, 监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac软件使用]]></title>
    <url>%2F2017%2F06%2F06%2FMac%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[推荐下载链接-xclient推荐下载链接-玩转苹果少数派 告别鼠标的效率神器: Alfred 编辑器: sublime, Pycharm, Atom 接口测试: charles, postman json格式化: Textlab 脑图: Xmind Markdown: 马克飞象, Mou 远程工具: teamviewer VPN: Tunnalblik 科学上网: shadowsocksx 密码记录: 1password]]></content>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lsof十几个实例]]></title>
    <url>%2F2017%2F06%2F06%2Flsof%E5%8D%81%E5%87%A0%E4%B8%AA%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[lsof十几个示例 lsof的意思是’列出打开的文件’，用于找出哪些文件被哪些进程打开或是占用。我们都知道Linux/UNIX的理念就是一切皆文件(包括pipes管道、sockets、directories目录、devices设备等等)。使用lsof命令的原因之一就是，当一个磁盘不能被卸载时，借助lsof这个命令我们可以轻易的识别哪些文件正在被占用. [toc] 1.通过lsof命令列出所有打开的文件 在下面的例子中，它会以长列表的形式显示打开的文件，为了便于理解，它以Command、PID、USER、FD、TYPE分类12345678910111213(July) [root@blog local]# lsofCOMMAND PID TID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root cwd DIR 253,0 4096 128 /systemd 1 root rtd DIR 253,0 4096 128 /systemd 1 root txt REG 253,0 1478168 198856 /usr/lib/systemd/systemdsystemd 1 root mem REG 253,0 20032 50421307 /usr/lib64/libuuid.so.1.3.0systemd 1 root mem REG 253,0 252704 50886702 /usr/lib64/libblkid.so.1.1.0systemd 1 root mem REG 253,0 90664 50421293 /usr/lib64/libz.so.1.2.7systemd 1 root mem REG 253,0 157424 50421256 /usr/lib64/liblzma.so.5.2.2systemd 1 root mem REG 253,0 19888 50421655 /usr/lib64/libattr.so.1.1.0systemd 1 root mem REG 253,0 19776 51932115 /usr/lib64/libdl-2.17.sosystemd 1 root mem REG 253,0 398264 51548149 /usr/lib64/libpcre.so.1.2.0systemd 1 root mem REG 253,0 2118128 50333965 /usr/lib64/libc-2.17.so 若不指定条件默认将显示所有进程打开的所有文件,lsof输出各列信息的意义如下: COMMAND：进程的名称 PID：进程标识符 USER：进程所有者 FD：文件描述符，应用程序通过文件描述符识别该文件。如cwd、txt等 cwd 表示应用程序的当前工作目录 RTD 根目录 txt txt类型文件是程序代码，应用程序二进制文件本身或共享库 MEM 内存映射文件 u 表示该文件被打开并处于读取/写入模式，而不是只读 ® 或只写 (w) 模式。 W 表示该应用程序具有对整个文件的写锁。该文件描述符用于确保每次只能打开一个应用程序实例。 R 读访问 初始打开每个应用程序时，都具有三个文件描述符，从 0 到 2，分别表示标准输入、输出和错误流。所以大多数应用程序所打开的文件的FD都是从3开始。 TYPE：文件类型，如DIR、REG等 DIR 目录 REG 基本文件 CHR 字符特殊文件 FIFO 先进先出 UNIX unix域套接字 DEVICE：指定磁盘的名称 SIZE：文件的大小 NODE：索引节点（文件在磁盘上的标识） NAME：打开文件的确切名称 2.列出特定用户打开的文件使用-u选项后接用户指定某个用户打开文件123456# lsof -u apacheCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 6032 apache cwd DIR 8,3 4096 2 /httpd 6032 apache rtd DIR 8,3 4096 2 /httpd 6032 apache txt REG 8,3 354688 1605148 /usr/sbin/httpdhttpd 6032 apache mem REG 8,3 65928 654110 /lib64/libnss_files-2.12.so 3.查找特定端口运行的进程使用-i选项来查找正在运行特定端口的进程123456789# lsof -i TCP:53COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnamed 16885 named 20u IPv4 61664 0t0 TCP localhost:domain (LISTEN)# lsof -i UDP:53COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnamed 16885 named 512u IPv4 61663 0t0 UDP localhost:domain# lsof -i:53named 16885 named 20u IPv4 61664 0t0 TCP localhost:domain (LISTEN)named 16885 named 512u IPv4 61663 0t0 UDP localhost:domain 4.列出ipv4 和ipv6的文件123456 #lsof -i 4COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1239 root 3u IPv4 10081 0t0 TCP *:ssh (LISTEN)# lsof -i 6COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1239 root 4u IPv6 10083 0t0 TCP *:ssh (LISTEN) #5.列出TCP端口范围1-1024端口12345# lsof -i TCP:1-1024COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1239 root 3u IPv4 10081 0t0 TCP *:ssh (LISTEN)sshd 1239 root 4u IPv6 10083 0t0 TCP *:ssh (LISTEN)httpd 2142 root 4u IPv6 13337 0t0 TCP *:http (LISTEN) 6.通过脱字符排除某个用户12345# lsof -u^rootCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdbus-daem 1212 dbus cwd DIR 8,3 4096 2 /dbus-daem 1212 dbus rtd DIR 8,3 4096 2 / 7.查找特定文件用户和命令1234# lsof -i -u apache COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 6032 apache txt REG 8,3 354688 1605148 /usr/sbin/httpdhttpd 6032 apache mem REG 8,3 9488 271645 /usr/lib64/apr-util-1/apr_ldap-1.so 8.查看所有的网络连接1234# lsof -iCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1239 root 3u IPv4 10081 0t0 TCP *:ssh (LISTEN)sshd 1239 root 4u IPv6 10083 0t0 TCP *:ssh (LISTEN) 9.采用pid搜索12345# lsof -p 1COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEinit 1 root cwd DIR 8,3 4096 2 /init 1 root rtd DIR 8,3 4096 2 /init 1 root txt REG 8,3 150352 527181 /sbin/init 10.杀死某个特定用户的所有活动1# kill -9 `lsof -t -u named` 11.查看谁在使用文件系统,在卸载文件系统时12345# lsof /mnt/COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 16672 root cwd DIR 11,0 8192 1856 /mntlsof 17041 root cwd DIR 11,0 8192 1856 /mntlsof 17042 root cwd DIR 11,0 8192 1856 /mnt 12.查看被删除的文件123# lsof | grep deleted --colorconsole-k 1291 root txt REG 8,3 155008 1577669 /usr/sbin/console-kit-daemon.#prelink#.bXthE2 (deleted)tail 17553 root 3r REG 8,3 6 523317 /tmp/test2 (deleted) 13.恢复误删除文件 当Linux计算机受到入侵时，常见的情况是日志文件被删除，以掩盖攻击者的踪迹。管理错误也可能导致意外删除重要的文件，比如在清理旧日志时，意外地删除了数据库的活动事务日志。有时可以通过lsof来恢复这些文件。 当进程打开了某个文件时，只要该进程保持打开该文件，即使将其删除，它依然存在于磁盘中。这意味着，进程并不知道文件已经被删除，它仍然可以向打开该文件时提供给它的文件描述符进行读取和写入。除了该进程之外，这个文件是不可见的，因为已经删除了其相应的目录索引节点。在/proc 目录下，其中包含了反映内核和进程树的各种文件。/proc目录挂载的是在内存中所映射的一块区域，所以这些文件和目录并不存在于磁盘中，因此当我们对这些文件进行读取和写入时，实际上是在从内存中获取相关信息。大多数与 lsof 相关的信息都存储于以进程的 PID 命名的目录中，即/proc/1234 中包含的是PID为1234 的进程的信息。 当系统中的某个文件被意外地删除了，只要这个时候系统中还有进程正在访问该文件，那么我们就可以通过lsof从/proc目录下恢复该文件的内容。 假如由于误操作将/var/log/messages文件删除掉了，那么这时要将/var/log/messages文件恢复的方法如下： 首先使用lsof来查看当前是否有进程打开/var/logmessages文件，如下: 12# lsof |grep /var/log/messagessyslogd 1283 root 2w REG 3,3 5381017 1773647 /var/log/messages (deleted) 从上面的信息可以看到 PID 1283（syslogd）打开文件的文件描述符为 2。同时还可以看到/var/log/messages已经标记被删除了。因此我们可以在 /proc/1283/fd/2 （fd下的每个以数字命名的文件表示进程对应的文件描述符）中查看相应的信息，如下： 1234567891011# head -n 10 /proc/1283/fd/2Aug 4 13:50:15 holmes86 syslogd 1.4.1: restart.Aug 4 13:50:15 holmes86 kernel: klogd 1.4.1, log source = /proc/kmsg started.Aug 4 13:50:15 holmes86 kernel: Linux version 2.6.22.1-8 (root@everestbuilder.linux-ren.org) (gcc version 4.2.0) #1 SMP Wed Jul 18 11:18:32 EDT 2007Aug 4 13:50:15 holmes86 kernel: BIOS-provided physical RAM map:Aug 4 13:50:15 holmes86 kernel: BIOS-e820: 0000000000000000 - 000000000009f000 (usable)Aug 4 13:50:15 holmes86 kernel: BIOS-e820: 000000000009f000 - 00000000000a0000 (reserved)Aug 4 13:50:15 holmes86 kernel: BIOS-e820: 0000000000100000 - 000000001f7d3800 (usable)Aug 4 13:50:15 holmes86 kernel: BIOS-e820: 000000001f7d3800 - 0000000020000000 (reserved)Aug 4 13:50:15 holmes86 kernel: BIOS-e820: 00000000e0000000 - 00000000f0007000 (reserved)Aug 4 13:50:15 holmes86 kernel: BIOS-e820: 00000000f0008000 - 00000000f000c000 (reserved) 从上面的信息可以看出，查看 /proc/1283/fd/2 就可以得到所要恢复的数据。如果可以通过文件描述符查看相应的数据，那么就可以使用 I/O 重定向将其复制到文件中，如: 1cat /proc/1283/fd/2 &gt; /var/log/messages]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>lsof</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[file_beat接入ELK]]></title>
    <url>%2F2017%2F06%2F06%2Ffile-beat%E6%8E%A5%E5%85%A5ELK%2F</url>
    <content type="text"><![CDATA[公司之前的日志收集使用的kids(知乎开源的日志收集工具，不知道为啥用这个，难道是因为跟我们你是邻居？)。然后统一收留存到一台机器，做中转留几天的方便大家查看，后续直接存到Hadoop中。后来搭建ELK就是直接在这个机器上起的logstash，说白了只是把这几天的日志做了可视化。但是现在我们有了另一种需求，需要收集各个机器的系统日志，加上之前做了bash_history 也需要收集。但是我并不是很想用kids,它改配置重启都比较麻烦。也不太想用整个的logstash，太大了，正好filebeat可以满足我们的需求，很轻而且使用go开发。但是还是有一个想法，之前的ELK 有的时候会出现数据延时很大的情况，我分析是ES集群的压力过大，所以我想在ES之前加一个缓冲，加一个kafka队列这正好是一个实验的机会。 之前架构： 架构调整如下： 参考:https://www.ibm.com/developerworks/cn/opensource/os-cn-elk-filebeat/index.html 版本介绍 logstash 2.3.2 Elasticsearch 2.3.2 kafka 0.10.0.2 filebeat 5.0.0 kibana 4.3.1由于ELKstack 各个组件之前有很强的版本依赖，所以安装之前最好确认下。据说filebeat5.0 logstash也得使用5.0,ES 也得使用5.0 但是我们使filebeat 和logstash之间加了一个kafka队列，就给这两个组件直接解耦了。filebeat5.0之后才能直接output到kafka。logstash2.3可以直接使用kafka 作为input。正好。。。 kafka安装 最新版本0.10.0.2java 1.8 部署参考: https://www.mtyun.com/library/32/how-to-install-kafka-on-centos7/kafka配置参考: http://www.cnblogs.com/davidwang456/p/4195873.html BrokerKafka集群包含一个或多个服务器，这种服务器被称为broker Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） PartitionParition是物理上的概念，每个Topic包含一个或多个Partition.很像ES的分片概念，合理设置可以提高效率 Producer负责发布消息到Kafka broker Consumer消息消费者，向Kafka broker读取消息的客户端。 Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。 1234567891011121314151617181920wget http://mirror.bit.edu.cn/apache/kafka/0.10.2.1/kafka_2.10-0.10.2.1.tgztar -zxvf kafka_2.10-0.10.2.1.tgz# 启动zookeeper bin/zookeeper-server-start.sh -daemon config/zookeeper.properties# 启动kafkabin/kafka-server-start.sh config/server.properties &amp;# 功能验证## 新建一个topic bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test## 查看topic bin/kafka-topics.sh --list --zookeeper localhost:2181## 手动产生消息bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test Hello Kafka！nihao shijie## 消费消息bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginningHello world!nihao shijie 单机多实例配置 由于kafka 使用的是机器的文件系统存储，所以本机多实例的集群很好。尽量减少网络通讯。 12345678cp config/server.properties config/server_1.properties# 修改配置文件 vim config/server_1.properties broker.id=1 port=9093 log.dirs=/tmp/kafka_1-logs# 启动bin/kafka-server-start.sh config/server_1.properties &amp; 机器调优File descriptors kafka会使用大量文件和网络socket，所以，我们需要把file descriptors的默认配置改为100000。修改方法如下(之前机器初始化的时候做过响应更改，可以忽略)123456789#vi /etc/sysctl.conffs.file-max = 32000#vi /etc/security/limits.confyourusersoftnofile10000youruserhardnofile30000 filebeat5 安装 版本5.0.0 12curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.0.0-x86_64.rpmrpm -ivh filebeat-5.0.0-x86_64.rpm 配置 官网配置参考 123456789101112131415161718192021222324252627282930313233343536373839404142filebeat.prospectors:- input_type: log paths: - /var/log/bash_history.log* fields: log_source: bash_history document_type: bash_history scan_frequency: 1s ignore_older: 30m- input_type: log paths: - /var/log/messages* fields: log_source: message document_type: message scan_frequency: 1s ignore_older: 30m- input_type: log paths: - /var/log/cron fields: log_source: cron document_type: cron scan_frequency: 1s ignore_older: 30m- input_type: log paths: - /var/log/secure fields: log_source: secure document_type: secure scan_frequency: 1s ignore_older: 30moutput.kafka: hosts: [&quot;localhost:9092&quot;, &quot;localhost:9093&quot;] topic: &apos;%&#123;[type]&#125;&apos; partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 logstash 配置 logstash 是把数据从kafka中拿过来，放到ES中配置如下 123456789101112131415input &#123; kafka &#123; zk_connect =&gt; &quot;localhost:2181&quot; topic_id =&gt; &quot;sys_log&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;10.215.33.36:9200&quot;] manage_template =&gt; true index =&gt; &quot;syslog-%&#123;+YYYY.MM.dd&#125;&quot; &#125;&#125;]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>ELK, filebeat, kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch调优实战过程]]></title>
    <url>%2F2017%2F06%2F06%2FElasticsearch%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[目前的集群有三个节点，配置如下可以看到三个节点都是master 都是data节点，分工并不明确分配的内存和磁盘不尽相同。现在的集群还是处于一个半原始的状态，没有做太多的优化，正好最近要扩容集群，我顺便优化一下，打算新开一台机器，把新节点用docker起，慢慢换，一步一步来。 关于集群的设置可以直接通过API传上去，也可以通过kopf插件更改(建议新手) 优化一 合理规划节点分片ES使得创建大量索引和超大量分片非常地容易，但更重要的是理解每个索引和分片都是一笔开销。所以什么都得辩证得看，能结合业务做到合适是最重要的。索引分片就是把索引数据切分成多个小的索引块，这些小的索引块能够分发到同一个集群中的不同节点。在检索时，检索的结果是该索引每个分片上检索结果的总和(尽管在某些场景中“总和”并不成立：单个分片有可能存储了所有的目标数据)。默认情况下，ElasticSearch会为每个索引创建5个主分片，就算是单结点集群亦是如此。像这样的冗余就称为过度分配：这种场景下，这种分配方式似乎是多此一举，而且只会在索引数据(把文档分发到多个分片上)和检索(必须从多个分片上查询数据然后全并结果)的时候增加复杂度，乐享其成的是，这些复杂的事情是自动处理的，但是为什么ElasticSearch要这样做呢？ 优化二 限制内存使用 参照官网一旦分析字符串被加载到 fielddata ，他们会一直在那里，直到被驱逐（或者节点崩溃）。由于这个原因，留意内存的使用情况，了解它是如何以及何时加载的，怎样限制对集群的影响是很重要的。 优化三 缓存参考地址清空缓存curl -XPOST ‘localhost:9200/_cache/clear’ 未完待续。。。]]></content>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK问题处理汇总]]></title>
    <url>%2F2017%2F06%2F05%2FELK%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1. Field data loading is forbidden on [FIELDNAME] 问题描述： 当我们在kibana画图时，有的nginx_access 日志中的有一部分字段是不能用的，在grafana中也不可以，但是别的索引文件可以，比如elapsed_log,search_log,错误如下图所示 问题原因问题原因就是我们索引的设置中 &quot;fielddata&quot; : { &quot;format&quot; : &quot;disabled&quot; } 这个设置存在导致的结果就是 Field data loading is forbidden on type Field data loading is forbidden on path Field data loading is forbidden on host 查看索引settings1curl 10.215.33.36:9200/logstash-*/_mapping 查看确实存在&quot;mapping&quot;:{&quot;fielddata&quot;:{&quot;format&quot;:&quot;disabled&quot;}这种设置， 解决过程修改mapping，ES的索引是已建成就固定的，中途修改只能是 建新索引–&gt;数据导入–&gt;旧索引删除。 所以我们可以直接修改template，等到第二天自动更新的时候新的索引设置就会正常了。具体操作记录在ES使用的template更新中。 参考：https://github.com/elastic/elasticsearch/issues/15267 2. 修改字段类型 数据通过logstash 的grok过滤之后，传输到ES当中，ES是可以自动发现的，自动生成mapping，并不用事先设置mapping，很方便。默认会把所有的字段设置成string,但是有的时候我们需要float等数据类型，怎么更改？？ 当前看有两种方式： 直接修改mapping。 在logstash grok的时候指定数据类型。 类似于这种%{NUMBER:status:float} ,NUMBER是grok支持的数据类型，status是自定义字段名称, float即为字段类型。但是这个可能不会立即生效。 3. mapping 冲突解决 问题2中第二种方法修改完字段类型之后，刷新索引，会造成新老数据的类型不一致导致冲突。 参考: https://dev.sobeslavsky.net/kibana-how-to-solve-mapping-conflict/其实完全不用理会 等到老的索引过期被删掉就好了 4. 自动删除索引脚本 收集的日志每天量很大有200多G，自己的三个ES节点存储有限，所以留存维持大约三四天的新鲜日志,需要编写个脚本定期删除索引。 12345678910111213141516171819202122232425#!/bin/bashtoday=`date +%Y.%m.%d`;echo &quot;今天是$&#123;today&#125;&quot;# 获得要删除的日期# 不指定参数时，默认删除3天前的数据（因为是凌晨删除，所以不含当天）daynum=3# 当参数个数大于1时，提示参数错误if [ $# -gt 1 ] ;then echo &quot;要么不传参数，要么只传1个参数!&quot; exit 101;fi# 当参数个数为1时，获取指定的参数if [ $# == 1 ] ;then daynum=$1fiesday=`date -d &apos;-&apos;&quot;$&#123;daynum&#125;&quot;&apos; day&apos; +%Y.%m.%d`;echo &quot;$&#123;daynum&#125;天前是$&#123;esday&#125;&quot;curl -XDELETE http://localhost:9200/logstash-nginx_access-$&#123;esday&#125;curl -XDELETE http://localhost:9200/elapsed_log-$&#123;esday&#125;curl -XDELETE http://localhost:9200/file_log-$&#123;esday&#125;curl -XDELETE http://localhost:9200/info_log-$&#123;esday&#125;curl -XDELETE http://localhost:9200/search.log-$&#123;esday&#125;echo &quot;$&#123;today&#125;执行完成&quot;# echo curl -XDELETE http://localhost:9200/aaa-*-$&#123;esday&#125; 内容如上，然后写进cron每天执行就好了。 5. logstash 插入json 格式数据 大数据那边search_log 是json格式的，安装普通的input插入就会有问题。 修改配置文件 input 中的 file 中 加入 codec =&gt; json { charset =&gt; “UTF-8” } 6. ES各节点内存分配 配置位置：/usr/local/elasticsearch-2.3.2/bin/elasticsearch.in.sh 三个节点 md6：16G md10：16G md11:32G 7. elasticsearch 节点安全重启 elasticsearch集群，有时候可能需要修改配置，增删硬件等操作，需要对节点进行升级等操作。但是服务不能停，如果直接kill掉节点，可能导致数据丢失。而且集群会认为该节点挂掉了，就开始转移数据（这个过程相当好资源，经历过两次，直接kill掉某一节点后集群开始relocation，网卡被打满，正常请求很多超时），当重启之后，它又会恢复数据，如果你当前的数据量已经很大了，这是很耗费机器和网络资源的。 第一步：先暂停集群的shard自动均衡 1234567891011curl -XPUT http://192.168.1.2:9200/_cluster/settings -d&apos; &#123; &quot;transient&quot; : &#123; &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot; &#125; &#125;&apos; 第二步：kill要升级的节点 1ps aux |grep elasticsearch |awk &apos;&#123;print $2&#125;&apos; |xargs kill 第三步：恢复集群的shard自动均衡 1234567891011curl -XPUT http://192.168.1.2/_cluster/settings -d&apos; &#123; &quot;transient&quot; : &#123; &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot; &#125; &#125;&apos; 8. 误删除node2的存储文件导致elasticsearch集群出现问题出现坏节点 集群状态如下图所示 此时kibana上面已经没有数据 ES某个节点数据被老大误删除重启elasticsearch 各个节点之后还是没有数据插入。解决办法： 需要删除坏的索引数据 就是今天的数据 具体方法：找到今天的索引elapsed_log-2016.11.03==&gt;点击”动作”==&gt;点击 “删除” 之后Unassigned 节点会消失， 刷新一下 kibana上数据出现。 9. Elasticsearch high disk watermark 问题问题描述：因为我们每个节点都不是单独只提供一个服务，每台机器的配置和硬盘大小都不尽一致，内存啥的可以很好的处理，但是硬盘大小怎么分配？比如说有一台机器600G硬盘，剩下两台2T硬盘，这种，如果平均分配恐怕是不行了，小磁盘爆了大磁盘还没有存一半。怎么办？ 问题解决 ES在你想到这个问题之前就已经想到了。操作如下 1234567curl -XPUT 10.215.33.36:9200/_cluster/settings -d &apos;&#123; &quot;transient&quot;: &#123; &quot;cluster.routing.allocation.disk.watermark.low&quot;: &quot;80%&quot;, &quot;cluster.routing.allocation.disk.watermark.high&quot;: &quot;80gb&quot;, &quot;cluster.info.update.interval&quot;: &quot;1m&quot; &#125;&#125;&apos; 解读下这三个参数。cluster.routing.allocation.disk.watermark.low Controls the low watermark for disk usage. It defaults to 85%.如果磁盘使用查过85% 就不回新建分片到这个节点上了，保证磁盘不会被撑爆。cluster.routing.allocation.disk.watermark.high Controls the high watermark. It defaults to 90%.默认值百分之90。如果某节点磁盘使用到达90% ES会自动把此节点上的分片转移到别的节点。也可以设置成某个数值类似上文50gbcluster.info.update.interval How often Elasticsearch should check on disk usage for each node in the cluster. Defaults to 30s. 多久检查一次磁盘用量，默认值30s.由于我们不可能在1分钟内写几十GB数据所以这个可以设置的稍长一些。 10. 时区问题处理之前看到过有人说ELK的失去问题，类似kibana显示出来时间与真实时间差8小时，我一直没有遇到，直到今天。。。问题描述: 事情的经过是这样的，我给ES扩容,然后修改了template（目的在给集群优化一下，提高下集群的QPS）但是导致之前grok失效了，第二天来了发现grafana中爬虫统计数据就到今天8点整，然后我意识到自己昨天的操作有问题，然后删除今天的数据重新导入。奇怪的事儿发生了。。我明明删除了几天的nginx_access日志的index但是kibana中还是有到今天八点的数据，我仔细一看今天的数据存在了昨天的索引当中。如下图然后我特别不解为啥ES会在8点钟新建一个索引？？？为啥？ 其实是logstash在ES建立的索引，它每天0点建立一个新的索引，kibana也接受这种设定, 在查询和展示时根据用户的时区进行处理。这导致了, 对于东八区, 2017-07-27日, 8点之前, 只有logstash-2017.07.26这个index, 到8点的时候, 创建新的index logstash-2017.07.27, 即, 对于我们这个时区的人来说, 一天的数据存在了两个index里面修改方案一 更改logstash的设定,使用localtime 修改方案二 不做修改，接受 11. 重建索引之前ELK就是给运维自己用，可以随便折腾，索引都是直接删除，或者配置好template等第二天重建索引再看新的数据，但是PM和CTO有时候都看grafana上的数据，运维操作就要求更规范了啊，其实不是给他们看，我们自己就该严格要求自己，下边记录下重建索引的的过程。]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK搭建]]></title>
    <url>%2F2017%2F06%2F05%2FELK%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[我来之前公司是有一套ELK服务的，但是不知道为何，没有推广起来，之前的运维都离职，原因不得而知，但是公司每天日志量还是挺大的分析起来十分费力，每天上完线之后有问题就得去机器上tail -f 各种grep AWK sort 定位问题有点慢。另外考虑到所有的nginx_access,elapsed，filelog，searchlog等都被收集到Hadoop的Hbase中，所以我想从中间截一下，用logstash把日志收集到ES集群，然后通过kibana展示出来 软件版本版本：（ELK版本相互很依赖，所以一定要注意看README.txt 不要做无用功） Logstash：2.3.2 elasticsearch: 2.3.2 kibana: kibana-4.3.1-linux-x64 升级教程 部署过程 刚开始的架构比较简单，只是logstash + ES集群(三个节点) + kibana Elasticsearch部署 解压到/usr/local/1unzip elasticsearch-2.3.2.zip -d /usr/local/ 插件安装 Elasticsearch插件安装方式（推荐） head 插件安装ElasticSearch-Head 是一个与Elastic集群（Cluster）相交互的Web前台。 ES-Head的主要作用 它展现ES集群的拓扑结构，并且可以通过它来进行索引（Index）和节点（Node）级别的操作它提供一组针对集群的查询API，并将结果以json和表格形式返回它提供一些快捷菜单，用以展现集群的各种状态 在Elasticsearch目录下1./bin/plugin install mobz/elasticsearch-head kopf 插件安装Kopf是一个ElasticSearch的管理工具，它也提供了对ES集群操作的API。 123cd /usr/local/ELK/elasticsearch-2.3.2 &amp;&amp; ./plugin install lmenezes/elasticsearch-kopf启动open http://localhost:9200/_plugin/kopf 插件访问方式http://localhost:9200/_plugin/headhttp://localhost:9200/_plugin/koph 下载安装方式 从https://github.com/mobz/elasticsearch-head下载ZIP包。在 elasticsearch 目录下创建目录plugins/head/ 并且将刚刚解压的elasticsearch-head-master目录下所有内容COPY到当前创建的plugins/head/目录下即可。 配置文件修改node112345678917: cluster.name: cy_es_cluster #集群名字（各节点一致） 23: node.name: node-1 #节点名称 43: bootstrap.mlockall: true #锁住内存 54: network.host: 10.***.1 #本机内网地址 58: http.port: 9200 60: http.cors.enrue 61: http.cors.allow-origin: &quot;/.*/&quot; 71: discovery.zen.ping.unicast.hosts: [&quot;10.***.1:9300&quot;, “10.***.2:9300”, “10.***.3:9300&quot;] 75: discovery.zen.minimum_master_nodes: 2 node2 123456789101112grep -n &apos;^ [a-Z]&apos; /usr/local/elasticsearch-2.3.2/config/elasticsearch.yml 17: cluster.name: cy_es_cluster #集群名字（各节点一致） 23: node.name: node-2 #节点名称 33: path.data: /data2/ES 37: path.logs: /data2/ES/logs 43: bootstrap.mlockall: true #锁住内存 54: network.host: 10.***.2 #本机内网地址 58: http.port: 9200 #端口 60: http.cors.enabled: true #kibana3 需要打开 kibana4忽略 61: http.cors.allow-origin: &quot;/.*/“ #kibana3 需要打开 kibana4忽略 73: discovery.zen.ping.unicast.hosts: [&quot;10.***.1:9300&quot;, “10.***.2:9300”, “10.***.3:9300&quot;] #集群discovery 77: discovery.zen.minimum_master_nodes: 2 #最小集群节点数目 新加node3在 ,配置一致 * 启动 nohup /usr/local/elasticsearch-2.3.2/bin/elasticsearch -Des.insecure.allow.root=true &amp; * 启动 systemctl start kibana.service * 查看各节点启动情况 http://10.***.01:9200/_plugin/head/ logstash部署12345678910111213141516171819202122232425262728293031323334353637tar -zxvf logstash-all-plugins-2.3.2.tar.gz -C /usr/local/vim /usr/local/logstash-2.3.2/conf/main.conf 修改配置文件 包括input filter output 配置文件详解 input &#123; file &#123; path =&gt; &quot;/home/chunyu/backup/nginx_log/access.log-*&quot; 日志文件可以模糊匹配 type =&gt; “nginx_access&quot; 类型 在后面output匹配用 ignore_older =&gt; 14400 读取4小时之内文件 exclude =&gt; &quot;*.lzma&quot; 不包括.lzma 结尾的文件 &#125; &#125; 注意在这遇到过匹配日志日期结尾的情况，例如test.2016-07-03-access.log 匹配为*.%&#123;yyyy&#125;-%&#123;MM&#125;-%&#123;dd&#125;-access.log反复不生效，遂采取ignore_older, 没有在官方找到匹配日期的方式。 filter &#123; if [type] == &quot;nginx_access&quot; &#123; grok &#123; patterns_dir =&gt; &quot;/usr/local/logstash-2.3.2/patterns/nginx&quot; 指定grok匹配文件 match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;NGINXACCESS&#125;&quot; &#125; &#125; geoip &#123; source =&gt; &quot;clientip&quot; &#125; &#125; &#125; grok 正则表达式需要自己编写 对照nginx log format 和nginx具体日志在grok debugger中写完 粘贴到/usr/local/logstash/patterns/nginx中即可output &#123; if [type] == &quot;nginx_access&quot; &#123; elasticsearch &#123; hosts =&gt; [&quot;10.***.01:9200&quot;] index =&gt; &quot;logstash-nginx_access-%&#123;+YYYY.MM.dd&#125;&quot; &#125; &#125; &#125; output就是把logstash数据输出给elastic search 注意这个index 便是 kibana中setting要输入的 grok表达式 注意，使用过程中有的情况会出现grok失效的情况，类似状态码为500 的时候或者某种情况，你所要过滤的字段在日志中是空的，可以使用“|”等提高容错性。以nginx_access.log实例(注意按照自己的日志format灵活变动)grokdebug地址 nginx_access.log 格式 12log_format main &apos;$remote_addr - - [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; [$request_time, $upstream_response_time] $host ($remote_port) &quot;sid=$cookie_sessionid&quot;&apos;; 对应grok表达式1NGINXACCESS %&#123;IPORHOST:clientip&#125; - - \[%&#123;HTTPDATE:time_local&#125;\] \&quot;%&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;\&quot; %&#123;NUMBER:status:float&#125; %&#123;NUMBER:body_bytes_sent:float&#125;\ (?:\&quot;(?:%&#123;URI:referrer&#125;|-)\&quot;|%&#123;QS:referrer&#125;) %&#123;QS:agent&#125; \[%&#123;NUMBER:request_time:float&#125;\, (%&#123;NUMBER:upstream_response_time:float&#125;|-)] %&#123;IPORHOST:host&#125; \(%&#123;NUMBER:remote_port&#125;\) %&#123;QS&#125; kibana 部署tar -zxvf kibana-4.1.1-linux-x64.tar.gz -C /usr/local/grep -n “^ [a-Z]” /usr/local/ELK/kibana-4.3.1-linux-x64/config/kibana.yml 12: elasticsearch.url: “http://10.***.1:9200” 只需要输入elastic search地址 端口 即可 启动 nohup /usr/local/ELK/kibana-4.3.1-linux-x64/bin/kibana &amp;登录 10.215.33.36:5601通过—&gt;settings设置index —&gt;查看discovery —&gt;制作visualize —&gt;设置dashboard 达到日志可视化 问题处理 地图配置kibana地图配置：默认的地图不能显示，所以我们换成了高德地图先下载GeoIP库12wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gzgunzip GeoLiteCity.dat.gz vim conf/main.conf 添加下图内容 重启logstash kibana修改123456vim ./src/ui/public/vislib/visualizations/_map.js +11 注释 url: &apos;https://otile&#123;s&#125;-s.mqcdn.com/tiles/1.0.0/map/&#123;z&#125;/&#123;x&#125;/&#123;y&#125;.jpeg&apos;,添加 url: &apos;http://webrd0&#123;s&#125;.is.autonavi.com/appmaptile?lang=zh_cn&amp;size=1&amp;scale=1&amp;style=7&amp;x=&#123;x&#125;&amp;y=&#123;y&#125;&amp;z=&#123;z&#125;&apos;, （style=6为卫星地图7为普通地图）vim ./src/ui/autoload.js +35末尾 添加 &apos;ui/vislib&apos;重启kibana 之后打开logs.chunyu.me setting重置 但是还是没有地图显示，界面为白板，报错Google一下需要改elastic search模板更改方法：打开 logstash main.conf配置文件 需要把output 的index 改为logstash开头使用elasticsearch默认模板即可 重启logstash效果图 lucene语法 ELK中，kibana或者grafana用来展示数据，Elasticsearch是搜索引擎也是存储中心，它是构建在Lucene之上的，所以过滤器语法和Lucene相同。 版本：（ELK版本相互很依赖，所以一定要注意看README.txt 不要做无用功） Logstash：2.3.2 elasticsearch: 2.3.2 kibana: kibana-4.3.1-linux-x64 参考链接1参考链接2 全文搜索500显示带有500 的内容 短语搜索使用双引号包起来“like Gecko” 字段 也可以按页面左侧显示的字段搜索 限定字段全文搜索:field:value 精确搜索：关键字加上双引号 filed:”value” status:404 搜索http状态码为404的文档字段本身是否存在 _exists_:http：返回结果中需要有http字段 _missing_:http：不能含有http字段通配符 ? 匹配单个字符 匹配0到多个字符kiba?a, el*search ? 不能用作第一个字符，例如：?text text 正则 es支持部分正则功能 mesg:/mes{2}ages?/模糊搜索 ~:在一个单词后面加上~启用模糊搜索 first~ 也能匹配到 frist 还可以指定需要多少相似度 cromm~0.3 会匹配到 from 和 chrome 数值范围0.0 ~ 1.0，默认0.5，越大越接近搜索的原始值近似搜索 在短语后面加上~ “select where”~3 表示 select 和 where 中间隔着3个单词以内范围搜索 数值和时间类型的字段可以对某一范围进行查询 length:[100 TO 200] date:{“now-6h” TO “now”} [ ] 表示端点数值包含在范围内，{ } 表示端点数值不包含在范围内逻辑操作 AND OR NOT+：搜索结果中必须包含此项 -：不能含有此项+android -OPPO 包含android 不包含OPPO+apache -jakarta test：结果中必须存在apache，不能有jakarta，test可有可无]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch使用]]></title>
    <url>%2F2017%2F06%2F05%2FElasticSearch%E4%B9%8Btemplate%2Cmapping%2Csetting%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[ElasticSearch之template,mapping,setting修改 自搭建ELK以来一直跟elasticsearch打交道，但是只停留在基本会用，其他并没有更深入研究，其中发现问题越来越多，从之前字段自动analyzed ，导致画图是字段不能用，到后来分片问题。所以是时候study，mark一下。 基本命令ES提供了丰富的API给我们使用通常我们可以使用curl来在线操作。也可以在kopf插件中操作（操作要谨慎）例如：1curl -XGET &apos;http://elasticsearch.example.com:9200/_all/_settings?pretty&apos; 参考：http://spuder.github.io/2015/elasticsearch-commands/ template 不重启更改 刚开始的时候，每次实验都去改/etc/elasticsearch/elasticsearch.yml配置文件。事实上在template里修改settings更方便而且灵活！当然最主要的，还是调节里面的properties设定，合理的控制store和analyze了。logstash 模板 查看template curl -XGET http://10.215.33.36:9200/_template 更改默认分片数量（参考链接） 索引分片数量是将一个索引一致性拆分成为n个部分，这样可以平均的存储到每个ES节点，方便更好的存储与查询。 查看默认template1curl elasticsearch.example.com:9200/_template/logstash?pretty 结果显示如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&#123; &quot;logstash&quot; : &#123; &quot;order&quot; : 0, &quot;template&quot; : &quot;logstash-*&quot;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;refresh_interval&quot; : &quot;5s&quot; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;_default_&quot; : &#123; &quot;dynamic_templates&quot; : [ &#123; &quot;message_field&quot; : &#123; &quot;mapping&quot; : &#123; &quot;fielddata&quot; : &#123; &quot;format&quot; : &quot;disabled&quot; &#125;, &quot;index&quot; : &quot;not_analyzed&quot;, &quot;omit_norms&quot; : true, &quot;type&quot; : &quot;string&quot; &#125;, &quot;match_mapping_type&quot; : &quot;string&quot;, &quot;match&quot; : &quot;message&quot; &#125; &#125;, &#123; &quot;string_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;fielddata&quot; : &#123; &quot;format&quot; : &quot;disabled&quot; &#125;, &quot;index&quot; : &quot;not_analyzed&quot;, &quot;omit_norms&quot; : true, &quot;type&quot; : &quot;string&quot;, &quot;fields&quot; : &#123; &quot;raw&quot; : &#123; &quot;ignore_above&quot; : 256, &quot;index&quot; : &quot;not_analyzed&quot;, &quot;type&quot; : &quot;string&quot; &#125; &#125; &#125;, &quot;match_mapping_type&quot; : &quot;string&quot;, &quot;match&quot; : &quot;*&quot; &#125; &#125; ], &quot;_all&quot; : &#123; &quot;omit_norms&quot; : true, &quot;enabled&quot; : true &#125;, &quot;properties&quot; : &#123; &quot;@timestamp&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;geoip&quot; : &#123; &quot;dynamic&quot; : true, &quot;properties&quot; : &#123; &quot;ip&quot; : &#123; &quot;type&quot; : &quot;ip&quot; &#125;, &quot;latitude&quot; : &#123; &quot;type&quot; : &quot;float&quot; &#125;, &quot;location&quot; : &#123; &quot;type&quot; : &quot;geo_point&quot; &#125;, &quot;longitude&quot; : &#123; &quot;type&quot; : &quot;float&quot; &#125; &#125; &#125;, &quot;@version&quot; : &#123; &quot;index&quot; : &quot;not_analyzed&quot;, &quot;type&quot; : &quot;string&quot; &#125; &#125; &#125; &#125;, &quot;aliases&quot; : &#123; &#125; &#125;&#125; 修改template，上传我们需要把这个json 重定向到一个文件中，其中没有用的部分去掉，加上需要改动的地方1curl http://10.215.33.36:9200/_template/logstash?pretty &gt; logstash_template.json 修改完的json如下(最好把修改之前的配置备份下，这是运维的基本) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&#123; &quot;template&quot; : &quot;logstash-*&quot;, &quot;settings&quot; : &#123; &quot;number_of_shards&quot;: 5 &#125;, &quot;mappings&quot; : &#123; &quot;_default_&quot; : &#123; &quot;dynamic_templates&quot; : [ &#123; &quot;date_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;format&quot; : &quot;dateOptionalTime&quot;, &quot;doc_values&quot; : true, &quot;type&quot; : &quot;date&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;date&quot; &#125; &#125;, &#123; &quot;byte_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;doc_values&quot; : true, &quot;type&quot; : &quot;byte&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;byte&quot; &#125; &#125;, &#123; &quot;double_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;doc_values&quot; : true, &quot;type&quot; : &quot;double&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;double&quot; &#125; &#125;, &#123; &quot;float_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;doc_values&quot; : true, &quot;type&quot; : &quot;float&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;float&quot; &#125; &#125;, &#123; &quot;integer_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;doc_values&quot; : true, &quot;type&quot; : &quot;integer&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;integer&quot; &#125; &#125;, &#123; &quot;long_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;doc_values&quot; : true, &quot;type&quot; : &quot;long&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;long&quot; &#125; &#125;, &#123; &quot;short_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;doc_values&quot; : true, &quot;type&quot; : &quot;short&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;short&quot; &#125; &#125;, &#123; &quot;string_fields&quot; : &#123; &quot;mapping&quot; : &#123; &quot;index&quot; : &quot;not_analyzed&quot;, &quot;omit_norms&quot; : true, &quot;doc_values&quot; : true, &quot;type&quot; : &quot;string&quot; &#125;, &quot;match&quot; : &quot;*&quot;, &quot;match_mapping_type&quot; : &quot;string&quot; &#125; &#125; ], &quot;properties&quot; : &#123; &quot;@version&quot; : &#123; &quot;index&quot; : &quot;not_analyzed&quot;, &quot;doc_values&quot; : true, &quot;type&quot; : &quot;string&quot; &#125; &#125;, &quot;_all&quot; : &#123; &quot;enabled&quot; : true &#125; &#125; &#125;, &quot;aliases&quot; : &#123; &#125;&#125; 最后需要把这个配置push上去，方法如下注意加&quot;@&quot;1curl -XPUT http://10.215.33.36:9200/_template/logstash -d &quot;@logstash_template_new.json&quot; 第二天就可以看到5个分片因为建好的index 配置是更改不了的 除非新建个新的把数据迁移下，我们没有这么着急，不如等到第二天 setting 不重启更改 配置副本数量 curl -XPUT http://10.215.33.36:9200/_settings -d ‘{“index”:{“number_of_replicas”:2}}’ mapping 不重启更改 查看 nginx_access index的节点 注意要在端口号后面加上索引的具体名称，支持正则，查看所有直接为空 curl -XGET ‘10.215.33.36:9200/logstash-nginx_access-*/_mapping?pretty’ 更改属性 curl -XPOST ‘10.215.33.36:9200/logstash-nginx_access-*/_mapping’ -d ‘{“mapping” :{“type” : “string”, “index” : “not_analyzed”}}’]]></content>
      <categories>
        <category>运维工具</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac+gihub pages+hexo搭建博客]]></title>
    <url>%2F2017%2F06%2F03%2FMac-gihub-pages-hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Mac下使用github+hexo建博客 很早之前就想建个博客，纠结于怎么建，有的同学使用LNMP，使用wordpress,Django，首先得有个服务器，还是不够方便。近期使用github托管一些平常使用的代码或者设置，而且平常习惯使用markdown写笔记，所以就想直接用github_pages+hexo建个博客。 下边是过程。 安装过程首先需要在Mac本地把git装好，公钥传到github，然后装好node，npm 12345- brew install nodejs- brew install npm - npm install -g hexo-cli- (看下安装目录，可以做个软链到/usr/bin/)- ln -s /usr/local/lib/node_modules/hexo-cli/bin/hexo /usr/bin/hexo 然后再自己Mac上建一个工作取名Blog，在github上面新建个工程参考github-pages官网必须以 git用户名.github.io这种格式命名工程名。之后再Blog 目录下clone该工程。 12- mkdir Blog- cd Blog &amp;&amp; git clone https://github.com/fanquqi/fanquqi.github.io.git 在本地建立站点安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。Hexo相关操作参考 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 所以以上的&lt;folder&gt;我们 用Blog代替操作可以改为这样,因为上文中我们已经手动建了一个Blog文件夹 123cd Blog hexo init npm install 新建完成后我们的文件夹目录如下： 123456789.├── _config.yml├── package.json├── scaffolds├── scripts├── source| ├── _drafts| └── _posts└── themes 之后运行 hexo start 可以在http://localhost:4000 看到自己的页面 关联github 设置好本地之后,需要关联github，以后就可以在本地写作，传到github上了 修改_config.yml,在最后加上(注意不能有tab，’:’后必须有个空格) 1234deploy: type: git repository: https://github.com/fanquqi/fanquqi.github.io.git branch: master 在blog文件夹目录下执行生成静态页面命令： 1$ hexo generate 或者：hexo g 如果出现如下报错 12ERROR Local hexo not found in ~/blogERROR Try runing: &apos;npm install hexo --save&apos; 执行1npm install hexo --save 如果没有出现略过此步 然后deploy 一下 同步到远程github1hexo deploy 如果出现一下报错 没有找到git 12➜ Blog hexo deployERROR Deployer not found: git 安装hexo-deploy-git 1234➜ Blog npm install hexo-deployer-git --savehexo-site@0.0.0 /Users/fanquanqing/workspace/Blog└─┬ hexo-deployer-git@0.3.0 └── moment@2.18.1 之后再次执行hexo generate和hexo deploy命令如果没有报错，你就可以在https://fanquqi.github.io 看到之前自己4000端口相同的内容了。好棒哦。。。 ##安装theme 主题可以到hexo官网下载 https://hexo.io/themes/ 这里以Next为例。 123456cd Blog &amp;&amp; git clone https://github.com/iissnan/hexo-theme-next themes/next#替换Blog/_config.yml themes: landscape 为 themes: nexthexo clean //清除缓存hexo generatahexo deployhexo start 配置查看http://theme-next.iissnan.com/getting-started.html 搜索功能添加 1234567891011# 安装hexo-generator-searchdb(给博客添加搜索功能)➜ Blog npm install hexo-generator-searchdb --savehexo-site@0.0.0 /Users/fanquanqing/workspace/Blog└─┬ hexo-generator-searchdb@1.0.7 └── striptags@3.0.1#修改_config.yml,添加search: path: search.xml field: post format: html limit: 10000 hexo start发现问题12➜ Blog hexo sERROR Plugin load failed: hexo-generator-searchdb 查看插件hexo-generator-searchdb 中的Redmine 发现node 需要降级到4.2.2以下 12npm install -g nn 4.2.2 menu 添加 默认的有：首页，归档，分类，标签，关于, 但是next主题默认只显示了首页 归档和标签。以 categories 添加为例(tags 也是如此) 1.新建categories page1hexo new page &quot;categories&quot; 会在source下面生成一个categories目录。 2.修改index.md 打开categories文件夹中的index.md页面，在头部添加 – type: “categories” （为了点击的时候链接生效）修改完之后12345---title: 分类date: 2017-06-03 22:17:53type: &quot;categories&quot;--- 3.在hexo &gt; theme &gt; next &gt; _config.yml 中修改menu下的categorues,去掉前面井号注释。 文章发表发表步骤如下 新建文章 hexo new urlooker使用 编辑文章 直接vim 或者用自己的markdown编辑器编辑，这里需要提醒下添加tags和 categories 使用 hexo中有Front-matter这个概念，是文件最上方以 — 分隔的区域，用于指定个别文件的变量。123456---title: urlooker使用date: 2017-06-02 18:57:50tags: open-falconcategoris: 监控--- 举例如上 添加评论功能 多说在2017.6.1停止服务了，比较尴尬，但是新的next 集成了友言的评论功能，设置非常方便 注册友言官网，在管理后台记下UID 添加UID到_config.yml 格式为 1youyan_uid: 2323232 静态文件处理 好像github-pages 最大存储为300M 所以不能尽情的存储，最好吧静态图片放到外面，我直接放到了七牛云存储上，每个图片生成一个外链，很方便。 重新部署下hexo即可。 好了，现在就可以随时markdown了。 参考： https://madongqiang2201.github.io/ http://theme-next.iissnan.com/getting-started.html http://jeasonstudio.github.io/2016/05/26/Mac%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8EGitHub-Page%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2/]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PXE网络引导批量安装操作系统]]></title>
    <url>%2F2016%2F08%2F03%2FPXE%E7%BD%91%E7%BB%9C%E5%BC%95%E5%AF%BC%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[前言：一般系统安装方式基本上都是做一个安装盘，然后每个机器都是插上U盘花上20分钟左右时间操作，包括选择引导方式，语言，键盘布局，设定初始root密码等一系列操作。较为繁琐，但是辛苦不是最大的问题，效率才是。 这次我们机房迁移一共有三十台机器，每台不算初始化时间，只是安装系统就得很长时间，所以我们选择用PXE网络引导的方式，告别U盘。 PXE技术分析：工作过程其实就是DHCP+TFTP+FTP三个服务配合操作（FTP也可以用HTTP服务代替 因为我们本次操作的机器HTTP端口被占用所以直接选择FTP） 原理：&emsp;&emsp;客户端PXE网卡启动从DHCP服务器获得IP从TFTP服务器上下载pxelinux.0、default根据配置文件default指定的vmlinuz、initrd.img启动系统内核,并下载指定的ks.cfg文件跟据ks.cfg去(HTTP/FTP/NFS)服务器下载RPM包并安装系统完成安装 操作步骤：DHCP服务器安装配置首先 确认DHCP服务没有安装， 1rpm -qa | grep dhcp 没有安装过就直接yum安装（也要注意同一内网之中有没有别的DHCP服务在运行，避免冲突） 1yum install dhcp* 然后配置一下 1vim /etc/dhcp/dhcpd.conf 配置完直接重启 dhcp服务。 service dhcp restart 但是我们会看到 这个时候直接运行 /usr/sbin/dhcpd就OK了(ps -ef 确认下进程是否在) TFTP服务安装首先确定机器有没有安装TFTP服务 rpm -qa | grep tftp如果没有直接yum安装 yum install tftp*然后稍微修改些配置 disable 由yes改为no server_args = 加上-u nobody (用户可以是所有人) /usr/lib/tftpboot 即为文件目录 服务启动 systemctl start tftp FTP服务安装确定ftp服务之前没有安装 yum -y install vsftpd 修改一些配置 vim /etc/vsftpd/vsftpd.conf 更改 anonymous_enable=YES 这里需要保证anonymous_enable=YES （匿名用户登录开启）然后测试ftp服务正常与否 可以找一台内网机器或者本机实验如果能用anonymous 免密码登录到ftp 证明服务正常 文件拷贝&emsp;&emsp;现在基本服务已经搭建完成了 我们需要准备一下镜像文件等挂载到TFTP 和 FTP服务器上供网络引导的机器读取下载。因为我们上边dhcp.conf中已经写到了 pxe client 要向 TFTP服务器请求的filename pxelinux.0所以我们需要把这个文件拷贝到TFTP文件目录下。PXE启动映像文件由syslinux提供，我们只要安装syslinux，就会生成一个pxelinux.0文件， 只需要将 pxelinux.0 这个文件复制到TFTP根目录即可。 yum install -y syslinux cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ 把我们之前下载好的镜像ISO文件挂载一下，目标路径为ftp服务器文件目录位置 mount -o loop /soft/CentOS-7-x86_64-Everything-1511.iso /var/ftp/pub/ 复制iso 镜像中的/image/pxeboot/initrd.img 和vmlinux 至/var/lib/tftpboot/ 文件夹中 cp /var/ftp/pub/image/pxeboot/initrd.img /var/lib/tftpboot/ cp /var/ftp/pub/image/pxeboot/vmlinux /var/lib/tftpboot/ 复制iso 镜像中的/isolinux/*.msg 至/var/lib/tftpboot/ 文件夹中 cp /var/ftp/pub/isolinux/*.msg /var/lib/tftpboot/ 在/var/lib/tftpboot/ 中新建一个pxelinux.cfg目录 mkdir /var/lib/tftpboot/pxelinux.cfg 将iso 镜像中的/isolinux 目录中的isolinux.cfg复制到pxelinux.cfg目录中，同时更改文件名称为default cp /var/ftp/pub/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default 修改default文件 vim /var/lib/tftpboot/pxelinux.cfg/default 服务启动 systemctl start vsftpd (如果有kickstart脚本也要在这里说明路径 例如: ks=ftp://10.215.33.12/pub/ks.cfg) 至此文件拷贝完成（别忘了重启三个服务DHCP，TFTP，FTP）网络引导已经配置OK 重启客户端服务器的时候只需要进到boot manager中选择pxe网络引导即可。 kickstart脚本测试完成后会附上]]></content>
      <categories>
        <category>基础运维</category>
      </categories>
      <tags>
        <tag>PXE</tag>
      </tags>
  </entry>
</search>